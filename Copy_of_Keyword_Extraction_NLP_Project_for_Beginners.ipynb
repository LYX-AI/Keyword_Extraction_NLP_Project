{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1702999,
          "sourceType": "datasetVersion",
          "datasetId": 1009351
        }
      ],
      "dockerImageVersionId": 30684,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LYX-AI/Keyword_Extraction_NLP_Project/blob/main/Copy_of_Keyword_Extraction_NLP_Project_for_Beginners.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "rowhitswami_nips_papers_1987_2019_updated_path = kagglehub.dataset_download('rowhitswami/nips-papers-1987-2019-updated')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "AaNgs_4VpA4r"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we're going to break down the most popular _keyword extraction_ algorithms. At the end, we'll create a small web application that takes a PDF file of an article (on any topic) and returns the keywords. To build the app, we'll need to go beyond this notebook and write a .py file, so when you're ready, follow the link here or in the bottom section to continue.\n",
        "\n",
        "Enjoy your learning!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "ZzOaPEBApA4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is Keyword Extraction?\n",
        "\n",
        "Keyword extraction is the process of selecting words or phrases from text that best reflect its content. Keywords convey the concentrated meaning of the text and can be used for categorization or retrieval.\n",
        "\n",
        "![image.png](attachment:7fbca9ff-8142-4789-84ff-34defbe74443.png)\n",
        "\n",
        "### General approach\n",
        "\n",
        "![image.png](attachment:67946850-f573-4005-af03-bf562efb8c4e.png)\n",
        "\n",
        "There are different methods of keyword extraction. But all of them have the following steps:\n",
        "\n",
        "- **Candidate Generation**: First up, we cast a wide net to catch potential keywords\n",
        "- **Candidate Scoring**: Next, we score each candidate based on how well they represent the text's main themes\n",
        "- **Candidate Ranking**: Finally, we rank these words based on their scores. The top performers get the title of \"keywords,\" ready to reflect the core ideas of the text.\n",
        "\n",
        "### Keyword Extraction Techniques\n",
        "\n",
        "Let's explore some of the most common techniques for keyword extraction. This is _just a brief overview_ of the algorithms we will cover in this notebook. If you don't understand what it's about, that's okay. A little later we'll break down how each algorithm works.\n",
        "\n",
        "All algorithms for keyword extraction can be categorized(分类) into statistical(统计), graph-based or word2vec (or deep networks) based algorithms. There are also algorithms based on machine learning, but we will not discuss them in this notebook. Just be aware of them.\n",
        "关键词提取的核心任务是从文本中找出最能代表其内容的词汇或短语。通过候选生成、评分和排序的步骤，我们可以有效地提取文本的核心思想。\n",
        "\n",
        "算法分类：关键词提取的算法可以分为几大类，主要包括统计方法（如TF-IDF）、基于图的算法、深度学习（如word2vec）等。理解这些算法能帮助我们选择合适的方法来处理不同类型的文本数据。\n",
        "\n",
        "统计方法简介：\n",
        "\n",
        "TF-IDF：通过计算词频和逆文档频率，找到在特定文档中独特而重要的词汇。\n",
        "\n",
        "Rake：通过分析词汇出现频率和共现关系来识别关键词，适合快速的文本分析。\n",
        "\n",
        "Yake：类似于Rake，但通过更复杂的统计特征进行优化，识别文本中不仅常见而且特别重要的词汇。\n",
        "\n",
        "这些方法是关键词提取领域的基础，理解它们能够帮助你在处理实际文本时选择合适的算法。\n",
        "\n",
        "\n",
        "\n",
        "一下三种方法是上面讲解的基于统计学的方法实现的算法\n",
        "#### Statistical Approaches\n",
        "- **TF-IDF** (Term Frequency-Inverse Document Frequency). This is a classic, folks. TF-IDF measures how important a word is to a document in a collection of documents. It’s like finding out which words in your document are the real VIPs because they're not commonly used elsewhere.\n",
        "- **Rake** (Rapid Automatic Keyword Extraction). Rake is pretty straightforward and efficient. It identifies key phrases in text by analyzing the frequency of word appearance and its co-occurrence with other words. Think of it as highlighting the standout phrases based on how often words show up together.\n",
        "- **Yake**. Yake takes a unique twist by using statistical features to rank keywords within a text based on their singularity and relevancy. It’s like having a smart assistant that picks out terms that are not just common, but also particularly significant in your text.\n",
        "\n",
        "TF-IDF：这个算法基于词频（TF）和逆文档频率（IDF）来计算词语在文档中的重要性。它能够帮助我们发现那些在某一特定文档中频繁出现，但在所有文档中都不常见的词语，这些通常是该文档的关键词。\n",
        "\n",
        "Rake：Rake算法通过分析词语的共现关系（即哪些词语经常一起出现）和出现的频率来快速识别关键词。它通常不需要像TF-IDF那样依赖大规模的语料库，可以快速识别文本中的关键短语。\n",
        "\n",
        "Yake：Yake也是一种基于统计的关键词提取算法，它类似于Rake，但引入了更多的统计特征来提高关键词提取的准确性。它不仅关注词语的频率，还会考虑其他因素，如词汇的独特性和与其他词的相关性，从而帮助识别文本中的关键内容。\n",
        "\n",
        "#### Graph-based Approaches **bold text**\n",
        "- **TextRank**. Imagine a system where words are nodes in a graph, connected based on their co-occurrence within a text. TextRank ranks these nodes (words) to figure out which are the most influential in the text. It’s like seeing which friend (word) in your social circle (document) is the most popular.\n",
        "- **SingleRank**. SingleRank takes TextRank a step further by adding weight to the connections between words based on their co-occurrence frequency. It’s akin to saying, not only is this friend popular, but their connections are super strong because they hang out together a lot.\n",
        "- **TopicRank**. TopicRank: This one groups words into clusters or topics first, and then performs a ranking similar to TextRank on these topics. It’s like organizing your friends into cliques and figuring out which clique is the life of the party.\n",
        "\n",
        "#### Deep Learning Techniques\n",
        "- **Word Embeddings** for Keyword Extraction. Deep learning brings us word embeddings, where words are converted into vector space models. This technique helps in extracting keywords by understanding the semantic richness of words, almost like capturing the essence of words in a mathematical form, which can really pinpoint the key concepts in a text.\n",
        "\n",
        "---\n",
        "\n",
        "### Applications of Keyword Extraction\n",
        "\n",
        "It's important to understand why keyword extraction is necessary. It seems that this is not as interesting a task as extracting named entities or training a neural network to generate text. But it is still a very important task.\n",
        "\n",
        "So, you can use Keywords for:\n",
        "\n",
        "- **Information Retrieval**: Enhancing search engine performance by indexing documents based on their keywords.\n",
        "- **Content Summarization**: Generating concise summaries of large texts.\n",
        "- **Document Clustering and Classification**: Organizing documents into categories based on their keywords.\n",
        "- **Trend Analysis**: Identifying emerging trends in large datasets by analyzing frequently occurring keywords.\n",
        "基于图的算法（如TextRank、SingleRank和TopicRank）使用图结构来理解词语之间的关系，这些算法可以通过计算词语在文本中的连接强度来识别关键词。TextRank看词语的整体影响力，SingleRank则通过共现频率进一步加权，TopicRank则在分组主题之后进行排名。\n",
        "\n",
        "深度学习技术（如词嵌入）为关键词提取提供了更为智能和精确的方法。通过将词语转化为向量，算法不仅关注词语的频率，还可以捕捉其在文本中的深层语义，使得关键词提取更加精准。\n",
        "\n",
        "以上都是一些关键词提取的算法我个人认为首次学习不用花太多时间搞懂它是干啥的\n",
        "---"
      ],
      "metadata": {
        "id": "wyA_1V8spA4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset Overview and EDA（了解项目用到的数据集）\n",
        "\n",
        "Before we break down each algorithm and put it into practice, let's look at the data we have. We want to understand the structure, size, and nature of our dataset.\n",
        "\n",
        "The NIPS (NeurIPS) paper dataset comprises thousands of research papers from the annual Neural Information Processing Systems conference. These papers cover a wide array of topics in machine learning, artificial intelligence, and statistics. The dataset typically includes titles, abstracts, full text of the papers, authors, and sometimes the keywords provided by the authors.\n",
        "\n",
        "### Setting Up the Environment\n",
        "\n",
        "First, ensure you have imported all necessary libraries and have the dataset ready. Kaggle datasets can typically be accessed directly within Kaggle notebooks through relative paths. If you are running this notebook locally, make sure that the imported libraries are installed in your environment."
      ],
      "metadata": {
        "id": "vga5uEAcpA4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Display plots inline and set default figure size\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Assuming the dataset is loaded into a Kaggle notebook with the filename 'nips_papers.csv'\n",
        "nips_papers = pd.read_csv('../input/nips-papers-1987-2019-updated/papers.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-26T17:59:07.886037Z",
          "iopub.execute_input": "2024-04-26T17:59:07.886445Z",
          "iopub.status.idle": "2024-04-26T17:59:18.830569Z",
          "shell.execute_reply.started": "2024-04-26T17:59:07.886412Z",
          "shell.execute_reply": "2024-04-26T17:59:18.829444Z"
        },
        "trusted": true,
        "id": "XBFeO7c-pA4y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Dataset Structure\n",
        "\n",
        "Let's begin with examining the first few rows, the data types of each column, and checking for missing values."
      ],
      "metadata": {
        "id": "2JgK1AADpA4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T20:13:14.18178Z",
          "iopub.execute_input": "2024-04-21T20:13:14.183011Z",
          "iopub.status.idle": "2024-04-21T20:13:14.201136Z",
          "shell.execute_reply.started": "2024-04-21T20:13:14.182973Z",
          "shell.execute_reply": "2024-04-21T20:13:14.200041Z"
        },
        "trusted": true,
        "id": "pTTFv-DRpA4z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we are most interested in the full_text column. It contains the text of the abstract and the paper."
      ],
      "metadata": {
        "id": "Dtjk9YCIpA4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nips_papers.isnull().sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:41:34.504507Z",
          "iopub.execute_input": "2024-04-20T19:41:34.505021Z",
          "iopub.status.idle": "2024-04-20T19:41:34.522897Z",
          "shell.execute_reply.started": "2024-04-20T19:41:34.504984Z",
          "shell.execute_reply": "2024-04-20T19:41:34.52061Z"
        },
        "trusted": true,
        "id": "dQ5HpIl3pA40"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "full_text is missing in only three rows. But there are more than three thousand rows where abstract text is missing."
      ],
      "metadata": {
        "id": "f7H2WuGkpA40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning the Data\n",
        "\n",
        "We'll just get rid of rows that don't have full_text."
      ],
      "metadata": {
        "id": "iFQGSSuApA40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping rows where 'abstract' is missing\n",
        "nips_papers.dropna(subset=['full_text'], inplace=True)\n",
        "\n",
        "# Reset index after dropping rows\n",
        "nips_papers.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:41:37.445413Z",
          "iopub.execute_input": "2024-04-20T19:41:37.445926Z",
          "iopub.status.idle": "2024-04-20T19:41:37.474138Z",
          "shell.execute_reply.started": "2024-04-20T19:41:37.445888Z",
          "shell.execute_reply": "2024-04-20T19:41:37.471594Z"
        },
        "trusted": true,
        "id": "gIahJDo1pA41"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing Trends\n",
        "\n",
        "Look for trends such as the number of papers published over the years. This can provide insights into the dataset's coverage and any growth trends in the field of machine learning and NLP."
      ],
      "metadata": {
        "id": "7tK0VOMWpA41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the number of papers per year\n",
        "papers_per_year = nips_papers.groupby('year')['title'].count()\n",
        "papers_per_year.plot(kind='line', marker='o', title='NIPS Papers per Year')\n",
        "plt.ylabel('Number of Papers')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:41:40.641949Z",
          "iopub.execute_input": "2024-04-20T19:41:40.643444Z",
          "iopub.status.idle": "2024-04-20T19:41:40.985424Z",
          "shell.execute_reply.started": "2024-04-20T19:41:40.643383Z",
          "shell.execute_reply": "2024-04-20T19:41:40.983974Z"
        },
        "trusted": true,
        "id": "5VoM8YzSpA41"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keyword Frequency Analysis\n",
        "\n",
        "Before diving into keyword extraction, analyzing the most frequent words in your texts (e.g., paper titles or abstracts) can give you a preliminary idea of common themes."
      ],
      "metadata": {
        "id": "wAOx3INCpA41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec = CountVectorizer(stop_words='english', max_features=100)\n",
        "word_counts = vec.fit_transform(nips_papers['full_text'])\n",
        "sum_words = word_counts.sum(axis=0)\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Plotting the top 20 words\n",
        "df_words_freq = pd.DataFrame(words_freq[:30], columns=['Word', 'Frequency'])\n",
        "sns.barplot(x='Frequency', y='Word', data=df_words_freq, palette='Blues_d')\n",
        "plt.title('Top 30 Most Frequent Words in NIPS Abstracts')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:42:05.706404Z",
          "iopub.execute_input": "2024-04-20T19:42:05.706968Z",
          "iopub.status.idle": "2024-04-20T19:42:56.844506Z",
          "shell.execute_reply.started": "2024-04-20T19:42:05.706926Z",
          "shell.execute_reply": "2024-04-20T19:42:56.843102Z"
        },
        "trusted": true,
        "id": "WcHEqGFHpA41"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "High-frequency words not filtered out by the standard stop words list but still irrelevant to content (like 'paper' or 'results') might need custom stop words processing or advanced techniques like TF-IDF to reduce their impact."
      ],
      "metadata": {
        "id": "P7phSGMqpA42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "### NLTK, spacy\n",
        "\n",
        "Two of the most popular Python libraries for these tasks are NLTK and spaCy.\n",
        "\n",
        "- [NLTK (Natural Language Toolkit)](https://www.nltk.org/) is widely used for teaching and research. It's a comprehensive library featuring many utilities for classical NLP tasks and text processing.\n",
        "- [spaCy](https://spacy.io/) is known for its fast processing speeds and ease of use. It also provides pre-trained models and word vectors and is designed specifically for production use."
      ],
      "metadata": {
        "id": "mXxG9LxVpA42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:44:26.560716Z",
          "iopub.execute_input": "2024-04-20T19:44:26.561248Z",
          "iopub.status.idle": "2024-04-20T19:44:26.700524Z",
          "shell.execute_reply.started": "2024-04-20T19:44:26.561215Z",
          "shell.execute_reply": "2024-04-20T19:44:26.699242Z"
        },
        "trusted": true,
        "id": "OR6K8sFnpA42"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization is the process of splitting text into meaningful units such as words, sentences, or tokens. This is typically the first step in text preprocessing.\n",
        "\n",
        "_*Running this cell takes time_"
      ],
      "metadata": {
        "id": "tTfNA9XgpA42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Using NLTK for tokenization\n",
        "nips_papers['tokens'] = nips_papers['full_text'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:44:33.509138Z",
          "iopub.execute_input": "2024-04-20T19:44:33.509622Z",
          "iopub.status.idle": "2024-04-20T19:54:50.762379Z",
          "shell.execute_reply.started": "2024-04-20T19:44:33.509589Z",
          "shell.execute_reply": "2024-04-20T19:54:50.759882Z"
        },
        "trusted": true,
        "id": "Bn4huWZ4pA42"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords removal\n",
        "\n",
        "Stopwords are commonly used words (such as \"the\", \"a\", \"an\", \"in\") that are often removed in the preprocessing phase because they carry less meaningful information for analysis."
      ],
      "metadata": {
        "id": "yH1PxDr5pA42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load general English stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:55:26.758982Z",
          "iopub.execute_input": "2024-04-20T19:55:26.760474Z",
          "iopub.status.idle": "2024-04-20T19:55:26.770547Z",
          "shell.execute_reply.started": "2024-04-20T19:55:26.760419Z",
          "shell.execute_reply": "2024-04-20T19:55:26.769209Z"
        },
        "trusted": true,
        "id": "31DAM8D6pA42"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with specialized datasets like NIPS papers, you might encounter domain-specific stopwords like \"neural\", \"network\", \"learning\", which are overly frequent and might be less informative for certain analyses."
      ],
      "metadata": {
        "id": "oLRo_IJKpA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom stopwords for NIPS context\n",
        "custom_stopwords = {\"cid\", \"neural\", \"network\", \"learning\", \"model\", \"algorithm\", \"data\",\n",
        "                    \"set\", \"function\", \"problem\", \"models\", \"number\", \"figure\", \"results\",\n",
        "                   \"information\", \"distribution\", \"using\", \"used\", \"use\", \"given\", \"method\",\n",
        "                   \"neural\", \"networks\"}\n",
        "\n",
        "# Function to remove both general and custom stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    return [token for token in tokens if token.lower() not in stop_words and\n",
        "            token.lower() not in custom_stopwords and not token.isdigit()]\n",
        "\n",
        "nips_papers['filtered_tokens'] = nips_papers['tokens'].apply(remove_stopwords)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:01:25.625652Z",
          "iopub.execute_input": "2024-04-20T20:01:25.626389Z",
          "iopub.status.idle": "2024-04-20T20:01:49.435082Z",
          "shell.execute_reply.started": "2024-04-20T20:01:25.626298Z",
          "shell.execute_reply": "2024-04-20T20:01:49.433166Z"
        },
        "trusted": true,
        "id": "U4h8KV3dpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "Stemming reduces words to their word stem, base, or root form. For example, \"fishing\", \"fished\", \"fisher\" all reduce to the stem \"fish\".\n",
        "\n",
        "_*Running this cell takes time_"
      ],
      "metadata": {
        "id": "efnrXbD7pA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to stem tokens\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "nips_papers['stemmed_tokens'] = nips_papers['filtered_tokens'].apply(stem_tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:02:18.102524Z",
          "iopub.execute_input": "2024-04-20T20:02:18.103082Z",
          "iopub.status.idle": "2024-04-20T20:13:41.111909Z",
          "shell.execute_reply.started": "2024-04-20T20:02:18.103041Z",
          "shell.execute_reply": "2024-04-20T20:13:41.10988Z"
        },
        "trusted": true,
        "id": "cGB-45DNpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers['stemmed_tokens'][0]"
      ],
      "metadata": {
        "trusted": true,
        "id": "-34p9ZnrpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *To fix: Lemmatization\n",
        "\n",
        "Lemmatization, similar to stemming, reduces words to their base or root form, but it ensures that the root word belongs to the language."
      ],
      "metadata": {
        "id": "9Zvf0jXTpA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize a list of tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Apply lemmatization to the filtered tokens\n",
        "nips_papers['lemmatized_tokens'] = nips_papers['filtered_tokens'].apply(lemmatize_tokens)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "17c3dSfMpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Algorithms\n",
        "\n",
        "### TF-IDF for Keyword Extraction\n",
        "\n",
        "![image.png](attachment:31a52603-1d81-4846-844a-79c77f5fac43.png)\n",
        "\n",
        "- **Calculate Term Frequency (TF)**: For each word in your document, calculate the term frequency. This is simply the number of times a word appears in a document divided by the total number of words in that document.\n",
        "\n",
        "$$\n",
        "\\text{TF}(word) = \\frac{\\text{Number of times the word appears in a document}}{\\text{Total number of words in the document}}\n",
        "$$\n",
        "\n",
        "    \n",
        "- **Calculate Inverse Document Frequency (IDF)**: This measures how common or rare a word is across all documents. It is calculated as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
        "\n",
        "$$\n",
        "\\text{IDF}(word) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing the word}}\\right)\n",
        "$$\n",
        "    \n",
        "- **Compute TF-IDF: Multiply TF by IDF.** The words with the highest TF-IDF scores are considered the most relevant keywords in the document.\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(word) = \\text{TF}(word) \\times \\text{IDF}(word)\n",
        "$$\n",
        "\n",
        "Very simple, huh?\n",
        "\n",
        "Let's implement this algorithm in pure Python."
      ],
      "metadata": {
        "id": "ir0etvzLpA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_tf(tokenized_docs):\n",
        "    tf_dicts = []\n",
        "    for doc in tokenized_docs:\n",
        "        tf_dict = {}\n",
        "        total_terms = len(doc)\n",
        "        for term in doc:\n",
        "            tf_dict[term] = tf_dict.get(term, 0) + 1 / total_terms\n",
        "        tf_dicts.append(tf_dict)\n",
        "    return tf_dicts\n",
        "\n",
        "def compute_df(tokenized_docs):\n",
        "    df_dict = {}\n",
        "    for doc in tokenized_docs:\n",
        "        for term in set(doc):\n",
        "            df_dict[term] = df_dict.get(term, 0) + 1\n",
        "    return df_dict\n",
        "\n",
        "def compute_idf(df_dict, total_docs):\n",
        "    idf_dict = {}\n",
        "    for term, count in df_dict.items():\n",
        "        idf_dict[term] = math.log(total_docs / count)\n",
        "    return idf_dict\n",
        "\n",
        "def compute_tf_idf(tf_dicts, idf_dict):\n",
        "    tf_idf_dicts = []\n",
        "    for tf_dict in tf_dicts:\n",
        "        tf_idf_dict = {}\n",
        "        for term, tf in tf_dict.items():\n",
        "            tf_idf_dict[term] = tf * idf_dict.get(term, 0)\n",
        "        tf_idf_dicts.append(tf_idf_dict)\n",
        "    return tf_idf_dicts\n",
        "\n",
        "def select_top_keywords(tf_idf_dicts, top_n=5):\n",
        "    top_keywords = []\n",
        "    for tf_idf_dict in tf_idf_dicts:\n",
        "        sorted_terms = sorted(tf_idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_keywords.append([term for term, score in sorted_terms[:top_n]])\n",
        "    return top_keywords"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:34:02.487809Z",
          "iopub.execute_input": "2024-04-20T20:34:02.489016Z",
          "iopub.status.idle": "2024-04-20T20:34:02.500136Z",
          "shell.execute_reply.started": "2024-04-20T20:34:02.488959Z",
          "shell.execute_reply": "2024-04-20T20:34:02.498518Z"
        },
        "trusted": true,
        "id": "pg8_bSyTpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_docs = nips_papers['stemmed_tokens'].tolist()\n",
        "tf_dicts = compute_tf(tokenized_docs)\n",
        "df_dict = compute_df(tokenized_docs)\n",
        "total_docs = len(tokenized_docs)\n",
        "idf_dict = compute_idf(df_dict, total_docs)\n",
        "tf_idf_dicts = compute_tf_idf(tf_dicts, idf_dict)\n",
        "top_tf_idf_keywords = select_top_keywords(tf_idf_dicts)\n",
        "\n",
        "# Update the DataFrame\n",
        "nips_papers['tf_idf_keywords'] = top_tf_idf_keywords"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:34:05.320552Z",
          "iopub.execute_input": "2024-04-20T20:34:05.321039Z",
          "iopub.status.idle": "2024-04-20T20:34:34.278624Z",
          "shell.execute_reply.started": "2024-04-20T20:34:05.321004Z",
          "shell.execute_reply": "2024-04-20T20:34:34.277292Z"
        },
        "trusted": true,
        "id": "Y4RHMuyXpA44"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers['tf_idf_keywords'][99]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:37:47.962391Z",
          "iopub.execute_input": "2024-04-20T20:37:47.962929Z",
          "iopub.status.idle": "2024-04-20T20:37:47.97194Z",
          "shell.execute_reply.started": "2024-04-20T20:37:47.962877Z",
          "shell.execute_reply": "2024-04-20T20:37:47.97052Z"
        },
        "trusted": true,
        "id": "nK03Pqm-pA44"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### TextRank ± filter punct.symbols, build graph\n",
        "\n",
        "_You might want to know how [PageRank](https://www.cis.upenn.edu/~mkearns/teaching/NetworkedLife/pagerank.pdf) algorithm works._\n",
        "\n",
        "![image.png](attachment:284b26a2-ced0-4091-9d48-8b7e9c148884.png)\n",
        "\n",
        "This algorithm is based on graphs. The idea is that after you get tokens from the text, you represent each token as a vertex of the graph. The links between these vertices are created when tokens occur close to each other in the text (i.e., we assume they are somehow related).\n",
        "\n",
        "The importance of each vertex is calculated using a method similar to PageRank. This involves iteratively assigning scores to each vertex based on the scores of its neighboring vertices."
      ],
      "metadata": {
        "id": "qbvXFcSUpA44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the networkx library to apply the PageRank algorithm on the graph."
      ],
      "metadata": {
        "id": "8hSAhhtSpA44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "def build_graph(words, window_size=4):\n",
        "    G = nx.Graph()\n",
        "    for index, word in enumerate(words):\n",
        "        for i in range(index + 1, index + window_size):\n",
        "            if i >= len(words):\n",
        "                break\n",
        "            node1, node2 = word, words[i]\n",
        "            if node1 != node2:\n",
        "                if G.has_edge(node1, node2):\n",
        "                    G[node1][node2]['weight'] += 1\n",
        "                else:\n",
        "                    G.add_edge(node1, node2, weight=1)\n",
        "    return G\n",
        "\n",
        "def extract_keywords(G):\n",
        "    ranks = nx.pagerank(G, weight='weight')\n",
        "    sorted_ranks = sorted(ranks.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [word for word, rank in sorted_ranks[:5]]  # Adjust the number of keywords as needed\n",
        "\n",
        "def apply_text_rank(tokens):\n",
        "    graph = build_graph(tokens)\n",
        "    return extract_keywords(graph)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:42:07.864165Z",
          "iopub.execute_input": "2024-04-20T20:42:07.864671Z",
          "iopub.status.idle": "2024-04-20T20:42:09.027968Z",
          "shell.execute_reply.started": "2024-04-20T20:42:07.864637Z",
          "shell.execute_reply": "2024-04-20T20:42:09.026853Z"
        },
        "trusted": true,
        "id": "9qqAPFdFpA44"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "_*Running this cell takes time_"
      ],
      "metadata": {
        "id": "413SANUHpA44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply TextRank to DataFrame\n",
        "nips_papers['text_rank_keywords'] = nips_papers['stemmed_tokens'].apply(apply_text_rank)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T21:16:34.778188Z",
          "iopub.execute_input": "2024-04-20T21:16:34.77875Z",
          "iopub.status.idle": "2024-04-20T21:40:55.767485Z",
          "shell.execute_reply.started": "2024-04-20T21:16:34.778709Z",
          "shell.execute_reply": "2024-04-20T21:40:55.766418Z"
        },
        "trusted": true,
        "id": "InwdSEc1pA45"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers['text_rank_keywords'][0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T21:46:28.697361Z",
          "iopub.execute_input": "2024-04-20T21:46:28.698168Z",
          "iopub.status.idle": "2024-04-20T21:46:28.707323Z",
          "shell.execute_reply.started": "2024-04-20T21:46:28.698117Z",
          "shell.execute_reply": "2024-04-20T21:46:28.705248Z"
        },
        "trusted": true,
        "id": "2_oB8TGYpA49"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, we will not visualize the full graph here. It is too large and there is little sense in it. But we can visualize a part of the graph for certain vertices."
      ],
      "metadata": {
        "id": "S4QxO2xnpA49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_keyword_subgraph(graph, keywords):\n",
        "    # Find nodes and neighbors\n",
        "    subgraph_nodes = set(keywords)\n",
        "    for keyword in keywords:\n",
        "        subgraph_nodes.update(graph.neighbors(keyword))\n",
        "\n",
        "    subgraph = graph.subgraph(subgraph_nodes)\n",
        "\n",
        "    # Draw the subgraph\n",
        "    pos = nx.spring_layout(subgraph)\n",
        "    nx.draw(subgraph, pos, with_labels=True, node_color='lightgreen', edge_color='#BBBBBB', node_size=300)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: visualize subgraph for specific keywords\n",
        "keywords = ['speech', 'vision']\n",
        "visualize_keyword_subgraph(graph, keywords)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T21:46:44.448996Z",
          "iopub.execute_input": "2024-04-20T21:46:44.449497Z",
          "iopub.status.idle": "2024-04-20T21:46:44.518784Z",
          "shell.execute_reply.started": "2024-04-20T21:46:44.449457Z",
          "shell.execute_reply": "2024-04-20T21:46:44.516721Z"
        },
        "trusted": true,
        "id": "9RC-F54BpA49"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SingleRank** and **TopicRank** are some variations ofr TextRank."
      ],
      "metadata": {
        "id": "3gTohbabpA49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Rake\n",
        "\n",
        "The RAKE algorithm is designed to efficiently identify key phrases from a text by analyzing the frequency of word appearance and its co-occurrence with other words in the text.\n",
        "\n",
        "RAKE starts by splitting the text into an array of words and phrases. It specifically looks for phrases by splitting the text at delimiters, which are typically punctuation marks and stop words.\n",
        "\n",
        "Each word is scored based on its frequency and its degree (the total number of times it appears in different phrases). The score is calculated using a formula:\n",
        "\n",
        "$$\n",
        "\\text{Score(word)} = \\frac{\\text{degree(word)}}{\\text{frequency(word)}}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\text{degree(word)}$ is the number of times a word co-occurs with other words across all phrases.\n",
        "- $\\text{frequency(word)}$ is the number of times the word appears in the entire text.\n",
        "\n",
        "Phrases are scored by summing the scores of the words they contain. This step emphasizes multi-word expressions that are more likely to represent key concepts.\n",
        "\n",
        "Then, the highest-scoring phrases are selected as the key phrases for the text.\n",
        "\n",
        "We aren't gonna implement this algorithm form scratch. Instead, we are going to use python package pke: https://github.com/boudinfl/pke"
      ],
      "metadata": {
        "id": "QgAlaXeZpA4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "jHpiPKn0pA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pke\n",
        "\n",
        "# Initialize RAKE extractor\n",
        "extractor = pke.unsupervised.RAKE()\n",
        "\n",
        "# Sample text\n",
        "text = \"Compatibility of systems of linear constraints over the set of natural numbers. \" \\\n",
        "       \"Criteria of compatibility of a system of linear Diophantine equations, strict \" \\\n",
        "       \"inequations, and nonstrict inequations are considered.\"\n",
        "\n",
        "# Load the text into the extractor\n",
        "extractor.load_document(input=text, language='en')\n",
        "\n",
        "# Select the longest sequence of nouns and adjectives as candidates.\n",
        "extractor.candidate_selection()\n",
        "\n",
        "# Calculate scores using the default scoring function\n",
        "extractor.candidate_weighting()\n",
        "\n",
        "# Get the 5 highest scored candidates\n",
        "keywords = extractor.get_n_best(n=5)\n",
        "\n",
        "# Print extracted keywords\n",
        "print(keywords)"
      ],
      "metadata": {
        "id": "Fcd6QB5WpA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### YAKE!\n",
        "\n",
        "YAKE! (Yet Another Keyword Extraction) designed to automatically detect keywords in text using an unsupervised approach. This algorithm leverages statistical features from the text itself without needing any external data.\n",
        "\n",
        "YAKE! calculates a set of features for each word in the text, including:\n",
        "- Case insensitivity: Emphasizes lower-case versions of words.\n",
        "- Word position: Early occurrences may be more important.\n",
        "- Word frequency: Frequent words are not necessarily important.\n",
        "- Word relatedness to context: Looks at the co-occurrence of words within their surrounding text.\n",
        "- Word differentiation: Identifies how spread out the word’s contexts are.\n",
        "\n",
        "Using these features, YAKE! computes a score for each word. A lower score suggests a higher relevance as a keyword. The scoring formula accounts for the uniqueness of the word’s occurrence across the document and its significance in individual contexts.\n",
        "\n",
        "After scoring, YAKE! selects the best-scoring words as keywords, paying attention to avoiding redundancy and favoring words that provide unique information.\n",
        "\n",
        "this algorithm is useful for dealing with large documents or collections of documents where you need a quick and effective method to identify the most relevant keywords without prior training or configuration."
      ],
      "metadata": {
        "id": "X2ftJOp_pA4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pke\n",
        "\n",
        "# Initialize YAKE extractor\n",
        "extractor = pke.unsupervised.YAKE()\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural language processing enables computers to understand human language. This technology powers many applications like speech recognition, machine translation, and sentiment analysis.\"\n",
        "\n",
        "# Load the text into the extractor\n",
        "extractor.load_document(input=text, language='en')\n",
        "\n",
        "# Candidate selection with no specific n-gram size, using default parameters\n",
        "extractor.candidate_selection()\n",
        "\n",
        "# Candidate weighting using YAKE's default parameters\n",
        "extractor.candidate_weighting()\n",
        "\n",
        "# Get the 5 highest scored candidates\n",
        "keywords = extractor.get_n_best(n=5)\n",
        "\n",
        "# Print extracted keywords\n",
        "print(keywords)"
      ],
      "metadata": {
        "id": "UnB_6BXdpA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Word Embeddings (keyBERT)\n",
        "\n",
        "KeyBERT is a keyword extraction model that utilizes state-of-the-art natural language processing technology based on word embeddings, specifically leveraging models like BERT (Bidirectional Encoder Representations from Transformers). Unlike traditional keyword extraction methods that rely on statistical measures or graph-based representations, KeyBERT uses the semantic meanings of words as represented by their embeddings. Here's how KeyBERT works:\n",
        "\n",
        "    Semantic Representation: KeyBERT first converts the text into a dense vector representation using a pre-trained language model (like BERT, RoBERTa, or DistilBERT). These embeddings capture the contextual relationships between words in the text.\n",
        "\n",
        "    Similarity Calculation: It then calculates the cosine similarity between the embeddings of the document and the embeddings of candidate phrases or words extracted from the document. This step assesses how representative the phrases are of the document's main topics.\n",
        "\n",
        "    Keyword Selection: Keywords are selected based on their similarity scores. The higher the score, the more relevant the word or phrase is to the overall context of the document.\n",
        "\n",
        "KeyBERT's strength lies in its ability to understand the nuances of language due to the sophisticated nature of the embeddings used, allowing it to identify keywords that are contextually relevant rather than merely frequent or statistically significant."
      ],
      "metadata": {
        "id": "8OHbmztEpA4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'full_text': [\n",
        "        \"Natural language processing enables computers to understand human language. This technology powers many applications like speech recognition, machine translation, and sentiment analysis.\",\n",
        "        \"Machine learning provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It's widely used in data analysis and complex algorithms.\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Load a model\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "\n",
        "# Initialize KeyBERT with the loaded model\n",
        "kw_model = KeyBERT(model=model)\n",
        "\n",
        "# Function to extract keywords\n",
        "def extract_keywords(text):\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)\n",
        "    return keywords\n",
        "\n",
        "# Apply the function to the 'full_text' column\n",
        "df['keywords'] = df['full_text'].apply(lambda x: extract_keywords(x))\n",
        "\n",
        "# Display the DataFrame with keywords\n",
        "print(df[['full_text', 'keywords']])"
      ],
      "metadata": {
        "id": "kPIVfnzfpA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "first_abstract = nips_papers.iloc[0]['abstract']\n",
        "print(\"TF-IDF Keywords:\", extract_keywords(first_abstract, 'tfidf'))\n",
        "print(\"TextRank Keywords:\", extract_keywords(first_abstract, 'textrank'))\n",
        "print(\"YAKE Keywords:\", extract_keywords(first_abstract, 'yake'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T21:56:54.158309Z",
          "iopub.execute_input": "2024-04-15T21:56:54.158681Z",
          "iopub.status.idle": "2024-04-15T21:57:00.742926Z",
          "shell.execute_reply.started": "2024-04-15T21:56:54.158654Z",
          "shell.execute_reply": "2024-04-15T21:57:00.741743Z"
        },
        "trusted": true,
        "id": "XXMG9zGipA4_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an App\n",
        "\n",
        "We're going to build an application that allows users to upload file or paste a text into a website window and receive keywords.\n",
        "\n",
        "![image.png](attachment:33e9fd66-c190-44bb-a2ae-ed2ffec1f902.png)\n",
        "\n",
        "To do this, we'll need to create a project repository: we'll use **Flask** and a simple **Bootstrap template**, and also write some auxiliary code that will **process the received data** and **log everything that happens**. This is a great exercise for aspiring engineers who want not only to understand algorithms but also to apply them in real-world scenarios.\n",
        "\n",
        "### [To continue this project visit my github](https://github.com/mlsamurai/keyword-extraction-project-nlp/blob/main/tutorial/Part%202.%20Application.ipynb/)\n",
        "\n",
        "Please upvote this notebook if it was helpful for you and share if you know someone who might find it helpful! Thank you."
      ],
      "metadata": {
        "id": "GgiBZfrbpA4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References and Links"
      ],
      "metadata": {
        "id": "HNuaCx7ZpA4_"
      }
    }
  ]
}