{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1702999,
          "sourceType": "datasetVersion",
          "datasetId": 1009351
        }
      ],
      "dockerImageVersionId": 30684,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LYX-AI/Keyword_Extraction_NLP_Project/blob/main/Copy_of_Keyword_Extraction_NLP_Project_for_Beginners.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#将本次项目的文件和文件夹都保存到Google Drive上\n",
        "#挂载 Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm9SO2Vg1gDP",
        "outputId": "48fe7a7d-6b6f-4a27-b863-480fbf69bec0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#创建路径\n",
        "import os\n",
        "\n",
        "# 在 Google Drive 中创建目录\n",
        "drive_path = '/content/drive/MyDrive/Kaggle/01Keyword_Extraction_NLP_Project_for_Beginners'\n",
        "os.makedirs(drive_path, exist_ok=True)  # 创建目录，如果已经存在则不报错\n"
      ],
      "metadata": {
        "id": "wxNsLJJn66Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#保存文件到 Google Drive：\n",
        "import shutil\n",
        "source_folder = '/content/input/nips-papers-1987-2019-updated/'  # 当前文件夹路径\n",
        "\n",
        "# 假设你已经上传了文件到 Colab\n",
        "for filename in os.listdir(source_folder):\n",
        "    file_path = os.path.join(source_folder, filename)  # 源文件路径\n",
        "    if os.path.isfile(file_path):  # 如果是文件\n",
        "        shutil.move(file_path, os.path.join(drive_path, filename))  # 移动到 Google Drive\n",
        "    elif os.path.isdir(file_path):  # 如果是子文件夹\n",
        "        shutil.move(file_path, os.path.join(drive_path, filename))  # 移动子文件夹\n",
        "\n",
        "# 验证文件是否保存成功\n",
        "os.listdir(drive_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2uClVxE7Q47",
        "outputId": "b50ca1d2-8423-4345-dbbc-d8e24e083d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['papers.csv', 'authors.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#从Drive中加载到Colab中\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/MyDrive/Kaggle/01Keyword_Extraction_NLP_Project_for_Beginners'\n",
        "# 检查文件是否存在\n",
        "print(os.listdir(drive_path))\n",
        "\n",
        "# 加载数据集\n",
        "authors_df = pd.read_csv(os.path.join(drive_path, 'authors.csv'))\n",
        "papers_df = pd.read_csv(os.path.join(drive_path, 'papers.csv'))\n",
        "\n",
        "# 查看数据的前几行\n",
        "authors_df.head(), papers_df.head()"
      ],
      "metadata": {
        "id": "8AFqw9UW9gsp",
        "outputId": "b853dfe8-b51e-440b-9dd6-51de4dc32c05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['papers.csv', 'authors.csv']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   source_id first_name    last_name institution\n",
              " 0         27       Alan       Murray         NaN\n",
              " 1         27    Anthony        Smith         NaN\n",
              " 2         27        Zoe       Butler         NaN\n",
              " 3         63      Yaser  Abu-Mostafa         NaN\n",
              " 4         60    Michael     Fleisher         NaN,\n",
              "    source_id  year                                              title  \\\n",
              " 0         27  1987                         Bit-Serial Neural Networks   \n",
              " 1         63  1987                        Connectivity Versus Entropy   \n",
              " 2         60  1987        The Hopfield Model with Multi-Level Neurons   \n",
              " 3         59  1987                               How Neural Nets Work   \n",
              " 4         69  1987  Spatial Organization of Neural Networks: A Pro...   \n",
              " \n",
              "   abstract                                          full_text  \n",
              " 0      NaN  573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...  \n",
              " 1      NaN  1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...  \n",
              " 2      NaN  278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...  \n",
              " 3      NaN  442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...  \n",
              " 4      NaN  740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...  )"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "rowhitswami_nips_papers_1987_2019_updated_path = kagglehub.dataset_download('rowhitswami/nips-papers-1987-2019-updated')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "AaNgs_4VpA4r"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we're going to break down the most popular _keyword extraction_ algorithms. At the end, we'll create a small web application that takes a PDF file of an article (on any topic) and returns the keywords. To build the app, we'll need to go beyond this notebook and write a .py file, so when you're ready, follow the link here or in the bottom section to continue.\n",
        "\n",
        "Enjoy your learning!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "ZzOaPEBApA4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is Keyword Extraction?\n",
        "\n",
        "Keyword extraction is the process of selecting words or phrases from text that best reflect its content. Keywords convey the concentrated meaning of the text and can be used for categorization or retrieval.\n",
        "\n",
        "![image.png](attachment:7fbca9ff-8142-4789-84ff-34defbe74443.png)\n",
        "\n",
        "### General approach\n",
        "\n",
        "![image.png](attachment:67946850-f573-4005-af03-bf562efb8c4e.png)\n",
        "\n",
        "There are different methods of keyword extraction. But all of them have the following steps:\n",
        "\n",
        "- **Candidate Generation**: First up, we cast a wide net to catch potential keywords\n",
        "- **Candidate Scoring**: Next, we score each candidate based on how well they represent the text's main themes\n",
        "- **Candidate Ranking**: Finally, we rank these words based on their scores. The top performers get the title of \"keywords,\" ready to reflect the core ideas of the text.\n",
        "\n",
        "### Keyword Extraction Techniques\n",
        "\n",
        "Let's explore some of the most common techniques for keyword extraction. This is _just a brief overview_ of the algorithms we will cover in this notebook. If you don't understand what it's about, that's okay. A little later we'll break down how each algorithm works.\n",
        "\n",
        "All algorithms for keyword extraction can be categorized(分类) into statistical(统计), graph-based or word2vec (or deep networks) based algorithms. There are also algorithms based on machine learning, but we will not discuss them in this notebook. Just be aware of them.\n",
        "关键词提取的核心任务是从文本中找出最能代表其内容的词汇或短语。通过候选生成、评分和排序的步骤，我们可以有效地提取文本的核心思想。\n",
        "\n",
        "算法分类：关键词提取的算法可以分为几大类，主要包括统计方法（如TF-IDF）、基于图的算法、深度学习（如word2vec）等。理解这些算法能帮助我们选择合适的方法来处理不同类型的文本数据。\n",
        "\n",
        "统计方法简介：\n",
        "\n",
        "TF-IDF：通过计算词频和逆文档频率，找到在特定文档中独特而重要的词汇。\n",
        "\n",
        "Rake：通过分析词汇出现频率和共现关系来识别关键词，适合快速的文本分析。\n",
        "\n",
        "Yake：类似于Rake，但通过更复杂的统计特征进行优化，识别文本中不仅常见而且特别重要的词汇。\n",
        "\n",
        "这些方法是关键词提取领域的基础，理解它们能够帮助你在处理实际文本时选择合适的算法。\n",
        "\n",
        "\n",
        "\n",
        "一下三种方法是上面讲解的基于统计学的方法实现的算法\n",
        "#### Statistical Approaches\n",
        "- **TF-IDF** (Term Frequency-Inverse Document Frequency). This is a classic, folks. TF-IDF measures how important a word is to a document in a collection of documents. It’s like finding out which words in your document are the real VIPs because they're not commonly used elsewhere.\n",
        "- **Rake** (Rapid Automatic Keyword Extraction). Rake is pretty straightforward and efficient. It identifies key phrases in text by analyzing the frequency of word appearance and its co-occurrence with other words. Think of it as highlighting the standout phrases based on how often words show up together.\n",
        "- **Yake**. Yake takes a unique twist by using statistical features to rank keywords within a text based on their singularity and relevancy. It’s like having a smart assistant that picks out terms that are not just common, but also particularly significant in your text.\n",
        "\n",
        "TF-IDF：这个算法基于词频（TF）和逆文档频率（IDF）来计算词语在文档中的重要性。它能够帮助我们发现那些在某一特定文档中频繁出现，但在所有文档中都不常见的词语，这些通常是该文档的关键词。\n",
        "\n",
        "Rake：Rake算法通过分析词语的共现关系（即哪些词语经常一起出现）和出现的频率来快速识别关键词。它通常不需要像TF-IDF那样依赖大规模的语料库，可以快速识别文本中的关键短语。\n",
        "\n",
        "Yake：Yake也是一种基于统计的关键词提取算法，它类似于Rake，但引入了更多的统计特征来提高关键词提取的准确性。它不仅关注词语的频率，还会考虑其他因素，如词汇的独特性和与其他词的相关性，从而帮助识别文本中的关键内容。\n",
        "\n",
        "#### Graph-based Approaches **bold text**\n",
        "- **TextRank**. Imagine a system where words are nodes in a graph, connected based on their co-occurrence within a text. TextRank ranks these nodes (words) to figure out which are the most influential in the text. It’s like seeing which friend (word) in your social circle (document) is the most popular.\n",
        "- **SingleRank**. SingleRank takes TextRank a step further by adding weight to the connections between words based on their co-occurrence frequency. It’s akin to saying, not only is this friend popular, but their connections are super strong because they hang out together a lot.\n",
        "- **TopicRank**. TopicRank: This one groups words into clusters or topics first, and then performs a ranking similar to TextRank on these topics. It’s like organizing your friends into cliques and figuring out which clique is the life of the party.\n",
        "\n",
        "#### Deep Learning Techniques\n",
        "- **Word Embeddings** for Keyword Extraction. Deep learning brings us word embeddings, where words are converted into vector space models. This technique helps in extracting keywords by understanding the semantic richness of words, almost like capturing the essence of words in a mathematical form, which can really pinpoint the key concepts in a text.\n",
        "\n",
        "---\n",
        "\n",
        "### Applications of Keyword Extraction\n",
        "\n",
        "It's important to understand why keyword extraction is necessary. It seems that this is not as interesting a task as extracting named entities or training a neural network to generate text. But it is still a very important task.\n",
        "\n",
        "So, you can use Keywords for:\n",
        "\n",
        "- **Information Retrieval**: Enhancing search engine performance by indexing documents based on their keywords.\n",
        "- **Content Summarization**: Generating concise summaries of large texts.\n",
        "- **Document Clustering and Classification**: Organizing documents into categories based on their keywords.\n",
        "- **Trend Analysis**: Identifying emerging trends in large datasets by analyzing frequently occurring keywords.\n",
        "基于图的算法（如TextRank、SingleRank和TopicRank）使用图结构来理解词语之间的关系，这些算法可以通过计算词语在文本中的连接强度来识别关键词。TextRank看词语的整体影响力，SingleRank则通过共现频率进一步加权，TopicRank则在分组主题之后进行排名。\n",
        "\n",
        "深度学习技术（如词嵌入）为关键词提取提供了更为智能和精确的方法。通过将词语转化为向量，算法不仅关注词语的频率，还可以捕捉其在文本中的深层语义，使得关键词提取更加精准。\n",
        "\n",
        "以上都是一些关键词提取的算法我个人认为首次学习不用花太多时间搞懂它是干啥的\n",
        "---"
      ],
      "metadata": {
        "id": "wyA_1V8spA4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset Overview and EDA（了解项目用到的数据集）\n",
        "\n",
        "Before we break down each algorithm and put it into practice, let's look at the data we have. We want to understand the structure, size, and nature of our dataset.\n",
        "\n",
        "The NIPS (NeurIPS) paper dataset comprises thousands of research papers from the annual Neural Information Processing Systems conference. These papers cover a wide array of topics in machine learning, artificial intelligence, and statistics. The dataset typically includes titles, abstracts, full text of the papers, authors, and sometimes the keywords provided by the authors.\n",
        "\n",
        "### Setting Up the Environment\n",
        "\n",
        "First, ensure you have imported all necessary libraries and have the dataset ready. Kaggle datasets can typically be accessed directly within Kaggle notebooks through relative paths. If you are running this notebook locally, make sure that the imported libraries are installed in your environment."
      ],
      "metadata": {
        "id": "vga5uEAcpA4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#上传数据集\n",
        "\n",
        "#1.上传kaggle的API key\n",
        "from google.colab import files\n",
        "files.upload()  # 运行后，Colab会提示你选择文件\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "TwpBdB1dfDlX",
        "outputId": "36171d96-525d-4232-f7d8-a9a41c3c33c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-326261a1-0191-4ee3-b9d4-9688cbc40505\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-326261a1-0191-4ee3-b9d4-9688cbc40505\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"dmhl1991121\",\"key\":\"85cca2fecbf2e0d6b5dd045dea8dedc7\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建Kaggle目录\n",
        "!mkdir -p ~/.kaggle"
      ],
      "metadata": {
        "id": "e3QHbT2tfpNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 移动上传的 kaggle.json 文件到正确的目录\n",
        "!mv kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "mvAxao8fgAnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置文件的权限\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "F8zRlg55gwAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la ~/.kaggle\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3vAiwXhtgjk",
        "outputId": "28c7fcd7-1a18-419e-8514-a1369b812237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwxr-xr-x 2 root root 4096 Jun 17 16:35 .\n",
            "drwx------ 1 root root 4096 Jun 17 16:23 ..\n",
            "-rw------- 1 root root   67 Jun 17 16:35 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#验证API是否导入成功\n",
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "906LT-zrhB9_",
        "outputId": "aff84993-1fe4-4d93-896d-54d55aac7fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                             title                                                     size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  -------------------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "rakeshkapilavai/extrovert-vs-introvert-behavior-data            Extrovert vs. Introvert Behavior Data                    31277  2025-06-13 14:26:48.303000          19056        416  1.0              \n",
            "bismasajjad/global-ai-job-market-and-salary-trends-2025         Global AI Job Market & Salary Trends 2025               529004  2025-06-01 07:20:49.537000           7392        119  0.9411765        \n",
            "adilshamim8/social-media-addiction-vs-relationships             Students' Social Media Addiction                          7851  2025-05-10 14:38:02.713000          18529        279  1.0              \n",
            "skullagos5246/upi-transactions-2024-dataset                     UPI Transactions 2024 Dataset                          5657850  2025-06-14 21:39:13.250000            651         22  1.0              \n",
            "therohithanand/used-car-price-prediction                        Used Car Price Prediction                               144387  2025-06-09 08:04:12.570000           2272         29  1.0              \n",
            "brendanartley/openfwi-preprocessed-72x72                        OpenFWI Preprocessed 72x72                         22114565718  2025-06-02 17:25:58.957000           3758         61  1.0              \n",
            "prajwaldongre/loan-application-and-transaction-fraud-detection  Loan Application & Transaction: Fraud Detection        8339124  2025-06-10 08:44:19.813000           1245         24  1.0              \n",
            "shalmamuji/personality-prediction-data-introvert-extrovert      Personality prediction data | introvert extrovert       164292  2025-06-12 10:38:45.273000            772         26  1.0              \n",
            "sahilislam007/sales-dataset                                     Sales Dataset                                             9016  2025-05-27 07:28:19.667000           1946         32  1.0              \n",
            "kanakbaghel/hospital-management-dataset                         Hospital Management Dataset                              11375  2025-05-30 14:40:55.287000           1574         24  0.9411765        \n",
            "samanfatima7/2020-2025-apple-stock-dataset                      2020-2025 Apple Stock Dataset                            52546  2025-06-03 11:57:03.600000           1114         26  0.9411765        \n",
            "abhishekdave9/digital-habits-vs-mental-health-dataset           Screen Time Impact on Mental Health                     559014  2025-06-13 11:42:28.647000            616         26  0.9411765        \n",
            "hbugrae/best-selling-steam-games-of-all-time                    Best-Selling Steam Games of All Time                    158051  2025-06-12 11:24:15.477000            966         23  1.0              \n",
            "flynn28/2025-premier-league-stats-matches-salaries              2025 Premier League: Stats, Matches, Salaries            59228  2025-05-26 01:25:53.030000           1407         30  1.0              \n",
            "shreyasdasari7/top-100-saas-companiesstartups                   Top 100 SaaS Companies/Startups 2025                      5050  2025-05-29 00:22:47.850000           1072         29  1.0              \n",
            "dansbecker/melbourne-housing-snapshot                           Melbourne Housing Snapshot                              461423  2018-06-05 12:52:24.087000         182563       1638  0.7058824        \n",
            "anirudhsub/twizzlerdata                                         Popularity of Twizzlers Dataset                            996  2025-06-13 22:01:17.120000            301         23  1.0              \n",
            "adilshamim8/rock-paper-scissors                                 Rock Paper Scissors SXSW: Hand Gesture Detection     210940029  2025-05-28 04:09:22.500000           1809         78  1.0              \n",
            "datasnaek/youtube-new                                           Trending YouTube Video Statistics                    210575746  2019-06-03 00:56:47.177000         274076       5688  0.7941176        \n",
            "zynicide/wine-reviews                                           Wine Reviews                                          53336293  2017-11-27 17:08:04.700000         327525       3743  0.7941176        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#从kaggle上下载数据集\n",
        "!kaggle datasets download -d nips-papers-1987-2019-updated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KovNsMvPssS1",
        "outputId": "0981811d-d59b-40b4-f052-3e3d70a95ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/metadata/dmhl1991121/nips-papers-1987-2019-updated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "根据以上代码结果可以看出无法顺利将数据集导入到colab，所以我选择手动从koggle上下载数据集然后再手动上传到../input/nips-papers-1987-2019-updated/  这个路径下"
      ],
      "metadata": {
        "id": "cTtYzU3fyUBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#创建路径\n",
        "import os\n",
        "\n",
        "# 创建指定路径\n",
        "path = '/content/input/nips-papers-1987-2019-updated/'\n",
        "os.makedirs(path, exist_ok=True)  # 如果路径已存在，则不会报错\n"
      ],
      "metadata": {
        "id": "QzcBpJaeyiMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 上传文件\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 将文件保存到指定路径\n",
        "for filename in uploaded.keys():\n",
        "    # 移动文件到指定路径\n",
        "    os.rename(filename, os.path.join(path, filename))\n",
        "\n",
        "print(\"文件已上传到：\", path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "jj2oNlLIyrgn",
        "outputId": "36878a9d-ac6b-41bf-a50d-edf99edebd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b69ecf47-1933-4f72-bb1f-2f6a3c473e6f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b69ecf47-1933-4f72-bb1f-2f6a3c473e6f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving authors.csv to authors.csv\n",
            "Saving papers.csv to papers.csv\n",
            "文件已上传到： /content/input/nips-papers-1987-2019-updated/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt#画图的库\n",
        "import seaborn as sns #画图使用的库\n",
        "from sklearn.feature_extraction.text import CountVectorizer#把文字装换成机器可以看懂的数字向量\n",
        "import numpy as np#数值运算的基础库\n",
        "\n",
        "# Display plots inline and set default figure size\n",
        "#%matplotlib inline 是 Jupyter/Colab 的“魔法命令”，作用是：图表直接显示在代码下面。设置图像默认的大小，方便观察。\n",
        "#plt.rcParams['figure.figsize']设置图像默认的大小，方便观察。\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Assuming the dataset is loaded into a Kaggle notebook with the filename 'nips_papers.csv'\n",
        "nips_papers = pd.read_csv('../content/drive/MyDrive/Kaggle/01Keyword_Extraction_NLP_Project_for_Beginners/papers.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-26T17:59:07.886037Z",
          "iopub.execute_input": "2024-04-26T17:59:07.886445Z",
          "iopub.status.idle": "2024-04-26T17:59:18.830569Z",
          "shell.execute_reply.started": "2024-04-26T17:59:07.886412Z",
          "shell.execute_reply": "2024-04-26T17:59:18.829444Z"
        },
        "trusted": true,
        "id": "XBFeO7c-pA4y"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers.head()"
      ],
      "metadata": {
        "id": "JJpuY9Cb-1bs",
        "outputId": "6525f5d4-54a6-46ac-f3d8-65cbf4f95efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   source_id  year                                              title  \\\n",
              "0         27  1987                         Bit-Serial Neural Networks   \n",
              "1         63  1987                        Connectivity Versus Entropy   \n",
              "2         60  1987        The Hopfield Model with Multi-Level Neurons   \n",
              "3         59  1987                               How Neural Nets Work   \n",
              "4         69  1987  Spatial Organization of Neural Networks: A Pro...   \n",
              "\n",
              "  abstract                                          full_text  \n",
              "0      NaN  573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...  \n",
              "1      NaN  1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...  \n",
              "2      NaN  278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...  \n",
              "3      NaN  442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...  \n",
              "4      NaN  740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-380d3044-36e8-4c38-8a9a-d46df64d8918\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27</td>\n",
              "      <td>1987</td>\n",
              "      <td>Bit-Serial Neural Networks</td>\n",
              "      <td>NaN</td>\n",
              "      <td>573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63</td>\n",
              "      <td>1987</td>\n",
              "      <td>Connectivity Versus Entropy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>1987</td>\n",
              "      <td>The Hopfield Model with Multi-Level Neurons</td>\n",
              "      <td>NaN</td>\n",
              "      <td>278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59</td>\n",
              "      <td>1987</td>\n",
              "      <td>How Neural Nets Work</td>\n",
              "      <td>NaN</td>\n",
              "      <td>442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69</td>\n",
              "      <td>1987</td>\n",
              "      <td>Spatial Organization of Neural Networks: A Pro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-380d3044-36e8-4c38-8a9a-d46df64d8918')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-380d3044-36e8-4c38-8a9a-d46df64d8918 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-380d3044-36e8-4c38-8a9a-d46df64d8918');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bca7acd2-a3e2-438f-8137-60c5385f10b7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bca7acd2-a3e2-438f-8137-60c5385f10b7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bca7acd2-a3e2-438f-8137-60c5385f10b7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "nips_papers",
              "summary": "{\n  \"name\": \"nips_papers\",\n  \"rows\": 9680,\n  \"fields\": [\n    {\n      \"column\": \"source_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1825,\n        \"min\": 1,\n        \"max\": 9406,\n        \"num_unique_values\": 4522,\n        \"samples\": [\n          5676,\n          2528,\n          5716\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 1987,\n        \"max\": 2019,\n        \"num_unique_values\": 33,\n        \"samples\": [\n          2018,\n          2002,\n          2013\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9680,\n        \"samples\": [\n          \"Label Embedding Trees for Large Multi-Class Tasks\",\n          \"Catastrophic Interference in Human Motor Learning\",\n          \"Provably Correct Automatic Sub-Differentiation for Qualified Programs\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6361,\n        \"samples\": [\n          \"We introduce algorithmic assurance, the problem of testing whether\\nmachine learning algorithms are conforming to their intended design\\ngoal. We address this problem by proposing an efficient framework\\nfor algorithmic testing. To provide assurance, we need to efficiently\\ndiscover scenarios where an algorithm decision deviates maximally\\nfrom its intended gold standard. We mathematically formulate this\\ntask as an optimisation problem of an expensive, black-box function.\\nWe use an active learning approach based on Bayesian optimisation\\nto solve this optimisation problem. We extend this framework to algorithms\\nwith vector-valued outputs by making appropriate modification in Bayesian\\noptimisation via the EXP3 algorithm. We theoretically analyse our\\nmethods for convergence. Using two real-world applications, we demonstrate\\nthe efficiency of our methods. The significance of our problem formulation\\nand initial solutions is that it will serve as the foundation in assuring\\nhumans about machines making complex decisions.\",\n          \"Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from high-dimensional  distributions in  Statistics and Machine learning. HMC is known to run very efficiently in practice and its popular second-order ``leapfrog\\\" implementation has long been conjectured to run in $d^{1/4}$ gradient evaluations. Here we show that this conjecture is true when sampling from strongly log-concave target distributions that satisfy a weak third-order regularity property associated with the input data.  Our regularity condition is weaker than the Lipschitz Hessian property and allows us to show faster convergence bounds for a much larger class of distributions than would be possible with the usual Lipschitz Hessian constant alone.  Important distributions that satisfy our regularity condition include posterior distributions used in Bayesian logistic regression for which the data satisfies an ``incoherence\\\" property. Our result compares favorably with the best available bounds for the class of strongly log-concave distributions, which grow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our simulations on synthetic data suggest that, when our regularity condition is satisfied, leapfrog HMC performs better than its competitors -- both in terms of accuracy and in terms of the number of gradient evaluations it requires.\",\n          \"Dependencies among neighbouring labels in a sequence is an important source of information for sequence labeling problems. However, only dependencies between adjacent labels are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we show that it is possible to design efficient inference algorithms for a conditional random field using features that depend on long consecutive label sequences (high-order features), as long as the number of distinct label sequences in the features used is small. This leads to efficient learning algorithms for these conditional random fields. We show experimentally that exploiting dependencies using high-order features can lead to substantial performance improvements for some problems and discuss conditions under which high-order features can be effective.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9673,\n        \"samples\": [\n          \"Nonparametric Multi-group Membership Model\\n\\nfor Dynamic Networks\\n\\nMyunghwan Kim\\nStanford University\\nStanford, CA 94305\\n\\nJure Leskovec\\n\\nStanford University\\nStanford, CA 94305\\n\\nmykim@stanford.edu\\n\\njure@cs.stanford.edu\\n\\nRelational data\\u2014like graphs, networks, and matrices\\u2014is often dynamic, where the relational struc-\\nture evolves over time. A fundamental problem in the analysis of time-varying network data is to\\nextract a summary of the common structure and the dynamics of the underlying relations between\\nthe entities. Here we build on the intuition that changes in the network structure are driven by dy-\\nnamics at the level of groups of nodes. We propose a nonparametric multi-group membership model\\nfor dynamic networks. Our model contains three main components: We model the birth and death of\\nindividual groups with respect to the dynamics of the network structure via a distance dependent In-\\ndian Buffet Process. We capture the evolution of individual node group memberships via a Factorial\\nHidden Markov model. And, we explain the dynamics of the network structure by explicitly mod-\\neling the connectivity structure of groups. We demonstrate our model\\u2019s capability of identifying the\\ndynamics of latent groups in a number of different types of network data. Experimental results show\\nthat our model provides improved predictive performance over existing dynamic network models on\\nfuture network forecasting and missing link prediction.\\n\\n1 Introduction\\n\\nStatistical analysis of social networks and other relational data is becoming an increasingly impor-\\ntant problem as the scope and availability of network data increases. Network data\\u2014such as the\\nfriendships in a social network\\u2014is often dynamic in a sense that relations between entities rise and\\ndecay over time. A fundamental problem in the analysis of such dynamic network data is to extract\\na summary of the common structure and the dynamics of the underlying relations between entities.\\n\\nAccurate models of structure and dynamics of network data have many applications. They allow us\\nto predict missing relationships [20, 21, 23], recommend potential new relations [2], identify clusters\\nand groups of nodes [1, 29], forecast future links [4, 9, 11, 24], and even predict group growth and\\nlongevity [15].\\n\\nHere we present a new approach to modeling network dynamics by considering time-evolving inter-\\nactions between groups of nodes as well as the arrival and departure dynamics of individual nodes\\nto these groups. We develop a dynamic network model, Dynamic Multi-group Membership Graph\\nModel, that identi\\ufb01es the birth and death of individual groups as well as the dynamics of node join-\\ning and leaving groups in order to explain changes in the underlying network linking structure. Our\\nnonparametric model considers an in\\ufb01nite number of latent groups, where each node can belong to\\nmultiple groups simultaneously. We capture the evolution of individual node group memberships\\nvia a Factorial Hidden Markov model. However, in contrast to recent works on dynamic network\\nmodeling [4, 5, 11, 12, 14], we explicitly model the birth and death dynamics of individual groups\\nby using a distance-dependent Indian Buffet Process [7]. Under our model only active/alive groups\\nin\\ufb02uence relationships in a network at a given time. Further innovation of our approach is that we\\nnot only model relations between the members of the same group but also account for links between\\nmembers and non-members. By explicitly modeling group lifespan and group connectivity structure\\nwe achieve greater modeling \\ufb02exibility, which leads to improved performance on link prediction and\\nnetwork forecasting tasks as well as to increased interpretability of obtained results.\\n\\n1\\n\\n\\fThe rest of the paper is organized as follows: Section 2 provides the background and Section 3\\npresents our generative model and motivates its parametrization. We discuss related work in Sec-\\ntion 4 and present model inference procedure in Section 5. Last, in Section 6 we provide experi-\\nmental results as well as analysis of the social network from the movie, The Lord of the Rings.\\n\\n2 Models of Dynamic Networks\\n\\nFirst, we describe general components of modern dynamic network models [4, 5, 11, 14]. In the\\nnext section we will then describe our own model and point out the differences to the previous work.\\n\\nDynamic networks are generally conceptualized as discrete time series of graphs on a \\ufb01xed set of\\nnodes N . Dynamic network Y is represented as a time series of adjacency matrices Y (t) for each\\ntime t = 1, 2, \\u00b7 \\u00b7 \\u00b7 , T . In this work, we limit our focus to unweighted directed as well as undirected\\nnetworks. So, each Y (t) is a N \\u00d7 N binary matrix where Y (t)\\nij = 1 if a link from node i to j exists\\nat time t and Y (t)\\n\\nij = 0 otherwise.\\n\\nEach node i of the network is associated with a number of latent binary features that govern the\\ninteraction dynamics with other nodes of the network. We denote the binary value of feature k of\\nnode i at time t by z(t)\\nik \\u2208 {0, 1}. Such latent features can be viewed as assigning nodes to multi-\\nple overlapping, latent clusters or groups [1, 21]. In our work, we interpret these latent features as\\nmemberships to latent groups such as social communities of people with the same interests or hob-\\nbies. We allow each node to belong to multiple groups simultaneously. We model each node-group\\nmembership using a separate Bernoulli random variable [17, 22, 29]. This is in contrast to mixed-\\nmembership models where the distribution over individual node\\u2019s group memberships is modeled\\nusing a multinomial distribution [1, 5, 12]. The advantage of our multiple-membership approach\\nis as follows. Mixed-membership models (i.e., multinomial distribution over group memberships)\\nessentially assume that by increasing the amount of node\\u2019s membership to some group k, the same\\nnode\\u2019s membership to some other group k\\u2032 has to decrease (due to the condition that the probabilities\\nnormalize to 1). On the other hand, multiple-membership models do not suffer from this assumption\\nand allow nodes to truely belong to multiple groups. Furthermore, we consider a nonparametric\\nmodel of groups which does not restrict the number of latent groups ahead of time. Hence, our\\nmodel adaptively learns the appropriate number of latent groups for a given network at a given time.\\n\\nIn dynamic network models, one also speci\\ufb01es a process by which nodes dynamically join and leave\\ngroups. We assume that each node i can join or leave a given group k according to a Markov model.\\nHowever, since each node can join multiple groups independently, we naturally consider factorial\\nhidden Markov models (FHMM) [8], where latent group membership of each node independently\\nevolves over time. To be concrete, each membership z(t)\\nik evolves through a 2-by-2 Markov transition\\nprobability matrix Q(t)\\n= r), where\\nr, s \\u2208 {0 = non-member, 1 = member}.\\n\\nk [r, s] corresponds to P (z(t)\\n\\nk where each entry Q(t)\\n\\nik = s|z(t\\u22121)\\n\\nik\\n\\nNow, given node group memberships z(t)\\nik at time t one also needs to specify the process of link\\ngeneration. Links of the network realize according to a link function f (\\u00b7). A link from node i to\\nnode j at time t occurs with probability determined by the link function f (z(t)\\nj\\u00b7 ). In our model,\\nwe develop a link function that not only accounts for links between group members but also models\\nlinks between the members and non-members of a given group.\\n\\ni\\u00b7 , z(t)\\n\\n3 Dynamic Multi-group Membership Graph Model\\n\\nNext we shall describe our Dynamic Multi-group Membership Graph Model (DMMG) and point out\\nthe differences with the previous work. In our model, we pay close attention to the three processes\\ngoverning network dynamics: (1) birth and death dynamics of individual groups, (2) evolution of\\nmemberships of nodes to groups, and (3) the structure of network interactions between group mem-\\nbers as well as non-members. We now proceed by describing each of them in turn.\\n\\nModel of active groups. Links of the network are in\\ufb02uenced not only by nodes changing member-\\nships to groups but also by the birth and death of groups themselves. New groups can be born and\\nold ones can die. However, without explicitly modeling group birth and death there exists ambiguity\\n\\n2\\n\\n\\fbetween group membership change and the birth/death of groups. For example, consider two dis-\\njoint groups k and l such that their lifetimes and members do not overlap. In other words, group l is\\nborn after group k dies out. However, if group birth and death dynamics is not explicitly modeled,\\nthen the model could interpret that the two groups correspond to a single latent group where all the\\nmembers of k leave the group before the members of l join the group. To resolve this ambiguity we\\ndevise an explicit model of birth/death dynamics of groups by introducing a notion of active groups.\\n\\nUnder our model, a group can be in one of two states: it can be either active (alive) or inactive (not\\nyet born or dead). However, once a group becomes inactive, it can never be active again. That is,\\nonce a group dies, it can never be alive again. To ensure coherence of group\\u2019s state over time, we\\nbuild on the idea of distance-dependent Indian Buffet Processes (dd-IBP) [7]. The IBP is named\\nafter a metaphorical process that gives rise to a probability distribution, where customers enter an\\nIndian Buffet restaurant and sample some subset of an in\\ufb01nitely long sequence of dishes. In the\\ncontext of networks, nodes usually correspond to \\u2018customers\\u2019 and latent features/groups correspond\\nto \\u2018dishes\\u2019. However, we apply dd-IBP in a different way. We regard each time step t as a \\u2018customer\\u2019\\nthat samples a set of active groups Kt. So, at the \\ufb01rst time step t = 1, we have P oisson(\\u03bb) number\\nof groups that are initially active, i.e., |K1| \\u223c P oisson(\\u03bb). To account for death of groups we\\nthen consider that each active group at time t \\u2212 1 can become inactive at the next time step t with\\nprobability \\u03b3. On the other hand, P oisson(\\u03b3\\u03bb) new groups are also born at time t. Thus, at each\\ntime currently active groups can die, while new ones can also be born. The hyperparameter \\u03b3\\ncontrols for how often new groups are born and how often old ones die. For instance, there will be\\nalmost no newborn or dead groups if \\u03b3 \\u2248 1, while there would be no temporal group coherence and\\npractically all the groups would die between consecutive time steps if \\u03b3 = 0.\\n\\nFigure 1(a) gives an example of the above process. Black circles indicate active groups and white\\ncircles denote inactive (not yet born or dead) groups. Groups 1 and 3 exist at t = 1 and Group 2\\nis born at t = 2. At t = 3, Group 3 dies but Group 4 is born. Without our group activity model,\\nGroup 3 could have been reused with a completely new set of members and Group 4 would have\\nnever been born. Our model can distinguish these two disjoint groups.\\n\\nFormally, we denote the number of active groups at time t by Kt = |Kt|. We also denote the state\\n(active/inactive) of group k at time t by W (t)\\nk = 1{k \\u2208 Kt}. For convenience, we also de\\ufb01ne a set\\nof newly active groups at time t be K+\\n\\nt = |K+\\nt |.\\nPutting it all together we can now fully describe the process of group birth/death as follows:\\n\\nk = 0 \\u2200t\\u2032 < t} and K +\\n\\nk = 1, W (t\\u2032)\\n\\nt = {k|W (t)\\n\\nfor t = 1\\nfor t > 1\\n\\nK +\\n\\nP oisson (\\u03b3\\u03bb) ,\\n\\nt \\u223c(cid:26)P oisson (\\u03bb) ,\\nk \\u223c\\uf8f1\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3\\n\\nBernoulli(1 \\u2212 \\u03b3)\\n1,\\n0,\\n\\nW (t)\\n\\nk\\n\\n= 1\\nt\\u2032=1 K +\\n\\nif W (t\\u22121)\\n\\nif Pt\\u22121\\n\\notherwise .\\n\\nt\\u2032 < k \\u2264Pt\\n\\nt\\u2032=1 K +\\n\\nt\\u2032\\n\\n(1)\\n\\nNote that under this model an in\\ufb01nite number of active groups can exist. This means our model au-\\ntomatically determines the right number of active groups and each node can belong to many groups\\nsimultaneously. We now proceed by describing the model of node group membership dynamics.\\n\\nDynamics of node group memberships. We capture the dynamics of nodes joining and leaving\\ngroups by assuming that latent node group memberships form a Markov chain. In this framework,\\nnode memberships to active groups evolve through time according to Markov dynamics:\\n\\nP (z(t)\\n\\nik |z(t\\u22121)\\n\\nik\\n\\n) = Qk =(cid:18) 1 \\u2212 ak\\n\\nbk\\n\\nak\\n\\n1 \\u2212 bk (cid:19) ,\\n\\nwhere matrix Qk[r, s] denotes a Markov transition from state r to state s, which can be a \\ufb01xed\\nparameter, group speci\\ufb01c, or otherwise domain dependent as long as it de\\ufb01nes a Markov transition\\nmatrix. Thus, the transition of node\\u2019s i membership to active group k can be de\\ufb01ned as follows:\\n\\nak, bk \\u223c Beta(\\u03b1, \\u03b2), z(t)\\n\\nik \\u223c W (t)\\n\\nk\\n\\n\\u00b7 Bernoulli(cid:18)a\\n\\n(t\\u22121)\\nik\\n\\n1\\u2212z\\nk\\n\\n(1 \\u2212 bk)z\\n\\n(t\\u22121)\\n\\nik (cid:19) .\\n\\n(2)\\n\\nTypically, \\u03b2 > \\u03b1, which ensures that group\\u2019s memberships are not too volatile over time.\\n\\n3\\n\\n\\f(a) Group activity model\\n\\n(b) Link function model\\n\\nFigure 1: (a) Birth and death of groups: Black circles represent active and white circles represent inactive\\n(unborn or dead) groups. A dead group can never become active again.\\ndenotes\\nbinary node group memberships. Entries of link af\\ufb01nity matrix \\u0398k denotes linking parameters between all 4\\ncombinations of members (z(t)\\nij , individual\\naf\\ufb01nities \\u0398k[z(t)\\n\\ni = 0). To obtain link probability p(t)\\n\\ni = 1) and non-members (z(t)\\n\\n] are combined using a logistic function g(\\u00b7)\\n\\n(b) Link function: z(t)\\n\\n, z(t)\\n\\ni\\n\\nj\\n\\nj\\n\\n.\\n\\nRelationship between node group memberships and links of the network. Last, we describe the\\npart of the model that establishes the connection between node\\u2019s memberships to groups and the\\nlinks of the network. We achieve this by de\\ufb01ning a link function f (i, j), which for given a pair of\\nnodes i, j determines their interaction probability p(t)\\n\\nij based on their group memberships.\\n\\nWe build on the Multiplicative Attribute Graph model [16, 18], where each group k is associated\\nwith a link af\\ufb01nity matrix \\u0398k \\u2208 R2\\u00d72. Each of the four entries of the link af\\ufb01nity matrix captures\\nthe tendency of linking between group\\u2019s members, members and non-members, as well as non-\\nmembers themselves. While traditionally link af\\ufb01nities were considered to be probabilities, we\\nrelax this assumption by allowing af\\ufb01nities to be arbitrary real numbers and then combine them\\nthrough a logistic function to obtain a \\ufb01nal link probability.\\n\\nThe model is illustrated in Figure 1(b). Given group memberships z(t)\\njk of nodes i and j at\\ntime t the binary indicators \\u201cselect\\u201d an entry \\u0398k[z(t)\\njk ] of matrix \\u0398k. This way linking tendency\\nfrom node i to node j is re\\ufb02ected based on their membership to group k. We then determine the\\noverall link probability p(t)\\n\\nij by combining the link af\\ufb01nities via a logistic function g(\\u00b7)1. Thus,\\n\\nik and z(t)\\n\\nik , z(t)\\n\\nij = f (z(t)\\np(t)\\n\\ni\\u00b7 , z(t)\\n\\nj\\u00b7 ) = g \\u01ebt +\\n\\n\\u0398k[z(t)\\n\\nik , z(t)\\n\\njk ]! , Yij \\u223c Bernoulli(p(t)\\n\\nij )\\n\\n(3)\\n\\n\\u221e\\n\\nXk=1\\n\\nwhere \\u01ebt is a density parameter that re\\ufb02ects the varying link density of network over time.\\n\\nNote that due to potentially in\\ufb01nite number of groups the sum of an in\\ufb01nite number of link af\\ufb01nities\\nmay not be tractable. To resolve this, we notice that for a given \\u0398k subtracting \\u0398k[0, 0] from all its\\nentries and then adding this value to \\u01ebt does not change the overall linking probability p(t)\\nij . Thus, we\\ncan set \\u0398k[0, 0] = 0 and then only a \\ufb01nite number of af\\ufb01nities selected by z(t)\\nik have to be considered.\\nFor all other entries of \\u0398k we use N (0, \\u03bd2) as a prior distribution.\\n\\nTo sum up, Figure 2 illustrates the three components of the DMMG in a plate notation. Group\\u2019s\\nstate W (t)\\nik is de\\ufb01ned as\\nthe FHMM over active groups. Then, the link between nodes i and j is determined based on the\\ngroups they belong to and the corresponding group link af\\ufb01nity matrices \\u0398.\\n\\nis determined by the dd-IBP process and each node-group membership z(t)\\n\\nk\\n\\n4 Related Work\\n\\nClassically, non-Bayesian approaches such as exponential random graph models [10, 27] have been\\nused to study dynamic networks. On the other hand, in the Bayesian approaches to dynamic network\\nanalysis latent variable models have been most widely used. These approaches differ by the struc-\\nture of the latent space that they assume. For example, euclidean space models [13, 24] place nodes\\n\\n1g(x) = exp(x)/(1 + exp(x))\\n\\n4\\n\\n\\fFigure 2: Dynamic Multi-group Membership Graph Model. Network Y depends on each node\\u2019s group mem-\\nberships Z and active groups W . Links of Y appear via link af\\ufb01nities \\u0398.\\n\\nin a low dimensional Euclidean space and the network evolution is then modeled as a regression\\nproblem of node\\u2019s future latent location. In contrast, our model uses HMMs, where latent vari-\\nables stochastically depend on the state at the previous time step. Related to our work are dynamic\\nmixed-membership models where a node is probabilistically allocated to a set of latent features. Ex-\\namples of this model include the dynamic mixed-membership block model [5, 12] and the dynamic\\nin\\ufb01nite relational model [14]. However, the critical difference here is that our model uses multi-\\nmemberships where node\\u2019s membership to one group does not limit its membership to other groups.\\nProbably most related to our work here are DRIFT [4] and LFP [11] models. Both of these models\\nconsider Markov switching of latent multi-group memberships over time. DRIFT uses the in\\ufb01nite\\nfactorial HMM [6], while LFP adds \\u201csocial propagation\\u201d to the Markov processes so that network\\nlinks of each node at a given time directly in\\ufb02uence group memberships of the corresponding node\\nat the next time. Compared to these models, we uniquely incorporate the model of group birth and\\ndeath and present a novel and powerful linking function.\\n\\n5 Model Inference via MCMC\\n\\nWe develop a Markov chain Monte Carlo (MCMC) procedure to approximate samples from the\\nposterior distribution of the latent variables in our model. More speci\\ufb01cally, there are \\ufb01ve types\\nof variables that we need to sample: node group memberships Z = {z(t)\\nik }, group states W =\\n{W (t)\\nk }, group membership transitions Q = {Qk}, link af\\ufb01nities \\u0398 = {\\u0398k}, and density parameters\\n\\u01eb = {\\u01ebt}. By sampling each type of variables while \\ufb01xing all the others, we end up with many\\nsamples representing the posterior distribution P (Z, W, Q, \\u0398, \\u01eb|Y, \\u03bb, \\u03b3, \\u03b1, \\u03b2). We shall now explain\\na sampling strategy for each varible type.\\n\\nSampling node group memberships Z. To sample node group membership z(t)\\nik , we use the\\nforward-backward recursion algorithm [26]. The algorithm \\ufb01rst de\\ufb01nes a deterministic forward\\npass which runs down the chain starting at time one, and at each time point t collects information\\nfrom the data and parameters up to time t in a dynamic programming cache. A stochastic backward\\npass starts at time T and samples each z(t)\\nik in backwards order using the information collected dur-\\ning the forward pass. In our case, we only need to sample z(T B\\nk indicate the\\nbirth time and the death time of group k. Due to space constraints, we discuss further details in the\\nextended version of the paper [19].\\n\\nk and T D\\n\\nwhere T B\\n\\nk :T D\\nk )\\n\\nik\\n\\nSampling group states W . To update active groups, we use the Metropolis-Hastings algorithm\\nwith the following proposal distribution P (W \\u2192 W \\u2032): We add a new group, remove an existing\\ngroup, or update the life time of an active group with the same probability 1/3. When adding a new\\ngroup k\\u2032 we select the birth and death time of the group at random such that 1 \\u2264 T B\\nk\\u2032 \\u2264 T D\\nk\\u2032 \\u2264 T .\\nFor removing groups we randomly pick one of existing groups k\\u2032\\u2032 and remove it by setting W (t)\\nk\\u2032\\u2032 = 0\\nfor all t. Finally, to update the birth and death time of an existing group, we select an existing group\\nand propose new birth and death time of the group at random. Once new state vector W \\u2032 is proposed\\nwe accept it with probability\\n\\nmin(cid:18)1,\\n\\nP (Y |W \\u2032)P (W \\u2032|\\u03bb, \\u03b3)P (W \\u2032 \\u2192 W )\\n\\nP (Y |W )P (W |\\u03bb, \\u03b3)P (W \\u2192 W \\u2032) (cid:19) .\\n\\n(4)\\n\\nWe compute P (W |\\u03bb, \\u03b3) and P (W \\u2032 \\u2192 W ) in a closed form, while we approximate the posterior\\nP (Y |W ) by sampling L Gibbs samples while keeping W \\ufb01xed.\\n\\n5\\n\\n\\fSampling group membership transition matrix Q. Beta distribution is a conjugate prior of\\nBernoulli distribution and thus we can sample each ak and bk in Qk directly from the posterior\\ndistribution: ak \\u223c Beta(\\u03b1 + N01,k, \\u03b2 + N00,k) and bk \\u223c Beta(\\u03b1 + N10,k, \\u03b2 + N11,k), where Nrs,k\\nis the number of nodes that transition from state r to s in group k (r, s \\u2208 {0 = non-member, 1 =\\nmember}).\\n\\nSampling link af\\ufb01nities \\u0398. Once node group memberships Z are determined, we update the entries\\nof link af\\ufb01nity matrices \\u0398k. Direct sampling of \\u0398 is intractable because of non-conjugacy of the\\nlogistic link function. An appropriate method in such case would be the Metropolis-Hastings that\\naccepts or rejects the proposal based on the likelihood ratio. However, to avoid low acceptance\\nrates and quickly move toward the mode of the posterior distribution, we develop a method based\\non Hybrid Monte Carlo (HMC) sampling [3]. We guide the sampling using the gradient of log-\\nlikelihood function with respect to each \\u0398k. Because links Y (t)\\nij are generated independently given\\ngroup memberships Z, the gradient with respect to \\u0398k[x, y] can be computed by\\n\\n\\u2212\\n\\n1\\n2\\u03c32 \\u03982\\n\\nk +Xi,j,t(cid:16)Y (t)\\n\\nij \\u2212 p(t)\\n\\nik = x, z(t)\\n\\njk = y} .\\n\\nij (cid:17) 1{z(t)\\n\\n(5)\\n\\nUpdating density parameter \\u01eb. Parameter vector \\u01eb is de\\ufb01ned over a \\ufb01nite dimension T . Therefore,\\nwe can update \\u01eb by maximizing the log-likelihood given all the other variables. We compute the\\ngradient update for each \\u01ebt and directly update \\u01ebt via a gradient step.\\n\\nUpdating hyperparameters. The number of groups over all time periods is given by a Poisson\\ndistribution with parameter \\u03bb (1 + \\u03b3 (T \\u2212 1)). Hence, given \\u03b3 we sample \\u03bb by using a Gamma\\nconjugate prior. Similarly, we can use the Beta conjugate prior for the group death process (i.e.,\\nBernoulli distribution) to sample \\u03b3. However, hyperparameters \\u03b1 and \\u03b2 do not have a conjugate\\nprior, so we update them by using a gradient method based on the sampled values of ak and bk.\\n\\nij\\n\\nTime complexity of model parameter estimation. Last, we brie\\ufb02y comment on the time com-\\nplexity of our model parameter estimation procedure. Each sample z(t)\\nik requires computation of\\nlink probability p(t)\\nfor all j 6= i. Since the expected number of active groups at each time is \\u03bb,\\nthis requires O(\\u03bbN 2T ) computations of p(t)\\nij . By caching the sum of link af\\ufb01nities between every\\npair of nodes sampling Z as well as W requires O(\\u03bbN 2T ) time. Sampling \\u0398 and \\u01eb also requires\\nO(\\u03bbN 2T ) because the gradient of each p(t)\\nij needs to be computed. Overall, our approach takes\\nO(\\u03bbN 2T ) to obtain a single sample, while models that are based on the interaction matrix between\\nall groups [4, 5, 11] require O(K 2N 2T ), where K is the expected number of groups. Furthermore,\\nit has been shown that O(log N ) groups are enough to represent networks [16, 18]. Thus, in practice\\nK (i.e., \\u03bb) is of order log N and the running time for each sample is O(N 2T log N ).\\n\\n6 Experiments\\n\\nWe evaluate our model on three different tasks. For quantitative evaluation, we perform missing link\\nprediction as well as future network forecasting and show our model gives favorable performance\\nwhen compared to current dynamic and static network models. We also analyze the dynamics of\\ngroups in a dynamic social network of characters in a movie \\u201cThe Lord of the Rings: The Two\\nTowers.\\u201d\\n\\nExperimental setup. For the two prediction experiments, we use the following three datasets. First,\\nthe NIPS co-authorships network connects two people if they appear on the same publication in\\nthe NIPS conference in a given year. Network spans T =17 years (1987 to 2003). Following [11]\\nwe focus on a subset of 110 most connected people over all time periods. Second, the DBLP co-\\nauthorship network is obtained from 21 Computer Science conferences from 2000 to 2009 (T =\\n10) [28]. We focus on 209 people by taking 7-core of the aggregated network for the entire time.\\nThird, the INFOCOM dataset represents the physical proximity interactions between 78 students at\\nthe 2006 INFOCOM conference, recorded by wireless detector remotes given to each attendee [25].\\nAs in [11] we use the processed data that removes inactive time slices to have T =50.\\n\\nTo evaluate the predictive performance of our model, we compare it to three baseline models. For\\na naive baseline model, we regard the relationship between each pair of nodes as the instance of\\n\\n6\\n\\n\\fModel\\n\\nNaive\\nLFRM\\nDRIFT\\n\\nDMMG\\n\\nTestLL\\n\\n-2030\\n-880\\n-758\\n\\n\\u2212624\\n\\nNIPS\\nAUC\\n\\n0.808\\n0.777\\n0.866\\n0.916\\n\\nF1\\n\\nTestLL\\n\\n-12051\\n0.177\\n-3783\\n0.195\\n-3108\\n0.296\\n0.434 \\u22122684\\n\\nDBLP\\nAUC\\n\\n0.814\\n0.784\\n0.916\\n0.939\\n\\nINFOCOM\\n\\nF1\\n\\nTestLL\\n\\nAUC\\n\\nF1\\n\\n-17821\\n0.300\\n-8689\\n0.146\\n-6654\\n0.421\\n0.492 \\u22126422\\n\\n0.677\\n0.946\\n0.973\\n0.976\\n\\n0.252\\n0.703\\n0.757\\n0.764\\n\\nTable 1: Missing link prediction. We bold the performance of the best scoring method. Our DMMG performs\\nthe best in all cases. All improvements are statistically signi\\ufb01cant at 0.01 signi\\ufb01cance level.\\n\\nindependent Bernoulli distribution with Beta(1, 1) prior. Thus, for a given pair of nodes, the link\\nprobability at each time equals to the expected probability from the posterior distribution given net-\\nwork data. Second baseline is LFRM [21], a model of static networks. For missing link prediction,\\nwe independently \\ufb01t LFRM to each snapshot of dynamic networks. For network forecasting task,\\nwe \\ufb01t LFRM to the most recent snapshot of a network. Even though LFRM does not capture time\\ndynamics, we consider this to be a strong baseline model. Finally, for the comparison with dynamic\\nnetwork models, we consider two recent state of the art models. The DRIFT model [4] is based\\non an in\\ufb01nite factorial HMM and authors kindly shared their implementation. We also consider the\\nLFP model [11] for which we were not able to obtain the implementation, but since we use the same\\ndatasets, we compare performance numbers directly with those reported in [11].\\n\\nTo evaluate predictive performance, we use various standard evaluation metrics. First, to assess\\ngoodness of inferred probability distributions, we report the log-likelihood of held-out edges. Sec-\\nond, to verify the predictive performance, we compute the area under the ROC curve (AUC). Last,\\nwe also report the maximum F1-score (F1) by scanning over all possible precision/recall thresholds.\\n\\nTask 1: Predicting missing links. To generate the datasets for the task of missing link prediction,\\nwe randomly hold out 20% of node pairs (i.e., either link or non-link) throughout the entire time\\nperiod. We then run each model to obtain 400 samples after 800 burn-in samples for each of 10\\nMCMC chains. Each sample gives a link probability for a given missing entry, so the \\ufb01nal link\\nprobability of a missing entry is computed by averaging the corresponding link probability over all\\nthe samples. This \\ufb01nal link probability provides the evaluation metric for a given missing data entry.\\n\\nTable 1 shows average evaluation metrics for each model and dataset over 10 runs. We also compute\\nthe p-value on the difference between two best results for each dataset and metric. Overall, our\\nDMMG model signi\\ufb01cantly outperforms the other models in every metric and dataset. Particularly\\nin terms of F1-score we gain up to 46.6% improvement over the other models.\\n\\nBy comparing the naive model and LFRM, we observe that LFRM performs especially poorly\\ncompared to the naive model in two networks with few edges (NIPS and DBLP). Intuitively this\\nmakes sense because due to the network sparsity we can obtain more information from the temporal\\ntrajectory of each link than from each snapshot of network. However, both DRIFT and DMMG\\nsuccessfully combine the temporal and the network information which results in better predictive\\nperformance. Furthermore, we note that DMMG outperforms the other models by a larger margin\\nas networks get sparser. DMMG makes better use of temporal information because it can explicitly\\nmodel temporally local links through active groups.\\n\\nLast, we also compare our model to the LFP model. The LFP paper reports AUC ROC score of\\n\\u223c0.85 for NIPS and \\u223c0.95 for INFOCOM on the same task of missing link prediction with 20%\\nheld-out missing data [11]. Performance of our DMMG on these same networks under the same\\nconditions is 0.916 for NIPS and 0.976 for INFOCOM, which is a strong improvement over LFP.\\n\\nTask 2: Future network forecasting. Here we are given a dynamic network up to time Tobs and\\nthe goal is to predict the network at the next time Tobs + 1. We follow the experimental protocol\\ndescribed in [4, 11]: We train the models on \\ufb01rst Tobs networks, \\ufb01x the parameters, and then for\\neach model we run MCMC sampling one time step into the future. For each model and network,\\nwe obtain 400 samples with 10 different MCMC chains, resulting in 400K network samples. These\\nnetwork samples provide a probability distribution over links at time Tobs + 1.\\n\\nTable 2 shows performance averaged over different Tobs values ranging from 3 to T -1. Overall,\\nDMMG generally exhibits the best performance, but performance results seem to depend on the\\ndataset. DMMG performs the best at 0.001 signi\\ufb01cance level in terms of AUC and F1 for the NIPS\\ndataset, and at 0.05 level for the INFOCOM dataset. While DMMG improves performance on AUC\\n\\n7\\n\\n\\fModel\\n\\nNaive\\nLFRM\\nDRIFT\\n\\nTestLL\\n\\n-547\\n-356\\n\\u2212148\\n\\nDMMG\\n\\n-170\\n\\nNIPS\\nAUC\\n\\n0.524\\n0.398\\n0.672\\n0.732\\n\\nF1\\n\\n0.130\\n0.011\\n0.084\\n0.196\\n\\nTestLL\\n\\n-3248\\n-1680\\n\\u22121324\\n\\nDBLP\\nAUC\\n0.668\\n0.492\\n0.650\\n\\n-1347\\n\\n0.652\\n\\nINFOCOM\\n\\nF1\\n\\nTestLL\\n\\nAUC\\n\\nF1\\n\\n0.243\\n0.024\\n0.122\\n0.245\\n\\n-774\\n-760\\n-661\\n\\n\\u2212625\\n\\n0.673\\n0.640\\n0.782\\n0.804\\n\\n0.270\\n0.248\\n0.381\\n0.392\\n\\nTable 2: Future network forecasting. DMMG performs best on NIPS and INFOCOM while results on DBLP\\nare mixed.\\n\\nhaldir\\ngandalf\\nmerry\\nfrodo\\nsam\\ngollum\\npippin\\naragorn\\nlegolas\\ngimli\\nsaruman\\neowyn\\neomer\\ntheoden\\ngrima\\nhama\\nfaramir\\narwen\\nelrond\\ngaladriel\\nmadril\\n\\nhaldir\\ngandalf\\nmerry\\nfrodo\\nsam\\ngollum\\npippin\\naragorn\\nlegolas\\ngimli\\nsaruman\\neowyn\\neomer\\ntheoden\\ngrima\\nhama\\nfaramir\\narwen\\nelrond\\ngaladriel\\nmadril\\n\\nhaldir\\ngandalf\\nmerry\\nfrodo\\nsam\\ngollum\\npippin\\naragorn\\nlegolas\\ngimli\\nsaruman\\neowyn\\neomer\\ntheoden\\ngrima\\nhama\\nfaramir\\narwen\\nelrond\\ngaladriel\\nmadril\\n\\n 1\\n\\n 2\\n\\n 3\\n\\n 4\\n\\n 5\\n\\n 1\\n\\n 2\\n\\n 3\\n\\n 4\\n\\n 5\\n\\n 1\\n\\n 2\\n\\n 3\\n\\n 4\\n\\n 5\\n\\n(a) Group 1\\n\\n(b) Group 2\\n\\n(c) Group 3\\n\\nFigure 3: Group arrival and departure dynamics of different characters in the Lord of the Rings. Dark areas in\\nthe plots correspond to a give node\\u2019s (y-axis) membership to each group over time (x-axis)\\n\\n.\\n\\n(9%) and F1 (133%), DRIFT achieves the best log-likelihood on the NIPS dataset. In light of our\\nprevious observations, we conjecture that this is due to change in network edge density between\\ndifferent snapshots. On the DBLP dataset, DRIFT gives the best log-likelihood, the naive model\\nperforms best in terms of AUC, and DMMG is the best on F1 score. However, in all cases of DBLP\\ndataset, the differences are not statistically signi\\ufb01cant. Overall, DMMG performs the best on NIPS\\nand INFOCOM and provides comparable performance on DBLP.\\n\\nTask 3: Case study of \\u201cThe Lord of the Rings: The Two Towers\\u201d social network. Last, we also\\ninvestigate groups identi\\ufb01ed by our model on a dynamic social network of characters in a movie,\\nThe Lord of the Rings: The Two Towers. Based on the transcript of the movie we created a dynamic\\nsocial network on 21 characters and T =5 time epochs, where we connect a pair of characters if they\\nco-appear inside some time window.\\n\\nWe \\ufb01t our model to this network and examine the results in Figure 3. Our model identi\\ufb01ed three\\ndynamic groups, which all nicely correspond to the Lord of the Rings storyline. For example,\\nthe core of Group 1 corresponds to Aragorn, elf Legolas, dwarf Gimli, and people in Rohan who\\nin the end all \\ufb01ght against the Orcs. Similarly, Group 2 corresponds to hobbits Sam, Frodo and\\nGollum on their mission to destroy the ring in Mordor, and are later joined by Faramir and ranger\\nMadril. Interestingly, Group 3 evolving around Merry and Pippin only forms at t=2 when they start\\ntheir journey with Treebeard and later \\ufb01ght against wizard Saruman. While the \\ufb01ght occurs in two\\nseparate places we \\ufb01nd that some scenes are not distinguishable, so it looks as if Merry and Pippin\\nfought together with Rohan\\u2019s army against Saruman\\u2019s army.\\n\\nAcknowledgments\\n\\nWe thank Creighton Heaukulani and Zoubin Ghahramani for sharing data and code. This research\\nhas been supported in part by NSF IIS-1016909, CNS-1010921, IIS-1149837, IIS-1159679, IARPA\\nAFRL FA8650-10-C-7058, Okawa Foundation, Docomo, Boeing, Allyes, Volkswagen, Intel, Alfred\\nP. Sloan Fellowship and the Microsoft Faculty Fellowship.\\n\\nReferences\\n\\n[1] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. Xing. Mixed membership stochastic blockmodels.\\n\\nJMLR, 9, 2008.\\n\\n[2] L. Backstrom and J. Leskovec. Supervised random walks: Predicting and recommending links in social\\n\\nnetworks. In WSDM, 2011.\\n\\n8\\n\\n\\f[3] S. Duane, A. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid monte carlo. Physics Letter B,\\n\\n195(2):216\\u2013222, 1987.\\n\\n[4] J. Foulds, A. U. Asuncion, C. DuBois, C. T. Butts, and P. Smyth. A dynamic relational in\\ufb01nite feature\\n\\nmodel for longitudinal social networks. In AISTATS, 2011.\\n\\n[5] W. Fu, L. Song, and E. P. Xing. Dynamic mixed membership blockmodel for evolving networks.\\n\\nIn\\n\\nICML, 2009.\\n\\n[6] J. V. Gael, Y. W. Teh, , and Z. Ghahramani. The in\\ufb01nite factorial hidden markov model. In NIPS, 2009.\\n\\n[7] S. J. Gershman, P. I. Frazier, and D. M. Blei. Distance dependent in\\ufb01nite latent feature models.\\n\\narXiv:1110.5454, 2012.\\n\\n[8] Z. Ghahramani and M. I. Jordan. Factorial hidden markov models. Machine Learning, 29(2-3):245\\u2013273,\\n\\n1997.\\n\\n[9] F. Guo, S. Hanneke, W. Fu, and E. P. Xing. Recovering temporally rewiring networks: a model-based\\n\\napproach. In ICML, 2007.\\n\\n[10] S. Hanneke, W. Fu, and E. P. Xing. Discrete temporal models of social networks. Electron. J. Statist.,\\n\\n4:585\\u2013605, 2010.\\n\\n[11] C. Heaukulani and Z. Ghahramani. Dynamic probabilistic models for latent feature propagation in social\\n\\nnetworks. In ICML, 2013.\\n\\n[12] Q. Ho, L. Song, and E. P. Xing. Evolving cluster mixed-membership blockmodel for time-varying net-\\n\\nworks. In AISTATS, 2011.\\n\\n[13] P. D. Hoff, A. E. Raftery, and M. S. Handcock. Latent space approaches to social network analysis. JASA,\\n\\n97(460):1090 \\u2013 1098, 2002.\\n\\n[14] K. Ishiguro, T. Iwata, N. Ueda, and J. Tenenbaum. Dynamic in\\ufb01nite relational model for time-varying\\n\\nrelational data analysis. In NIPS, 2010.\\n\\n[15] S. Kairam, D. Wang, and J. Leskovec. The life and death of online groups: Predicting group growth and\\n\\nlongevity. In WSDM, 2012.\\n\\n[16] M. Kim and J. Leskovec. Modeling social networks with node attributes using the multiplicative attribute\\n\\ngraph model. In UAI, 2011.\\n\\n[17] M. Kim and J. Leskovec. Latent multi-group membership graph model. In ICML, 2012.\\n\\n[18] M. Kim and J. Leskovec. Multiplicative attribute graph model of real-world networks. Internet Mathe-\\n\\nmatics, 8(1-2):113\\u2013160, 2012.\\n\\n[19] M. Kim and J. Leskovec. Nonparametric multi-group membership model for dynamic networks.\\n\\narXiv:1311.2079, 2013.\\n\\n[20] J. R. Lloyd, P. Orbanz, Z. Ghahramani, and D. M. Roy. Random function priors for exchangeable arrays\\n\\nwith applications to graphs and relational data. In NIPS, 2012.\\n\\n[21] K. T. Miller, T. L. Grifths, and M. I. Jordan. Nonparametric latent feature models for link prediction. In\\n\\nNIPS, 2009.\\n\\n[22] M. M\\u00f8rup, M. N. Schmidt, and L. K. Hansen.\\n\\nIn\\ufb01nite multiple membership relational modeling for\\n\\ncomplex networks. In MLSP, 2011.\\n\\n[23] K. Palla, D. A. Knowles, and Z. Ghahramani. An in\\ufb01nite latent attribute model for network data.\\n\\nIn\\n\\nICML, 2012.\\n\\n[24] P. Sarkar and A. W. Moore. Dynamic social network analysis using latent space models. In NIPS, 2005.\\n\\n[25] J. Scott, R. Gass, J. Crowcroft, P. Hui, C. Diot, and A. Chaintreau. CRAWDAD data set cambridge/haggle\\n\\n(v. 2009-05-29), May 2009.\\n\\n[26] S. L. Scott. Bayesian methods for hidden markov models. JASA, 97(457):337\\u2013351, 2002.\\n\\n[27] T. A. B. Snijders, G. G. van de Bunt, and C. E. G. Steglich. Introduction to stochastic actor-based models\\n\\nfor network dynamics. Social Networks, 32(1):44\\u201360, 2010.\\n\\n[28] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. Arnetminer: Extraction and mining of academic\\n\\nsocial networks. In KDD\\u201908, 2008.\\n\\n[29] J. Yang and J. Leskovec. Community-af\\ufb01liation graph model for overlapping community detection. In\\n\\nICDM, 2012.\\n\\n9\\n\\n\\f\",\n          \"Tangent Prop - A formalism for specifying \\nselected invariances in an adaptive network \\n\\nPatrice Simard \\n\\nAT&T Bell Laboratories \\n101 Crawford Corner Rd \\n\\nHolmdel, NJ 07733 \\n\\nBernard Victorri \\nUniversite de Caen \\nCaen 14032 Cedex \\n\\nFrance \\n\\nYann Le Cun \\n\\nAT&T Bell Laboratories \\n101 Crawford Corner Rd \\n\\nHolmdel, NJ 07733 \\n\\nJohn Denker \\n\\nAT&T Bell Laboratories \\n101 Crawford Corner Rd \\n\\nHolmdel, NJ 07733 \\n\\nAbstract \\n\\nIn many machine learning applications, one has access, not only to training \\ndata, but also to some high-level a priori knowledge about the desired be(cid:173)\\nhavior of the system. For example, it is known in advance that the output \\nof a character recognizer should be invariant with respect to small spa(cid:173)\\ntial distortions of the input images (translations, rotations, scale changes, \\netcetera). \\nWe have implemented a scheme that allows a network to learn the deriva(cid:173)\\ntive of its outputs with respect to distortion operators of our choosing. \\nThis not only reduces the learning time and the amount of training data, \\nbut also provides a powerful language for specifying what generalizations \\nwe wish the network to perform. \\n\\n1 \\n\\nINTRODUCTION \\n\\nIn machine learning, one very often knows more about the function to be learned \\nthan just the training data. An interesting case is when certain directional deriva(cid:173)\\ntives of the desired function are known at certain points. For example, an image \\n895 \\n\\n\\f896 \\n\\nSimard, Victorri, Le Cun, and Denker \\n\\nFigure 1: Top: Small rotations of an original digital image of the digit \\\"3\\\" (center). \\nMiddle: Representation of the effect of the rotation in the input vector space space \\n(assuming there are only 3 pixels). Bottom: Images obtained by moving along the \\ntangent to the transformation curve for the same original digital image (middle). \\n\\nrecognition system might need to be invariant with respect to small distortions of \\nthe input image such as translations, rotations, scalings, etc.; a speech recognition \\nsystem n.ight need to be invariant to time distortions or pitch shifts. \\nIn other \\nwords, the derivative of the system's output should be equal to zero when the input \\nis transformed in certain ways. \\n\\nGiven a large amount of training data and unlimited training time, the system \\ncould learn these invariances from the data alone, but this is often infeasible. The \\nlimitation on data can be overcome by training the system with additional data \\nobtained by distorting (translating, rotating, etc.) \\nthe original patterns (Baird, \\n1990). The top of Fig. 1 shows artificial data generated by rotating a digital image of \\nthe digit \\\"3\\\" (with the original in the center). This procedure, called the \\\"distortion \\nmodel\\\" , has two drawbacks. First, the user must choose the magnitude of distortion \\nand how many instances should be generated. Second, and more importantly, the \\ndistorted data is highly correlated with the original data. This makes traditional \\nlearning algorithms such as back propagation very inefficient. The distorted data \\ncarries only a very small incremental amount of information, since the distorted \\npatterns are not very different from the original ones. It may not be possible to \\nadjust the learning system so that learning the invariances proceeds at a reasonable \\nrate while learning the original points is non-divergent. \\n\\nThe key idea in this paper is that it is possible to directly learn the effect on \\nthe output of distorting the input, independently from learning the undistorted \\n\\n\\fTangent Prop-A formalism for specifying selected invariances in an adaptive network \\n\\n897 \\n\\nF(x) \\n\\nF(x) \\n\\nx1 \\n\\nx2 \\n\\nx3 \\n\\nx4 \\n\\nx \\n\\nx1 \\n\\nx2 \\n\\nx3 \\n\\nx4 \\n\\nx \\n\\nFigure 2: Learning a given function (solid line) from a limited set of example (Xl \\nto X4). The fitted curves are shown in dotted line. Top: The only constraint is that \\nthe fitted curve goes through the examples. Bottom: The fitted curves not only \\ngoes through each examples but also its derivatives evaluated at the examples agree \\nwith the derivatives of the given function. \\n\\npatterns. When a pattern P is transformed (e.g. rotated) with a transformation \\ns that depends on one parameter a (e.g. the angle of the rotation), the set of all \\nthe transformed patterns S(P) = {sea, P) Va} is a one dimensional curve in the \\nvector space of the inputs (see Fig. 1). In certain cases, such as rotations of digital \\nimages, this curve must be made continuous using smoothing techniques, as will be \\nshown below. When the set of transformations is parameterized by n parameters \\nai (rotation, translation, scaling, etc.), S(P) is a manifold of at most n dimensions. \\nThe patterns in S(P) that are obtained through small transformations of P, i.e. \\nthe part of S( P) that is close to P, can be approximated by a plane tangent to \\nthe manifold S(P) at point P. Small transformations of P can be obtained by \\nadding to P a linear combination of vectors that span the tangent plane (tangent \\nvectors). The images at the bottom of Fig. 1 were obtained by that procedure. \\nMore importantly, the tangent vectors can be used to specify high order constraints \\non the function to be learned, as explained below. \\n\\nTo illustrate the method, consider the problem of learning a single-valued function \\nF from a limited set of examples. Fig. 2 (left) represents a simple case where the \\ndesired function F (solid line) is to be approximated by a function G (dotted line) \\nfrom four examples {(Xi, F(Xi))}i=1,2,3,4. As exemplified in the picture, the fitted \\nfunction G largely disagrees with the desired function F between the examples. If \\nthe functions F and G are assumed to be differentiable (which is generally the case), \\nthe approximation G can be greatly improved by requiring that G's derivatives \\nevaluated at the points {xd are equal to the derivatives of F at the same points \\n(Fig. 2 right). This result can be extended to multidimensional inputs. In this case, \\nwe can impose the equality of the derivatives of F and G in certain directions, not \\nnecessarily in all directions of the input space. \\nSuch constraints find immediate use in traditional learning problems. It is often the \\ncase that a priori knowledge is available on how the desired function varies with \\n\\n\\f898 \\n\\nSimard, Victorri, Le Cun, and Denker \\n\\npattern P \\n\\npattern P \\nrotated by ex \\n\\n-\\n\\ntangent \\nvector \\n\\n--\\n\\nFigure 3: How to compute a tangent vector for a given transformation (in this case \\na rotation). \\n\\nrespect to some transformations of the input. It is straightforward to derive the \\ncorresponding constraint on the directional derivatives of the fitted function G in \\nthe directions of the transformations (previously named tangent vectors). Typical \\nexamples can be found in pattern recognition where the desired classification func(cid:173)\\ntion is known to be invariant with respect to some transformation of the input such \\nas translation, rotation, scaling, etc., in other words, the directional derivatives of \\nthe classification function in the directions of these transformations is zero. \\n\\n2 \\n\\nIMPLEMENTATION \\n\\nThe implementation can be divided into two parts. The first part consists in com(cid:173)\\nputing the tangent vectors. This part is independent from the learning algorithm \\nused subsequently. The second part consists in modifying the learning algorithm \\n(for instance backprop) to incorporate the information about the tangent vectors. \\nPart I: Let x be an input pattern and s be a transformation operator acting \\non the input space and depending on a parameter a. If s is a rotation operator \\nfor instance, then s( a, x) denotes the input x rotated by the angle a. We will \\nrequire that the transformation operator s be differentiable with respect to a and \\nx, and that s(O, x) = x. The tangent vector is by definition 8s(a, x)/8a. It can be \\napproximated by a finite difference, as shown in Fig. 3. In the figure, the input space \\nis a 16 by 16 pixel image and the patterns are images of handwritten digits. The \\ntransformations considered are rotations of the digit images. The tangent vector \\nis obtained in two steps. First the image is rotated by an infinitesimal amount a. \\nThis is done by computing the rotated coordinates of each pixel and interpolating \\nthe gray level values at the new coordinates. This operation can be advantageously \\ncombined with some smoothing using a convolution. A convolution with a Gaussian \\nprovides an efficient interpolation scheme in O(nm) multiply-adds, where nand m \\nare the (gaussian) kernel and image sizes respectively. The next step is to subtract \\n(pixel by pixel) the rotated image from the original image and to divide the result \\n\\n\\fTangent Prop-A formalism for specifying selected invariances in an adaptive network \\n\\n899 \\n\\nby the scalar 0 (see Fig. 3). If Ie types of transformations are considered, there \\nwill be Ie different tangent vectors per pattern. For most algorithms, these do not \\nrequire any storage space since they can be generated as needed from the original \\npattern at negligible cost. \\nPart IT: Tangent prop is an extension of the backpropagation algorithm, allowing \\nit to learn directional derivatives. Other algorithms such as radial basis functions \\ncan be extended in a similar fashion. \\n\\nTo implement our idea, we will modify the usual weight-update rule: \\nis replaced with ~w = -7] ow (E + J.tEr) \\n\\noE \\n~w = -7] ow \\n\\n0 \\n\\n(1) \\n\\nwhere 7] is the learning rate, E the usual objective function, Er an additional objec(cid:173)\\ntive function (a regularizer) that measures the discrepancy between the actual and \\ndesired directional derivatives in the directions of some selected transformations, \\nand J.t is a weighting coefficient. \\nLet x be an input pattern, y = G(x) be the input-output function of the network. \\nThe regularizer Er is of the form \\n\\nwhere Er(x) is \\n\\nEr(x) \\n\\n:e e trainingset \\n\\n(2) \\n\\nHere, Ki(x) is the desired directional derivative of G in the direction induced by \\ntransformation Si applied to pattern x. The second term in the norm symbol is the \\nactual directional derivative, which can be rewritten as \\n\\n= G'{x). OSi(O, x) \\n\\n00 \\n\\n0=0 \\n\\n0=0 \\n\\nwhere G'(x) is the Jacobian of G for pattern x, and OSi(O, x)Joo is the tangent \\nvector associated to transformation Si as described in Part I. Multiplying the tangent \\nvector by the Jacobian involves one forward propagation through a \\\"linearized\\\" \\nversion of the network. In the special case where local invariance with respect to \\nthe Si'S is desired, Ki(x) is simply set to o. \\nComposition of transformations: The theory of Lie groups (Gilmore, 1974) \\nensures that compositions of local (small) transformations Si correspond to linear \\ncombinations of the corresponding tangent vectors (the local transformations Si \\nhave a structure of Lie algebra). Consequently, if Er{x) = 0 is verified, the network \\nderivative in the direction of a linear combination of the tangent vectors is equal \\nto the same linear combination of the desired derivatives. In other words if the \\nnetwork is successfully trained to be locally invariant with respect to, say, horizontal \\ntranslation and vertical translations, it will be invariant with respect to compositions \\nthereof. \\nWe have derived and implemented an efficient algorithm, \\\"tangent prop\\\" , for per(cid:173)\\nforming the weight update (Eq. 1). It is analogous to ordinary backpropagation, \\n\\n\\f900 \\n\\nSimard, Victorri, Le Cun, and Denker \\n\\nW'+l \\n\\nIti \\n\\nW l+1 \\n\\nIti \\n\\ne: l \\n\\nb'.-l , \\n\\nx\\u00b7 , \\n'-I \\n\\nNetwork \\n\\nj3J-1 \\n\\ne;-I \\nJacobian nework \\n\\nFigure 4: forward propagated variables (a, x, a, e), and backward propagated vari(cid:173)\\nables (b, y, p, t/J) in the regular network (roman symbols) and the Jacobian (lin(cid:173)\\nearized) network (greek symbols) \\n\\nbut in addition to propagating neuron activations, it also propagates the tangent \\nvectors. The equations can be easily derived from Fig. 4. \\nForward propagation: \\n\\na~ = ~ wL x'.-l \\nI, , \\n\\u2022 \\n\\nL...J \\ni \\n\\nx~ = u(aD \\n\\nTangent forward propagation: \\n\\n, _ ~ , ~'-1 \\nai - L...J wW\\\"i \\n\\ni \\n\\ne! = u'(a~)a~ \\n\\nTangent gradient backpropagation: \\n\\n(31 - ~ w'+1.I.l+1 \\ni - L...J \\nIt \\n\\nIti \\u00a5lit \\n\\nGradient backpropagation: \\n\\nb' - ~ w1+ 1yl+1 \\ni - L...J \\nIt \\n\\nIti \\n\\nIt \\n\\nWeight update: \\n\\n8[E(W, Up) + I'Er (W, Up, Tp)] _ 1-1 , + ~'-l.I.' \\n\\u00a5Ii \\n\\n- Xi Yi \\n\\nI'\\\\oi \\n\\nw\\u00b7\\u00b7 I, \\n8 ' \\n\\n(3) \\n\\n(4) \\n\\n(5) \\n\\n(6) \\n\\n(7) \\n\\n\\fTangent Prop--A formalism for specifying selected invariances in an adaptive network \\n\\n901 \\n\\n60 \\n\\n50 \\n\\n%Erroron \\nthe test set \\n\\n20 \\n\\n10 \\n\\n160 \\n\\n320 \\n\\nTraining set size \\n\\nFigure 5: Generalization performance curve as a function of the training set size for \\nthe tangent prop and the backprop algorithms \\n\\nThe regularization parameter jJ is tremendously important, because it determines \\nthe tradeoff between minimizing the usual objective function and minimizing the \\ndirectional derivative error. \\n\\n3 RESULTS \\n\\nTwo experiments illustrate the advantages of tangent prop. The first experiment \\nis a classification task, using a small (linearly separable) set of 480 binarized hand(cid:173)\\nwritten digit. The training sets consist of 10, 20, 40, 80, 160 or 320 patterns, and \\nthe training set contains the remaining 160 patterns. The patterns are smoothed \\nusing a gaussian kernel with standard deviation of one half pixel. For each of the \\ntraining set patterns, the tangent vectors for horizontal and vertical translation \\nare computed. The network has two hidden layers with locally connected shared \\nweights, and one output layer with 10 units (5194 connections, 1060 free parame(cid:173)\\nters) (Le Cun, 1989). The generalization performance as a function of the training \\nset size for traditional backprop and tangent prop are compared in Fig. 5. We have \\nconducted additional experiments in which we implemented not only translations \\nbut also rotations, expansions and hyperbolic deformations. This set of 6 gener(cid:173)\\nators is a basis for all linear transformations of coordinates for two dimensional \\nimages. It is straightforward to implement other generators including gray-Ievel(cid:173)\\nshifting, \\\"smooth\\\" segmentation, local continuous coordinate transformations and \\nindependent image segment transformations. \\n\\nThe next experiment is designed to show that in applications where data is highly \\n\\n\\f902 \\n\\nSimard, Victorri, Le Cun, and Denker \\n\\nAv\\\"ge NMSE VI 1ge \\n\\nA-. NMSE VI. \\n\\n0.15 \\n\\n0.1 \\n\\n.15 \\n\\n.1 \\n\\no \\no 1000 2000 3000 4000 5000 6000 7000 8000 0000 10000 \\n\\noL-~~==~~=;~==+=~~~ \\n1000 2000 3000 4000 5000 6000 7000 8000 0000 10000 \\n0 \\n\\n-\\n\\n\\\" \\n\\n..... \\n\\n15 \\n\\no \\n\\n-0.5 \\n\\n-1 \\n\\n..... \\n\\n15 -\\n\\n\\\" \\n\\n0 \\n\\n-.5 \\n\\n-1 \\n\\n-1 .5 +--_+_-_--+_-_+_-_-_ \\n\\n-1.5 +--_+_-_--+--_+_-_-__t \\n\\n-1 .5 \\n\\n-1 \\n\\n1.5 \\n\\n-1.5 \\n\\n-1 \\n\\n0 \\n\\n0.5 \\n\\n-0.5 \\nDistortion model \\n\\no \\n\\n.5 \\n\\n- .5 \\nTangent prop \\n\\n1.5 \\n\\nFigure 6: Comparison of the distortion model (left column) and tangent prop (right \\ncolumn). The top row gives the learning curves (error versus number of sweeps \\nthrough the training set). The bottom row gives the final input-output function of \\nthe network; the dashed line is the result for unadorned back prop. \\n\\n\\fTangent Prop-A formalism for specifying selected invariances in an adaptive network \\n\\n903 \\n\\ncorrelated, tangent prop yields a large speed advantage. Since the distortion model \\nimplies adding lots of highly correlated data, the advantage of tangent prop over \\nthe distortion model becomes clear. \\nThe task is to approximate a function that has plateaus at three locations. We want \\nto enforce local invariance near each of the training points (Fig. 6, bottom). The \\nnetwork has one input unit, 20 hidden units and one output unit. Two strategies are \\npossible: either generate a small set of training point covering each of the plateaus \\n(open squares on Fig. 6 bottom), or generate one training point for each plateau \\n(closed squares), and enforce local invariance around them (by setting the desired \\nderivative to 0). The training set of the former method is used as a measure the \\nperformance for both methods. All parameters were adjusted for approximately \\noptimal performance in all cases. The learning curves for both models are shown in \\nFig. 6 (top). Each sweep through the training set for tangent prop is a little faster \\nsince it requires only 6 forward propagations, while it requires 9 in the distortion \\nmodel. As can be seen, stable performance is achieved after 1300 sweeps for the \\ntangent prop, versus 8000 for the distortion model. The overall speedup is therefore \\nabout 10. \\nTangent prop in this example can take advantage of a very large regularization term. \\nThe distortion model is at a disadvantage because the only parameter that effec(cid:173)\\ntively controls the amount of regularization is the magnitude of the distortions, and \\nthis cannot be increased to large values because the right answer is only invariant \\nunder small distortions. \\n\\n4 CONCLUSIONS \\n\\nWhen a priori information about invariances exists, this information must be made \\navailable to the adaptive system. There are several ways of doing this, including the \\ndistortion model and tangent prop. The latter may be much more efficient in some \\napplications, and it permits separate control of the emphasis and learning rate for \\nthe invariances, relative to the original training data points. Training a system to \\nhave zero derivatives in some directions is a powerful tool to express invariances to \\ntransformations of our choosing. Tests of this procedure on large-scale applications \\n(handwritten zipcode recognition) are in progress. \\n\\nReferences \\n\\nBaird, H. S. (1990). Document Image Defect Models. In IAPR 1990 Workshop on \\n\\nSytactic and Structural Pattern Recognition, pages 38-46, Murray Hill, NJ. \\n\\nGilmore, R. (1974). Lie Groups, Lie Algebras and some of their Applications. Wiley, \\n\\nNew York. \\n\\nLe Cun, Y. (1989) . Generalization and Network Design Strategies. In Pfeifer, R., \\nSchreter, Z., Fogelman, F., and Steels, L., editors, Connectionism in Perspec(cid:173)\\ntive, Zurich, Switzerland. Elsevier. an extended version was published as a \\ntechnical report of the University of Toronto. \\n\\n\\f\",\n          \"Minimizing  Statistical Bias with Queries \\n\\nDavid A.  Cohn \\n\\nAdaptive Systems Group \\n\\nHarlequin,  Inc. \\n\\nOne  Cambridge Center \\nCambridge,  MA  02142 \\ncOhnCharlequin.com \\n\\nAbstract \\n\\nI describe  a  querying criterion that attempts to minimize the error \\nof a  learner  by  minimizing its estimated squared  bias.  I  describe \\nexperiments  with  locally-weighted  regression  on  two  simple prob(cid:173)\\nlems,  and observe  that this  \\\"bias-only\\\"  approach  outperforms the \\nmore  common  \\\"variance-only\\\"  exploration  approach,  even  in  the \\npresence  of noise. \\n\\n1 \\n\\nINTRODUCTION \\n\\nIn recent  years, there has been an explosion of interest in \\\"active\\\"  machine learning \\nsystems.  These  are  learning  systems  that  make  queries,  or  perform  experiments \\nto  gather data that  are expected  to maximize performance.  When  compared with \\n\\\"passive\\\"  learning  systems,  which  accept  given,  or  randomly  drawn  data,  active \\nlearners have demonstrated significant decreases  in the amount of data required  to \\nachieve equivalent performance.  In industrial applications,  where  each  experiment \\nmay take  days  to  perform  and  cost  thousands  of dollars,  a  method  for  optimally \\nselecting  these  points would offer enormous savings in time and  money. \\nAn  active  learning system  will  typically attempt to select  data that  will  minimize \\nits  predictive  error.  This  error  can  be  decomposed  into  bias  and  variance  terms. \\nMost  research  in selecting  optimal actions or  queries  has  assumed  that the learner \\nis  approximately unbiased,  and that to minimize learner error,  variance is  the only \\nthing  to  minimize  (e.g.  Fedorov  [1972]'  MacKay  [1992]'  Cohn  [1996],  Cohn  et  al., \\n[1996],  Paass  [1995]).  In  practice,  however,  there  are very  few  problems for  which \\nwe have unbiased learners.  Frequently, bias constitutes a large portion of a learner's \\nerror;  if the learner is deterministic and the data are noise-free, then bias is the  only \\nsource  of error.  Note  that the bias  term here  is  a  statistical bias,  distinct from  the \\ninductive  bias  discussed  in some  machine  learning  research  [Dietterich  and  Kong, \\n1995]. \\n\\n\\f418 \\n\\nD.A. Cohn \\n\\nIn this paper I describe an algorithm which selects actions/ queries designed to mini(cid:173)\\nmize the bias of a locally weighted regression-based  learner.  Empirically, \\\"variance(cid:173)\\nminimizing\\\" strategies which ignore bias seem to perform well, even in cases  where, \\nstrictly speaking,  there is  no  variance  to  minimize.  In  the  tasks  considered  in  this \\npaper,  the  bias-minimizing strategy  consistently  outperforms  variance  minimiza(cid:173)\\ntion, even in  the presence  of noise. \\n\\n1.1  BIAS  AND  VARIANCE \\n\\nLet us  begin  by defining P(x, y)  to be the unknown joint distribution over  x and y, \\nand  P( x)  to  be  the  known  marginal distribution  of x  (commonly called  the  input \\ndistribution).  We  denote  the  learner's  output  on input  x,  given  training set  D  as \\ny(x; D).  We  can  then  write the expected  error of the learner as \\n\\n1 E  [(y(x;D) - y(x))2Ix] P(x)dx, \\n\\n(1) \\n\\nwhere E[\\u00b7] denotes the expectation over P and over training sets D.  The expectation \\ninside the integral may be  decomposed as follows  (Geman et al. , 1992): \\n\\nE  [(y(x;D) - y(x))2Ix] \\n\\nE  [(y(x) - E[ylx]?] \\n\\n(2) \\n\\n+ (Ev [y(x; D)] - E[ylx])2 \\n\\n+Ev [(y(x;D) - Ev[y(x;D)])2] \\n\\nwhere Ev [.] denotes the expectation over training sets.  The first  term in Equation 2 \\nis the variance of y given x - it is the noise in the distribution, and does  not depend \\non our learner or how the training data are chosen.  The second term is the learner's \\nsquared bias, and the third is its variance; these last two terms comprise the expected \\nsquared error of the learner  with  respect  to the regression  function  E[Ylx]. \\nMost  research  in  active  learning  assumes  that  the  second  term  of Equation  2  is \\napproximately zero,  that  is,  that  the  learner  is  unbiased.  If this  is  the  case,  then \\none may concentrate on selecting data so  as to minimize the variance of the learner. \\nAlthough this  \\\"all-variance\\\" approach is optimal when the learner is  unbiased, truly \\nunbiased  learners  are  rare.  Even  when  the  learner's  representation  class  is  able \\nto  match  the  target  function  exactly,  bias is  generally  introduced  by  the  learning \\nalgorithm  and  learning  parameters.  From  the  Bayesian  perspective,  a  learner  is \\nonly unbiased  if its  priors are  exactly  correct. \\nThe optimal choice  of query would,  of course, minimize  both  bias and variance,  but \\nI leave that for future work.  For the purposes of this paper, I will only be concerned \\nwith  selecting  queries  that  are  expected  to  minimize  learner  bias.  This  approach \\nis  justified  in  cases  where  noise  is  believed  to  be  only  a  small  component  of the \\nlearner's  error.  If the  learner  is  deterministic  and  there  is  no  noise,  then  strictly \\nspeaking, there  is  no error  due  to  variance -\\nall  the error  must be  due  to learner \\nbias.  In cases with non-determinism or noise, all-bias minimization, like all-variance \\nminimization, becomes  an approximation of the optimal approach. \\n\\nThe learning model discussed  in this paper is  a  form of locally  weighted  regression \\n(LWR)  [Cleveland et  al.,  1988],  which  has  been  used  in  difficult  machine  learning \\ntasks,  notably  the  \\\"robot  juggler\\\"  of Schaal  and  Atkeson  [1994].  Previous  work \\n[Cohn et al.,  1996]  discussed  all-variance query selection for  LWR; in the remainder \\nof this paper, I describe  a method for  performing all-bias query selection.  Section 2 \\ndescribes the criterion that must be optimized for all-bias query selection.  Section 3 \\ndescribes  the  locally  weighted  regression  learner  used  in  this  paper  and  describes \\n\\n\\fMinimizing Statistical Bias with Queries \\n\\n419 \\n\\nhow  the  all-bias criterion  may be  computed for  it .  Section  4  describes  the  results \\nof experiments using this criterion on several simple domains.  Directions for future \\nwork  are discussed  in  Section 5. \\n\\n2  ALL-BIAS  QUERY  SELECTION \\n\\nLet  us  assume for  the moment that we  have  a source of noise-free examples (Xi, Yi) \\nand  a  deterministic learner  which,  given  input  X,  outputs estimate Y(X).l  Let  us \\nalso  assume  that  we  have  an  accurate estimate of the  bias  of y which  can  be  used \\nto estimate  the  true  function  y(x)  =  y(x)  - bias(x).  We  will  break  these  rather \\nstrong assumptions of noise-free examples and accurate bias estimates in Section 4, \\nbut they  are useful for  deriving  the theoretical  approach described  below. \\n\\nGiven  an  accurate  bias estimate,  we  must force  the  biased  estimator into the  best \\napproximation of y(x)  with  the fewest  number of examples.  This,  in effect,  trans(cid:173)\\nforms  the  query  selection  problem  into  an  example filter  problem  similar  to  that \\nstudied  by  Plutowski  and  White  [1993]  for  neural  networks.  Below,  I  derive  this \\ncriterion for  estimating the change  in error at  X  given  a  new  queried  example at x. \\nSince  we  have  (temporarily)  assumed  a  deterministic  learner  and  noise-free  data, \\nthe expected  error in  Equation 2 simplifies to: \\n\\nE  [(Y( X; 'D)  - y( x))2Ix, 'D] \\n\\n(Y(x; 'D)  - y(x))2 \\n\\n(3) \\n\\nWe  want to select  a new  x such that when we  add (x, f)),  the resulting squared bias \\nis  minimized: \\n\\n(Y'  - y? ==  (y(x; 'D U (x, f)))  - y(x))2 . \\n\\n(4) \\nI will, for the remainder of the paper, use the  \\\"'\\\"  to indicate estimates based on the \\ninitial training set  plus  the  additional example  (x, y).  To  minimize  Expression  4, \\nwe  need  to  compute  how  a  query  at  x will  change  the  learner's  bias  at  x.  If we \\nassume  that  we  know  the  input  distribution,2  then  we  can  integrate  this  change \\nover  the  entire  domain  (using  Monte  Carlo  procedures)  to  estimate  the  resulting \\naverage  change,  and select  a  x such  that  the  expected  squared  bias  is  minimized. \\nDefining bias ==  y - y and f:,.y  ==  y'  - y, we  can  write the new  squared  bias as: \\n\\nbias,2 \\n\\n(y'  - y)2  =  (Y + f:,.y  _ y)2 \\nf:,.y2  + 2f:,.y . bias + bias2 \\n\\n(5) \\nNote  that  since  bias  as  defined  here  is  independent  of x,  minimizing  the  bias  is \\nequivalent to minimizing f:,.y2  + 2f:,.y . bias. \\nThe estimate of bias'  tells  us  how much our bias will change for  a given x. We may \\noptimize this value over x in one of a number of ways.  In low dimensional spaces,  it \\nis often sufficient to consider a set of \\\"candidate\\\" x and select the one promising the \\nsmallest resulting  error.  In  higher  dimensional spaces,  it  is  often  more efficient  to \\nsearch  for  an optimal x with  a  response  surface  technique  [Box  and  Draper, 1987], \\nor hill climb on  abias,2 / ax. \\nEstimates  of bias  and  f:,.y  depend  on  the  specific  learning  model  being  used.  In \\nSection 3, I describe a locally weighted regression model, and show how differentiable \\nestimates of bias  and f:,.y  may be  computed for  it. \\n\\n1 For  clarity,  I  will  drop  the  argument  :z;  except  where  required  for  disambiguation.  I \\n\\nwill  also  denote only  the univariate  case;  the results  apply  in  higher  dimensions  as  well. \\n2This assumption is  contrary to the assumption norma.lly  made in some forms of learn(cid:173)\\n\\ning,  e.g.  PAC-learning,  but it is  appropriate in  many  domains. \\n\\n\\f420 \\n\\nD.  A.  Cohn \\n\\n2.1  AN  ASIDE:  WHY  NOT JUST USE Y - Mas? \\n\\nIf we  have  an accurate  bias estimate, it is  reasonable to ask  why  we  do  not simply \\nuse  the  corrected  y - C;;;S  as  our  predictor.  The  answer  has  two  parts,  the  first \\nof  which  is  that  for  most  learners,  there  are  no  perfect  bias  estimators  -\\nthey \\nintroduce their own  bias and  variance,  which  must  be  addressed  in data selection. \\nSecond,  we  can  define  a  composite learner Ye  ==  Y - C:;;;S.  Given  a random training \\nsample  then,  we  would  expect  Ye  to  outperform  y.  However,  there  is  no  obvious \\nway  to select  data for  this composite learner other than selecting  to maximize the \\nperformance  of its  two  components.  In  our  case,  the  second  component  (the  bias \\nestimate)  is  non-analytic,  which  leaves  us  selecting  data  so  as  to  maximize  the \\nperformance of the  first  component  (the uncorrected  estimator).  We  are  now  back \\nto  our  original  problem:  we  can  select  data so  as  to  minimize either  the  bias  or \\nvariance of the uncorrected  LWR-based learner.  Since the purpose of the correction \\nis to give an unbiased estimator, intuition suggests that variance minimization would \\nbe the more sensible route in this  case.  Empirically, this approach does  not appear \\nto yield  any  benefit  over  uncorrected  variance minimization (see  Figure  1). \\n\\n3  LOCALLY WEIGHTED REGRESSION \\n\\nThe type  of learner  I  consider here  is  a form  of locally weighted  regression  (LWR) \\nthat is  a slight variation on  the  LOESS  model of Cleveland et  al.  [1988]  (see  Cohn \\net al., [1996]  for details).  The LOESS  model performs a linear regression  on points \\nin  the  data set,  weighted  by  a  kernel  centered  at  x.  The kernel  shape  is  a  design \\nparameter:  the original LOESS  model uses  a  \\\"tricubic\\\"  kernel;  in my experiments \\nI  use  the more common Gaussian \\n\\nwhere Ie  is a smoothing parameter.  For brevity, I will drop the argument x for  hi(x), \\nand define  n  = 2:i  hi.  We  can then  write  the weighted  means and  covariances  as: \\n\\n\\\"\\\"  Xi \\n, \\nn \\n. \\n\\nJ.l:r;  = L..J hi - ,  \\nJ.ly  = L h,-, \\n\\nYi \\nn \\n\\n, \\n. \\n\\n\\\"\\\" \\n\\nU:r;y  =  L..J hi \\n\\n, \\n. \\n\\n(Xi  - X)(Yi  - J.ly) \\n\\nn \\n\\n. \\n\\nWe use these means and covariances to produce an estimate Y at the x  around which \\nthe kernel  is  centered,  with  a  confidence  term in  the form of a  variance estimate: \\n\\nIn  all the experiments  discussed  in  this paper,  the smoothing parameter Ie  was  set \\nso  as  to minimize u2. \\nThe  low  cost  of incorporating  new  training  examples  makes  this  form  of locally \\nweighted regression  appealing for  learning systems which must operate in real time, \\nor with time-varying target functions  (e.g.  [Schaal and Atkeson  1994]). \\n\\n\\fMinimizing Statistical Bias with Queries \\n\\n421 \\n\\nI \\n\\nI \\nY \\n\\n) \\n\\nA \\n\\nA \\n\\nA  I \\n\\nA \\n\\n3.1  COMPUTING D..y  FOR LWR \\nIf we  know  what  new  point  (x, y)  we're  going  to  add,  computing D..y  for  LWR  is \\nstraightforward.  Defining h as  the  weight  given to x,  and n as  n + h we  can write \\n~y  =  y  - y  =  J.L  + -\\n\\nx  - J.Lx \\n\\nh (Y  ___ J.Ly)  _  uxy (x _  J.Lx)  + (x _  n~x _  ~x) . nuXY_ + h . ~x -:xKii - J.Ly) \\n\\nU xy ( \\nU/2 \\nx \\n\\nU xy ( \\n-\\nu2 \\nx \\n\\nn \\n\\nnu;+h\\u00b7(x-J.Lx)2 \\nNote  that computing D..y  requires  us  to know both the x and y of the new  point .  In \\npractice,  we only know x.  If we  assume, however,  that we  can estimate the learner's \\nbias  at  any  x,  then  we  can  also estimate  the  unknown  value  y ~ y(x)  - bias(x) . \\nBelow,  I  consider  how  to compute the bias  estimate. \\n\\nn \\n\\nn \\n\\nI  ) \\nx - J.L \\nx \\n\\n- J.L \\ny \\n\\n-\\n\\nu; \\n\\n3.2  ESTIMATING  BIAS  FOR LWR \\n\\nThe most common technique for estimating bias is  cross-validation .  Standard cross(cid:173)\\nvalidation however, only gives estimates of the  bias  at our specific  training points , \\nwhich  are  usually combined  to form  an  average  bias estimate.  This is  sufficient  if \\none  assumes  that the  training distribution is  representative  of the  test  distribution \\n(which  it  isn't  in  query  learning)  and  if one  is  content  to just  estimate  the  bias \\nwhere  one  already has  training data (which  we  can't be). \\nIn the query selection  problem, we  must be  able to estimate the bias  at all possible \\nx.  Box  and  Draper  [1987]  suggest  fitting  a  higher  order  model and measuring the \\ndifference.  For  the  experiments  described  in  this  paper,  this  method yielded  poor \\nresults; two other bias-estimation techniques,  however, performed very  well. \\n\\nOne  method  of estimating  bias  is  by  bootstrapping  the  residuals  of  the  training \\npoints.  One produces a  \\\"bootstrap sample\\\" of the learner's residuals on the training \\ndata, and adds them to the original predictions to create a synthetic training set .  By \\naveraging  predictions  over  a  number of bootstrapped  training sets  and  comparing \\nthe average prediction with that of the original predictor, one arrives at a first-order \\nbootstrap estimate of the predictor's bias [Connor 1993; Efron and Tibshirani, 1993] . \\nIt is  known  that this  estimate is  itself biased  towards  zero;  a  standard heuristic  is \\nto divide the estimate by  0.632  [Efron,  1983]. \\nAnother method of estimating bias of a learner is  by fitting  its own cross-validated \\nresiduals.  We  first  compute the cross-validated residuals  on  the training examples. \\nThese  produce  estimates  of the  learner's  bias  at  each  of the  training  points.  We \\ncan then  use  these  residuals  as  training examples for  another  learner  (again  LWR) \\nto produce estimates of what the cross-validated error would be  in places  where  we \\ndon't have  training data. \\n\\n4  EMPIRICAL  RESULTS \\n\\nIn  the  previous  two  sections,  I  have  explained  how  having  an  estimate of D..y  and \\nbias  for  a  learner  allows  one  to  compute  the  learner's  change  in  bias  given  a  new \\nquery,  and  have  shown  how  these  estimates  may be  computed  for  a  learner  that \\nuses  locally weighted regression.  Here,  I apply these  results to two simple problems \\nand  demonstrate  that  they  may  actually  be  used  to  select  queries  that  minimize \\nthe statistical bias (and the error)  of the learner.  The problems involve learning the \\nkinematics  of a  planar  two-jointed  robot  arm:  given  the  shoulder  and elbow joint \\nangles, the learner must predict  the  tip position. \\n\\n\\f422 \\n\\n4.1  BIAS  ESTIMATES \\n\\nD.A. Cohn \\n\\nI tested the accuracy of the two bias estimators by observing their correlations on 64 \\nreference  inputs, given 100 random training examples from the planar arm problem. \\nThe  bias estimates had  a  correlation  with  actual biases  of 0.852  for  the bootstrap \\nmethod, and 0.871  for  the  cross-validation method. \\n\\n4.2  BIAS  MINIMIZATION \\n\\nI ran two sets of experiments using the bias-minimizing criterion in conjunction with \\nthe  bias  estimation technique  of the  previous  section  on  the  planar arm  problem. \\nThe bias minimization criterion was  used  as follows:  At each time step, the learner \\nwas  given  a  set  of 64  randomly chosen  candidate  queries  and  64  uniformly chosen \\nreference  points.  It evaluated  E' (x)  for  each  reference  point  given  each  candidate \\npoint and selected  for  its next  query  the candidate point with the smallest average \\nE' (x) over the reference  points.  I compared the bias-minimizing strategy (using the \\ncross-validation and bootstrap estimation techniques)  against random sampling and \\nthe  variance-minimizing strategy  discussed  in  Cohn  et  al.  [1996].  On  a  Sparc  10, \\nwith m training examples, the  average evaluation times per candidate per reference \\npoint were  58 + 0.16m J.lseconds  for  the variance criterion, 65 + 0.53m J.lseconds  for \\nthe cross-validation-based bias criterion, and 83 + 3. 7m J.lseconds  for  the bootstrap(cid:173)\\nbased  bias criterion  (with  20x resampling) . \\nTo test  whether the  bias-only assumption was robust  against the presence  of noise, \\n1 % Gaussian  noise  was  added  to  the  input  values  of the  training  data  in  all  ex(cid:173)\\nperiments.  This simulates noisy  position effectors  on  the arm , and  results  in  non(cid:173)\\nGaussian noise  in  the output coordinate  system. \\n\\nIn  the  first  series  of experiments,  the  candidate  shoulder  and  elbow  joint  angles \\nwere  drawn  uniformly over  (U[O, 271\\\"],  U[O,  71\\\")) .  In  unconstrained  domains like  this, \\nrandom sampling is  a fairly good default  strategy.  The bias minimization strategies \\nstill  significantly  outperform  both  random  sampling  and  the  variance  minimizing \\nstrategy in these  experiments (see  Figure  1). \\n\\n-1 \\n\\n10 \\n\\ng \\n'\\\" 'il ,0-2 \\n:a \\n~ \\nc: \\n~10  .  random \\n\\n-3 \\n\\n' '\\\\ \\n\\n\\\".'~ \\n\\nvariance-min \\n-\\no  cross-val-min \\n~ x  bootstrap-min \\n200 \\n\\n100 \\n\\n10  0 \\n\\n300 \\n\\ntrainlno set size \\n\\nI, \\n\\\\\\\\ \\n--\\n\\n1 \\n10 \\n\\ng  0 \\n\\n\\\"'10 \\n\\n}1O-1 \\n~  -2 \\n\\n,- -\\n\\n.:  ~~&e-.!)jni(llIZI~  , \\no \\n\\n10 \\n10-3  ~  ~~~~rmiWngar- iOlmizing \\n400 \\n\\n300 \\n\\n200 \\n\\n1 00 \\n\\ntrainino set size \\n\\nIheta 1 \\n\\n(left)  MSE  as  a  function  of number of noisy  training examples for  the \\nFigure  1: \\nunconstrained  arm  problem.  Errors  are  averaged  over  10  runs  for  the  bootstrap \\nmethod and 15  runs for  all others.  One run with the cross-validation-based method \\nwas  excluded  when  k  failed  to  converge  to  a  reasonable  value.  (center)  MSE  as \\na  function  of number of noisy  training examples for  the  constrained  arm problem . \\nThe bias correction strategy discussed in Section 2.1  does no better than the uncor(cid:173)\\nrected  variance-minimizing strategy,  and  much  worse  than  the  bias-minimization \\nstrategy.  (right)  Sample  exploration  trajectory  in  joint-space  for  the  constrained \\narm problem , explored  according  to the bias minimizing criterion. \\n\\nIn the second series of experiments, candidates were  drawn uniformly from a region \\n\\n\\fMinimizing Statistical Bias with Queries \\n\\n423 \\n\\nlocal to  the previously  selected  query:  (01  \\u00b1 0.217\\\", O2  \\u00b1 0.117\\\").  This corresponds  to \\nrestricting  the  arm  to local  motions.  In  a  constrained  problem such  as  this,  ran(cid:173)\\ndom  sampling  is  a  poor  strategy;  both  the  bias  and  variance-reducing  strategies \\noutperform it at least  an order of magnitude.  Further, the  bias-minimization strat(cid:173)\\negy  outperforms variance minimization by a large  margin (Figure 1).  Figure  1 also \\nshows  an  exploration  trajectory  produced  by  pursuing  the  bias-minimizing crite(cid:173)\\nrion.  It is  noteworthy that, although the implementation in this case  was a  greedy \\n(one-step)  minimization, the trajectory results  in globally good exploration. \\n\\n5  DISCUSSION \\n\\nI  have argued  in  this paper  that, in many situations, selecting  queries  to minimize \\nlearner bias is an appropriate and effective strategy for  active learning.  I have given \\nempirical  evidence  that,  with  a  LWR-based  learner  and  the  examples  considered \\nhere,  the strategy is effective  even  in the presence  of noise. \\n\\nBeyond  minimizing either  bias  or  variance,  an  important next  step  is  to explicitly \\nminimize  them  together .  The  bootstrap-based  estimate should  facilitate  this,  as \\nit produces  a  complementary variance estimate with little additional computation. \\nBy optimizing over both criteria simultaneously, we expect to derive a criterion that \\nthat, in  terms of statistics,  is  truly optimal for  selecting  queries. \\n\\nREFERENCES \\nBox,  G.,  &  Draper, N.  (1987).  Empirical model-building  and  response  surfaces, \\nWiley,  New  York. \\nCleveland,  W.,  Devlin,  S.,  &  Grosse,  E.  (1988) .  Regression  by  local fitting. \\nJournal  of Econometrics,  37, 87-114. \\nCohn,  D.  (1996)  Neural  network  exploration  using  optimal  experiment  design. \\nNeural Networks,  9(6):1071-1083. \\nCohn,  D.,  Ghahramani,  Z.,  &  Jordan,  M.  (1996) .  Active  learning  with sta(cid:173)\\ntistical models.  Journal  of Artificial Inteligence  Research 4:129-145 . \\nConnor, J. (1993).  Bootstrap Methods in Neural Network Time Series  Prediction. \\nIn  J .  Alspector  et  al.,  eds.,  Proc.  of the  Int.  Workshop  on  Applications  of Neural \\nNetworks  to  Telecommunications,  Lawrence  Erlbaum, Hillsdale,  N.J. \\nDietterich,  T.,  &  Kong,  E.  (1995) .  Error-correcting  output  coding  corrects \\nbias  and  variance.  In  S.  Prieditis  and  S.  Russell,  eds.,  Proceedings  of the  12th \\nInternational  Conference  on  Machine  Learning. \\nEfron, B. (1983) Estimating the error rate of a prediction rule:  some improvements \\non cross-validation.  J.  Amer.  Statist.  Assoc.  78:316-331. \\nEfron, B.  &  Tibshirani, R.  (1993).  An introduction  to  the  bootstrap.  Chapman \\n&  Hall,  New  York . \\nFedorov, V.  (1972).  Theory  of Optimal Experiments.  Academic Press,  New  York. \\nGeman, S.,  Bienenstock, E.,  &  Doursat, R.  (1992).  Neural  networks and the \\nbias/variance dilemma.  Neural  Computation,  4,  1-58. \\nMacKay,  D.  (1992).  Information-based objective functions  for  active  data selec(cid:173)\\ntion,  Neural  Computation,  4,  590-604. \\nPaass,  G.,  and Kindermann, J. (1994).  Bayesian  Query Construction for  Neu(cid:173)\\nral  Network  Models.  In  G.  Tesauro  et  al.,  eds.,  Advances  in  Neural  Information \\nProcessing Systems  7,  MIT Press. \\nPlutowski, M.,  &  White,  H.  (1993).  Selecting concise  training sets from  clean \\ndata.  IEEE  Transactions  on  Neural  Networks,  4, 305-318. \\nSchaal,  S.  &  Atkeson,  C.  (1994).  Robot  Juggling:  An  Implementation  of \\nMemory-based Learning.  Control Systems 14, 57-71. \\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Dataset Structure\n",
        "\n",
        "Let's begin with examining the first few rows, the data types of each column, and checking for missing values."
      ],
      "metadata": {
        "id": "2JgK1AADpA4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T20:13:14.18178Z",
          "iopub.execute_input": "2024-04-21T20:13:14.183011Z",
          "iopub.status.idle": "2024-04-21T20:13:14.201136Z",
          "shell.execute_reply.started": "2024-04-21T20:13:14.182973Z",
          "shell.execute_reply": "2024-04-21T20:13:14.200041Z"
        },
        "trusted": true,
        "id": "pTTFv-DRpA4z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we are most interested in the full_text column. It contains the text of the abstract and the paper."
      ],
      "metadata": {
        "id": "Dtjk9YCIpA4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nips_papers.isnull().sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:41:34.504507Z",
          "iopub.execute_input": "2024-04-20T19:41:34.505021Z",
          "iopub.status.idle": "2024-04-20T19:41:34.522897Z",
          "shell.execute_reply.started": "2024-04-20T19:41:34.504984Z",
          "shell.execute_reply": "2024-04-20T19:41:34.52061Z"
        },
        "trusted": true,
        "id": "dQ5HpIl3pA40",
        "outputId": "8133dba8-f4c9-4a2c-9089-083dd1ec21f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_id       0\n",
            "year            0\n",
            "title           0\n",
            "abstract     3319\n",
            "full_text       3\n",
            "dtype: int64\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上代码是用来统计每一列中的空行情况\n",
        "比如：\n",
        "**title\tabstract\tyear**\n",
        "Deep Learning\tNeural nets...\t2015\n",
        "(空)\tGANs are...\t2016\n",
        "CNN Paper\t(空)\t2015\n",
        "\n",
        "title       1\n",
        "abstract    1\n",
        "year        0\n",
        "------------------------------------------------\n",
        "这么做的意义是什么\n",
        "✅ 这行代码的意义是什么？\n",
        "在做任何机器学习或文本处理之前，我们必须了解数据质量，特别是：\n",
        "\n",
        "哪些列有缺失（空白）？\n",
        "\n",
        "缺失的数量大不大？\n",
        "\n",
        "是不是需要填补或者删除？\n",
        "\n",
        "如果你不知道哪些地方是空的，后续的模型会报错，或者结果不准确。"
      ],
      "metadata": {
        "id": "RYSHu_UXBRci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "full_text is missing in only three rows. But there are more than three thousand rows where abstract text is missing."
      ],
      "metadata": {
        "id": "f7H2WuGkpA40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning the Data\n",
        "\n",
        "We'll just get rid of rows that don't have full_text."
      ],
      "metadata": {
        "id": "iFQGSSuApA40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "根据上述代码nips_papers.isnull(),会把空白的表格用True填充上"
      ],
      "metadata": {
        "id": "Yk-pMO4rDfgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping rows where 'abstract' is missing\n",
        "nips_papers.dropna(subset=['full_text'], inplace=True)\n",
        "#删除full_text这一列中的空值，inplace=True直接在原来的 nips_papers 表上改，不新建副本。\n",
        "# Reset index after dropping rows 重新整理表格的行号（index），让它从 0 开始递增，并去掉原来的行号。\n",
        "nips_papers.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:41:37.445413Z",
          "iopub.execute_input": "2024-04-20T19:41:37.445926Z",
          "iopub.status.idle": "2024-04-20T19:41:37.474138Z",
          "shell.execute_reply.started": "2024-04-20T19:41:37.445888Z",
          "shell.execute_reply": "2024-04-20T19:41:37.471594Z"
        },
        "trusted": true,
        "id": "gIahJDo1pA41"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing Trends\n",
        "\n",
        "Look for trends such as the number of papers published over the years. This can provide insights into the dataset's coverage and any growth trends in the field of machine learning and NLP."
      ],
      "metadata": {
        "id": "7tK0VOMWpA41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the number of papers per year\n",
        "papers_per_year = nips_papers.groupby('year')['title'].count()\n",
        "papers_per_year.plot(kind='line', marker='o', title='NIPS Papers per Year')\n",
        "plt.ylabel('Number of Papers')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:41:40.641949Z",
          "iopub.execute_input": "2024-04-20T19:41:40.643444Z",
          "iopub.status.idle": "2024-04-20T19:41:40.985424Z",
          "shell.execute_reply.started": "2024-04-20T19:41:40.643383Z",
          "shell.execute_reply": "2024-04-20T19:41:40.983974Z"
        },
        "trusted": true,
        "id": "5VoM8YzSpA41"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keyword Frequency Analysis\n",
        "\n",
        "Before diving into keyword extraction, analyzing the most frequent words in your texts (e.g., paper titles or abstracts) can give you a preliminary idea of common themes."
      ],
      "metadata": {
        "id": "wAOx3INCpA41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec = CountVectorizer(stop_words='english', max_features=100)\n",
        "word_counts = vec.fit_transform(nips_papers['full_text'])\n",
        "sum_words = word_counts.sum(axis=0)\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Plotting the top 20 words\n",
        "df_words_freq = pd.DataFrame(words_freq[:30], columns=['Word', 'Frequency'])\n",
        "sns.barplot(x='Frequency', y='Word', data=df_words_freq, palette='Blues_d')\n",
        "plt.title('Top 30 Most Frequent Words in NIPS Abstracts')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:42:05.706404Z",
          "iopub.execute_input": "2024-04-20T19:42:05.706968Z",
          "iopub.status.idle": "2024-04-20T19:42:56.844506Z",
          "shell.execute_reply.started": "2024-04-20T19:42:05.706926Z",
          "shell.execute_reply": "2024-04-20T19:42:56.843102Z"
        },
        "trusted": true,
        "id": "WcHEqGFHpA41"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "High-frequency words not filtered out by the standard stop words list but still irrelevant to content (like 'paper' or 'results') might need custom stop words processing or advanced techniques like TF-IDF to reduce their impact."
      ],
      "metadata": {
        "id": "P7phSGMqpA42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "### NLTK, spacy\n",
        "\n",
        "Two of the most popular Python libraries for these tasks are NLTK and spaCy.\n",
        "\n",
        "- [NLTK (Natural Language Toolkit)](https://www.nltk.org/) is widely used for teaching and research. It's a comprehensive library featuring many utilities for classical NLP tasks and text processing.\n",
        "- [spaCy](https://spacy.io/) is known for its fast processing speeds and ease of use. It also provides pre-trained models and word vectors and is designed specifically for production use."
      ],
      "metadata": {
        "id": "mXxG9LxVpA42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:44:26.560716Z",
          "iopub.execute_input": "2024-04-20T19:44:26.561248Z",
          "iopub.status.idle": "2024-04-20T19:44:26.700524Z",
          "shell.execute_reply.started": "2024-04-20T19:44:26.561215Z",
          "shell.execute_reply": "2024-04-20T19:44:26.699242Z"
        },
        "trusted": true,
        "id": "OR6K8sFnpA42"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization is the process of splitting text into meaningful units such as words, sentences, or tokens. This is typically the first step in text preprocessing.\n",
        "\n",
        "_*Running this cell takes time_"
      ],
      "metadata": {
        "id": "tTfNA9XgpA42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Using NLTK for tokenization\n",
        "nips_papers['tokens'] = nips_papers['full_text'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:44:33.509138Z",
          "iopub.execute_input": "2024-04-20T19:44:33.509622Z",
          "iopub.status.idle": "2024-04-20T19:54:50.762379Z",
          "shell.execute_reply.started": "2024-04-20T19:44:33.509589Z",
          "shell.execute_reply": "2024-04-20T19:54:50.759882Z"
        },
        "trusted": true,
        "id": "Bn4huWZ4pA42"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords removal\n",
        "\n",
        "Stopwords are commonly used words (such as \"the\", \"a\", \"an\", \"in\") that are often removed in the preprocessing phase because they carry less meaningful information for analysis."
      ],
      "metadata": {
        "id": "yH1PxDr5pA42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load general English stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:55:26.758982Z",
          "iopub.execute_input": "2024-04-20T19:55:26.760474Z",
          "iopub.status.idle": "2024-04-20T19:55:26.770547Z",
          "shell.execute_reply.started": "2024-04-20T19:55:26.760419Z",
          "shell.execute_reply": "2024-04-20T19:55:26.769209Z"
        },
        "trusted": true,
        "id": "31DAM8D6pA42"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with specialized datasets like NIPS papers, you might encounter domain-specific stopwords like \"neural\", \"network\", \"learning\", which are overly frequent and might be less informative for certain analyses."
      ],
      "metadata": {
        "id": "oLRo_IJKpA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom stopwords for NIPS context\n",
        "custom_stopwords = {\"cid\", \"neural\", \"network\", \"learning\", \"model\", \"algorithm\", \"data\",\n",
        "                    \"set\", \"function\", \"problem\", \"models\", \"number\", \"figure\", \"results\",\n",
        "                   \"information\", \"distribution\", \"using\", \"used\", \"use\", \"given\", \"method\",\n",
        "                   \"neural\", \"networks\"}\n",
        "\n",
        "# Function to remove both general and custom stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    return [token for token in tokens if token.lower() not in stop_words and\n",
        "            token.lower() not in custom_stopwords and not token.isdigit()]\n",
        "\n",
        "nips_papers['filtered_tokens'] = nips_papers['tokens'].apply(remove_stopwords)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:01:25.625652Z",
          "iopub.execute_input": "2024-04-20T20:01:25.626389Z",
          "iopub.status.idle": "2024-04-20T20:01:49.435082Z",
          "shell.execute_reply.started": "2024-04-20T20:01:25.626298Z",
          "shell.execute_reply": "2024-04-20T20:01:49.433166Z"
        },
        "trusted": true,
        "id": "U4h8KV3dpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "Stemming reduces words to their word stem, base, or root form. For example, \"fishing\", \"fished\", \"fisher\" all reduce to the stem \"fish\".\n",
        "\n",
        "_*Running this cell takes time_"
      ],
      "metadata": {
        "id": "efnrXbD7pA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to stem tokens\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "nips_papers['stemmed_tokens'] = nips_papers['filtered_tokens'].apply(stem_tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:02:18.102524Z",
          "iopub.execute_input": "2024-04-20T20:02:18.103082Z",
          "iopub.status.idle": "2024-04-20T20:13:41.111909Z",
          "shell.execute_reply.started": "2024-04-20T20:02:18.103041Z",
          "shell.execute_reply": "2024-04-20T20:13:41.10988Z"
        },
        "trusted": true,
        "id": "cGB-45DNpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers['stemmed_tokens'][0]"
      ],
      "metadata": {
        "trusted": true,
        "id": "-34p9ZnrpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *To fix: Lemmatization\n",
        "\n",
        "Lemmatization, similar to stemming, reduces words to their base or root form, but it ensures that the root word belongs to the language."
      ],
      "metadata": {
        "id": "9Zvf0jXTpA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize a list of tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Apply lemmatization to the filtered tokens\n",
        "nips_papers['lemmatized_tokens'] = nips_papers['filtered_tokens'].apply(lemmatize_tokens)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "17c3dSfMpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Algorithms\n",
        "\n",
        "### TF-IDF for Keyword Extraction\n",
        "\n",
        "![image.png](attachment:31a52603-1d81-4846-844a-79c77f5fac43.png)\n",
        "\n",
        "- **Calculate Term Frequency (TF)**: For each word in your document, calculate the term frequency. This is simply the number of times a word appears in a document divided by the total number of words in that document.\n",
        "\n",
        "$$\n",
        "\\text{TF}(word) = \\frac{\\text{Number of times the word appears in a document}}{\\text{Total number of words in the document}}\n",
        "$$\n",
        "\n",
        "    \n",
        "- **Calculate Inverse Document Frequency (IDF)**: This measures how common or rare a word is across all documents. It is calculated as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
        "\n",
        "$$\n",
        "\\text{IDF}(word) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing the word}}\\right)\n",
        "$$\n",
        "    \n",
        "- **Compute TF-IDF: Multiply TF by IDF.** The words with the highest TF-IDF scores are considered the most relevant keywords in the document.\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(word) = \\text{TF}(word) \\times \\text{IDF}(word)\n",
        "$$\n",
        "\n",
        "Very simple, huh?\n",
        "\n",
        "Let's implement this algorithm in pure Python."
      ],
      "metadata": {
        "id": "ir0etvzLpA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_tf(tokenized_docs):\n",
        "    tf_dicts = []\n",
        "    for doc in tokenized_docs:\n",
        "        tf_dict = {}\n",
        "        total_terms = len(doc)\n",
        "        for term in doc:\n",
        "            tf_dict[term] = tf_dict.get(term, 0) + 1 / total_terms\n",
        "        tf_dicts.append(tf_dict)\n",
        "    return tf_dicts\n",
        "\n",
        "def compute_df(tokenized_docs):\n",
        "    df_dict = {}\n",
        "    for doc in tokenized_docs:\n",
        "        for term in set(doc):\n",
        "            df_dict[term] = df_dict.get(term, 0) + 1\n",
        "    return df_dict\n",
        "\n",
        "def compute_idf(df_dict, total_docs):\n",
        "    idf_dict = {}\n",
        "    for term, count in df_dict.items():\n",
        "        idf_dict[term] = math.log(total_docs / count)\n",
        "    return idf_dict\n",
        "\n",
        "def compute_tf_idf(tf_dicts, idf_dict):\n",
        "    tf_idf_dicts = []\n",
        "    for tf_dict in tf_dicts:\n",
        "        tf_idf_dict = {}\n",
        "        for term, tf in tf_dict.items():\n",
        "            tf_idf_dict[term] = tf * idf_dict.get(term, 0)\n",
        "        tf_idf_dicts.append(tf_idf_dict)\n",
        "    return tf_idf_dicts\n",
        "\n",
        "def select_top_keywords(tf_idf_dicts, top_n=5):\n",
        "    top_keywords = []\n",
        "    for tf_idf_dict in tf_idf_dicts:\n",
        "        sorted_terms = sorted(tf_idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_keywords.append([term for term, score in sorted_terms[:top_n]])\n",
        "    return top_keywords"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:34:02.487809Z",
          "iopub.execute_input": "2024-04-20T20:34:02.489016Z",
          "iopub.status.idle": "2024-04-20T20:34:02.500136Z",
          "shell.execute_reply.started": "2024-04-20T20:34:02.488959Z",
          "shell.execute_reply": "2024-04-20T20:34:02.498518Z"
        },
        "trusted": true,
        "id": "pg8_bSyTpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_docs = nips_papers['stemmed_tokens'].tolist()\n",
        "tf_dicts = compute_tf(tokenized_docs)\n",
        "df_dict = compute_df(tokenized_docs)\n",
        "total_docs = len(tokenized_docs)\n",
        "idf_dict = compute_idf(df_dict, total_docs)\n",
        "tf_idf_dicts = compute_tf_idf(tf_dicts, idf_dict)\n",
        "top_tf_idf_keywords = select_top_keywords(tf_idf_dicts)\n",
        "\n",
        "# Update the DataFrame\n",
        "nips_papers['tf_idf_keywords'] = top_tf_idf_keywords"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:34:05.320552Z",
          "iopub.execute_input": "2024-04-20T20:34:05.321039Z",
          "iopub.status.idle": "2024-04-20T20:34:34.278624Z",
          "shell.execute_reply.started": "2024-04-20T20:34:05.321004Z",
          "shell.execute_reply": "2024-04-20T20:34:34.277292Z"
        },
        "trusted": true,
        "id": "Y4RHMuyXpA44"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers['tf_idf_keywords'][99]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:37:47.962391Z",
          "iopub.execute_input": "2024-04-20T20:37:47.962929Z",
          "iopub.status.idle": "2024-04-20T20:37:47.97194Z",
          "shell.execute_reply.started": "2024-04-20T20:37:47.962877Z",
          "shell.execute_reply": "2024-04-20T20:37:47.97052Z"
        },
        "trusted": true,
        "id": "nK03Pqm-pA44"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### TextRank ± filter punct.symbols, build graph\n",
        "\n",
        "_You might want to know how [PageRank](https://www.cis.upenn.edu/~mkearns/teaching/NetworkedLife/pagerank.pdf) algorithm works._\n",
        "\n",
        "![image.png](attachment:284b26a2-ced0-4091-9d48-8b7e9c148884.png)\n",
        "\n",
        "This algorithm is based on graphs. The idea is that after you get tokens from the text, you represent each token as a vertex of the graph. The links between these vertices are created when tokens occur close to each other in the text (i.e., we assume they are somehow related).\n",
        "\n",
        "The importance of each vertex is calculated using a method similar to PageRank. This involves iteratively assigning scores to each vertex based on the scores of its neighboring vertices."
      ],
      "metadata": {
        "id": "qbvXFcSUpA44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the networkx library to apply the PageRank algorithm on the graph."
      ],
      "metadata": {
        "id": "8hSAhhtSpA44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "def build_graph(words, window_size=4):\n",
        "    G = nx.Graph()\n",
        "    for index, word in enumerate(words):\n",
        "        for i in range(index + 1, index + window_size):\n",
        "            if i >= len(words):\n",
        "                break\n",
        "            node1, node2 = word, words[i]\n",
        "            if node1 != node2:\n",
        "                if G.has_edge(node1, node2):\n",
        "                    G[node1][node2]['weight'] += 1\n",
        "                else:\n",
        "                    G.add_edge(node1, node2, weight=1)\n",
        "    return G\n",
        "\n",
        "def extract_keywords(G):\n",
        "    ranks = nx.pagerank(G, weight='weight')\n",
        "    sorted_ranks = sorted(ranks.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [word for word, rank in sorted_ranks[:5]]  # Adjust the number of keywords as needed\n",
        "\n",
        "def apply_text_rank(tokens):\n",
        "    graph = build_graph(tokens)\n",
        "    return extract_keywords(graph)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:42:07.864165Z",
          "iopub.execute_input": "2024-04-20T20:42:07.864671Z",
          "iopub.status.idle": "2024-04-20T20:42:09.027968Z",
          "shell.execute_reply.started": "2024-04-20T20:42:07.864637Z",
          "shell.execute_reply": "2024-04-20T20:42:09.026853Z"
        },
        "trusted": true,
        "id": "9qqAPFdFpA44"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "_*Running this cell takes time_"
      ],
      "metadata": {
        "id": "413SANUHpA44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply TextRank to DataFrame\n",
        "nips_papers['text_rank_keywords'] = nips_papers['stemmed_tokens'].apply(apply_text_rank)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T21:16:34.778188Z",
          "iopub.execute_input": "2024-04-20T21:16:34.77875Z",
          "iopub.status.idle": "2024-04-20T21:40:55.767485Z",
          "shell.execute_reply.started": "2024-04-20T21:16:34.778709Z",
          "shell.execute_reply": "2024-04-20T21:40:55.766418Z"
        },
        "trusted": true,
        "id": "InwdSEc1pA45"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers['text_rank_keywords'][0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T21:46:28.697361Z",
          "iopub.execute_input": "2024-04-20T21:46:28.698168Z",
          "iopub.status.idle": "2024-04-20T21:46:28.707323Z",
          "shell.execute_reply.started": "2024-04-20T21:46:28.698117Z",
          "shell.execute_reply": "2024-04-20T21:46:28.705248Z"
        },
        "trusted": true,
        "id": "2_oB8TGYpA49"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, we will not visualize the full graph here. It is too large and there is little sense in it. But we can visualize a part of the graph for certain vertices."
      ],
      "metadata": {
        "id": "S4QxO2xnpA49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_keyword_subgraph(graph, keywords):\n",
        "    # Find nodes and neighbors\n",
        "    subgraph_nodes = set(keywords)\n",
        "    for keyword in keywords:\n",
        "        subgraph_nodes.update(graph.neighbors(keyword))\n",
        "\n",
        "    subgraph = graph.subgraph(subgraph_nodes)\n",
        "\n",
        "    # Draw the subgraph\n",
        "    pos = nx.spring_layout(subgraph)\n",
        "    nx.draw(subgraph, pos, with_labels=True, node_color='lightgreen', edge_color='#BBBBBB', node_size=300)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: visualize subgraph for specific keywords\n",
        "keywords = ['speech', 'vision']\n",
        "visualize_keyword_subgraph(graph, keywords)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T21:46:44.448996Z",
          "iopub.execute_input": "2024-04-20T21:46:44.449497Z",
          "iopub.status.idle": "2024-04-20T21:46:44.518784Z",
          "shell.execute_reply.started": "2024-04-20T21:46:44.449457Z",
          "shell.execute_reply": "2024-04-20T21:46:44.516721Z"
        },
        "trusted": true,
        "id": "9RC-F54BpA49"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SingleRank** and **TopicRank** are some variations ofr TextRank."
      ],
      "metadata": {
        "id": "3gTohbabpA49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Rake\n",
        "\n",
        "The RAKE algorithm is designed to efficiently identify key phrases from a text by analyzing the frequency of word appearance and its co-occurrence with other words in the text.\n",
        "\n",
        "RAKE starts by splitting the text into an array of words and phrases. It specifically looks for phrases by splitting the text at delimiters, which are typically punctuation marks and stop words.\n",
        "\n",
        "Each word is scored based on its frequency and its degree (the total number of times it appears in different phrases). The score is calculated using a formula:\n",
        "\n",
        "$$\n",
        "\\text{Score(word)} = \\frac{\\text{degree(word)}}{\\text{frequency(word)}}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\text{degree(word)}$ is the number of times a word co-occurs with other words across all phrases.\n",
        "- $\\text{frequency(word)}$ is the number of times the word appears in the entire text.\n",
        "\n",
        "Phrases are scored by summing the scores of the words they contain. This step emphasizes multi-word expressions that are more likely to represent key concepts.\n",
        "\n",
        "Then, the highest-scoring phrases are selected as the key phrases for the text.\n",
        "\n",
        "We aren't gonna implement this algorithm form scratch. Instead, we are going to use python package pke: https://github.com/boudinfl/pke"
      ],
      "metadata": {
        "id": "QgAlaXeZpA4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "jHpiPKn0pA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pke\n",
        "\n",
        "# Initialize RAKE extractor\n",
        "extractor = pke.unsupervised.RAKE()\n",
        "\n",
        "# Sample text\n",
        "text = \"Compatibility of systems of linear constraints over the set of natural numbers. \" \\\n",
        "       \"Criteria of compatibility of a system of linear Diophantine equations, strict \" \\\n",
        "       \"inequations, and nonstrict inequations are considered.\"\n",
        "\n",
        "# Load the text into the extractor\n",
        "extractor.load_document(input=text, language='en')\n",
        "\n",
        "# Select the longest sequence of nouns and adjectives as candidates.\n",
        "extractor.candidate_selection()\n",
        "\n",
        "# Calculate scores using the default scoring function\n",
        "extractor.candidate_weighting()\n",
        "\n",
        "# Get the 5 highest scored candidates\n",
        "keywords = extractor.get_n_best(n=5)\n",
        "\n",
        "# Print extracted keywords\n",
        "print(keywords)"
      ],
      "metadata": {
        "id": "Fcd6QB5WpA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### YAKE!\n",
        "\n",
        "YAKE! (Yet Another Keyword Extraction) designed to automatically detect keywords in text using an unsupervised approach. This algorithm leverages statistical features from the text itself without needing any external data.\n",
        "\n",
        "YAKE! calculates a set of features for each word in the text, including:\n",
        "- Case insensitivity: Emphasizes lower-case versions of words.\n",
        "- Word position: Early occurrences may be more important.\n",
        "- Word frequency: Frequent words are not necessarily important.\n",
        "- Word relatedness to context: Looks at the co-occurrence of words within their surrounding text.\n",
        "- Word differentiation: Identifies how spread out the word’s contexts are.\n",
        "\n",
        "Using these features, YAKE! computes a score for each word. A lower score suggests a higher relevance as a keyword. The scoring formula accounts for the uniqueness of the word’s occurrence across the document and its significance in individual contexts.\n",
        "\n",
        "After scoring, YAKE! selects the best-scoring words as keywords, paying attention to avoiding redundancy and favoring words that provide unique information.\n",
        "\n",
        "this algorithm is useful for dealing with large documents or collections of documents where you need a quick and effective method to identify the most relevant keywords without prior training or configuration."
      ],
      "metadata": {
        "id": "X2ftJOp_pA4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pke\n",
        "\n",
        "# Initialize YAKE extractor\n",
        "extractor = pke.unsupervised.YAKE()\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural language processing enables computers to understand human language. This technology powers many applications like speech recognition, machine translation, and sentiment analysis.\"\n",
        "\n",
        "# Load the text into the extractor\n",
        "extractor.load_document(input=text, language='en')\n",
        "\n",
        "# Candidate selection with no specific n-gram size, using default parameters\n",
        "extractor.candidate_selection()\n",
        "\n",
        "# Candidate weighting using YAKE's default parameters\n",
        "extractor.candidate_weighting()\n",
        "\n",
        "# Get the 5 highest scored candidates\n",
        "keywords = extractor.get_n_best(n=5)\n",
        "\n",
        "# Print extracted keywords\n",
        "print(keywords)"
      ],
      "metadata": {
        "id": "UnB_6BXdpA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Word Embeddings (keyBERT)\n",
        "\n",
        "KeyBERT is a keyword extraction model that utilizes state-of-the-art natural language processing technology based on word embeddings, specifically leveraging models like BERT (Bidirectional Encoder Representations from Transformers). Unlike traditional keyword extraction methods that rely on statistical measures or graph-based representations, KeyBERT uses the semantic meanings of words as represented by their embeddings. Here's how KeyBERT works:\n",
        "\n",
        "    Semantic Representation: KeyBERT first converts the text into a dense vector representation using a pre-trained language model (like BERT, RoBERTa, or DistilBERT). These embeddings capture the contextual relationships between words in the text.\n",
        "\n",
        "    Similarity Calculation: It then calculates the cosine similarity between the embeddings of the document and the embeddings of candidate phrases or words extracted from the document. This step assesses how representative the phrases are of the document's main topics.\n",
        "\n",
        "    Keyword Selection: Keywords are selected based on their similarity scores. The higher the score, the more relevant the word or phrase is to the overall context of the document.\n",
        "\n",
        "KeyBERT's strength lies in its ability to understand the nuances of language due to the sophisticated nature of the embeddings used, allowing it to identify keywords that are contextually relevant rather than merely frequent or statistically significant."
      ],
      "metadata": {
        "id": "8OHbmztEpA4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'full_text': [\n",
        "        \"Natural language processing enables computers to understand human language. This technology powers many applications like speech recognition, machine translation, and sentiment analysis.\",\n",
        "        \"Machine learning provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It's widely used in data analysis and complex algorithms.\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Load a model\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "\n",
        "# Initialize KeyBERT with the loaded model\n",
        "kw_model = KeyBERT(model=model)\n",
        "\n",
        "# Function to extract keywords\n",
        "def extract_keywords(text):\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)\n",
        "    return keywords\n",
        "\n",
        "# Apply the function to the 'full_text' column\n",
        "df['keywords'] = df['full_text'].apply(lambda x: extract_keywords(x))\n",
        "\n",
        "# Display the DataFrame with keywords\n",
        "print(df[['full_text', 'keywords']])"
      ],
      "metadata": {
        "id": "kPIVfnzfpA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "first_abstract = nips_papers.iloc[0]['abstract']\n",
        "print(\"TF-IDF Keywords:\", extract_keywords(first_abstract, 'tfidf'))\n",
        "print(\"TextRank Keywords:\", extract_keywords(first_abstract, 'textrank'))\n",
        "print(\"YAKE Keywords:\", extract_keywords(first_abstract, 'yake'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T21:56:54.158309Z",
          "iopub.execute_input": "2024-04-15T21:56:54.158681Z",
          "iopub.status.idle": "2024-04-15T21:57:00.742926Z",
          "shell.execute_reply.started": "2024-04-15T21:56:54.158654Z",
          "shell.execute_reply": "2024-04-15T21:57:00.741743Z"
        },
        "trusted": true,
        "id": "XXMG9zGipA4_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an App\n",
        "\n",
        "We're going to build an application that allows users to upload file or paste a text into a website window and receive keywords.\n",
        "\n",
        "![image.png](attachment:33e9fd66-c190-44bb-a2ae-ed2ffec1f902.png)\n",
        "\n",
        "To do this, we'll need to create a project repository: we'll use **Flask** and a simple **Bootstrap template**, and also write some auxiliary code that will **process the received data** and **log everything that happens**. This is a great exercise for aspiring engineers who want not only to understand algorithms but also to apply them in real-world scenarios.\n",
        "\n",
        "### [To continue this project visit my github](https://github.com/mlsamurai/keyword-extraction-project-nlp/blob/main/tutorial/Part%202.%20Application.ipynb/)\n",
        "\n",
        "Please upvote this notebook if it was helpful for you and share if you know someone who might find it helpful! Thank you."
      ],
      "metadata": {
        "id": "GgiBZfrbpA4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References and Links"
      ],
      "metadata": {
        "id": "HNuaCx7ZpA4_"
      }
    }
  ]
}