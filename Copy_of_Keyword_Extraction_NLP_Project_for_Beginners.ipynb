{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1702999,
          "sourceType": "datasetVersion",
          "datasetId": 1009351
        }
      ],
      "dockerImageVersionId": 30684,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LYX-AI/Keyword_Extraction_NLP_Project/blob/main/Copy_of_Keyword_Extraction_NLP_Project_for_Beginners.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#将本次项目的文件和文件夹都保存到Google Drive上\n",
        "#挂载 Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm9SO2Vg1gDP",
        "outputId": "4e49fecb-35d8-40de-8c78-ea7c2e442b39"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#创建路径\n",
        "import os\n",
        "\n",
        "# 在 Google Drive 中创建目录\n",
        "drive_path = '/content/drive/MyDrive/Kaggle/01Keyword_Extraction_NLP_Project_for_Beginners'\n",
        "os.makedirs(drive_path, exist_ok=True)  # 创建目录，如果已经存在则不报错\n"
      ],
      "metadata": {
        "id": "wxNsLJJn66Dn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#保存文件到 Google Drive：\n",
        "import shutil\n",
        "source_folder = '/content/input/nips-papers-1987-2019-updated/'  # 当前文件夹路径\n",
        "\n",
        "# 假设你已经上传了文件到 Colab\n",
        "for filename in os.listdir(source_folder):\n",
        "    file_path = os.path.join(source_folder, filename)  # 源文件路径\n",
        "    if os.path.isfile(file_path):  # 如果是文件\n",
        "        shutil.move(file_path, os.path.join(drive_path, filename))  # 移动到 Google Drive\n",
        "    elif os.path.isdir(file_path):  # 如果是子文件夹\n",
        "        shutil.move(file_path, os.path.join(drive_path, filename))  # 移动子文件夹\n",
        "\n",
        "# 验证文件是否保存成功\n",
        "os.listdir(drive_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2uClVxE7Q47",
        "outputId": "b50ca1d2-8423-4345-dbbc-d8e24e083d36"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['papers.csv', 'authors.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#从Drive中加载到Colab中\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/MyDrive/Kaggle/01Keyword_Extraction_NLP_Project_for_Beginners'\n",
        "# 检查文件是否存在\n",
        "print(os.listdir(drive_path))\n",
        "\n",
        "# 加载数据集\n",
        "authors_df = pd.read_csv(os.path.join(drive_path, 'authors.csv'))\n",
        "papers_df = pd.read_csv(os.path.join(drive_path, 'papers.csv'))\n",
        "\n",
        "# 查看数据的前几行\n",
        "authors_df.head(), papers_df.head()"
      ],
      "metadata": {
        "id": "8AFqw9UW9gsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "rowhitswami_nips_papers_1987_2019_updated_path = kagglehub.dataset_download('rowhitswami/nips-papers-1987-2019-updated')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "AaNgs_4VpA4r"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we're going to break down the most popular _keyword extraction_ algorithms. At the end, we'll create a small web application that takes a PDF file of an article (on any topic) and returns the keywords. To build the app, we'll need to go beyond this notebook and write a .py file, so when you're ready, follow the link here or in the bottom section to continue.\n",
        "\n",
        "Enjoy your learning!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "ZzOaPEBApA4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is Keyword Extraction?\n",
        "\n",
        "Keyword extraction is the process of selecting words or phrases from text that best reflect its content. Keywords convey the concentrated meaning of the text and can be used for categorization or retrieval.\n",
        "\n",
        "![image.png](attachment:7fbca9ff-8142-4789-84ff-34defbe74443.png)\n",
        "\n",
        "### General approach\n",
        "\n",
        "![image.png](attachment:67946850-f573-4005-af03-bf562efb8c4e.png)\n",
        "\n",
        "There are different methods of keyword extraction. But all of them have the following steps:\n",
        "\n",
        "- **Candidate Generation**: First up, we cast a wide net to catch potential keywords\n",
        "- **Candidate Scoring**: Next, we score each candidate based on how well they represent the text's main themes\n",
        "- **Candidate Ranking**: Finally, we rank these words based on their scores. The top performers get the title of \"keywords,\" ready to reflect the core ideas of the text.\n",
        "\n",
        "### Keyword Extraction Techniques\n",
        "\n",
        "Let's explore some of the most common techniques for keyword extraction. This is _just a brief overview_ of the algorithms we will cover in this notebook. If you don't understand what it's about, that's okay. A little later we'll break down how each algorithm works.\n",
        "\n",
        "All algorithms for keyword extraction can be categorized(分类) into statistical(统计), graph-based or word2vec (or deep networks) based algorithms. There are also algorithms based on machine learning, but we will not discuss them in this notebook. Just be aware of them.\n",
        "关键词提取的核心任务是从文本中找出最能代表其内容的词汇或短语。通过候选生成、评分和排序的步骤，我们可以有效地提取文本的核心思想。\n",
        "\n",
        "算法分类：关键词提取的算法可以分为几大类，主要包括统计方法（如TF-IDF）、基于图的算法、深度学习（如word2vec）等。理解这些算法能帮助我们选择合适的方法来处理不同类型的文本数据。\n",
        "\n",
        "统计方法简介：\n",
        "\n",
        "TF-IDF：通过计算词频和逆文档频率，找到在特定文档中独特而重要的词汇。\n",
        "\n",
        "Rake：通过分析词汇出现频率和共现关系来识别关键词，适合快速的文本分析。\n",
        "\n",
        "Yake：类似于Rake，但通过更复杂的统计特征进行优化，识别文本中不仅常见而且特别重要的词汇。\n",
        "\n",
        "这些方法是关键词提取领域的基础，理解它们能够帮助你在处理实际文本时选择合适的算法。\n",
        "\n",
        "\n",
        "\n",
        "一下三种方法是上面讲解的基于统计学的方法实现的算法\n",
        "#### Statistical Approaches\n",
        "- **TF-IDF** (Term Frequency-Inverse Document Frequency). This is a classic, folks. TF-IDF measures how important a word is to a document in a collection of documents. It’s like finding out which words in your document are the real VIPs because they're not commonly used elsewhere.\n",
        "- **Rake** (Rapid Automatic Keyword Extraction). Rake is pretty straightforward and efficient. It identifies key phrases in text by analyzing the frequency of word appearance and its co-occurrence with other words. Think of it as highlighting the standout phrases based on how often words show up together.\n",
        "- **Yake**. Yake takes a unique twist by using statistical features to rank keywords within a text based on their singularity and relevancy. It’s like having a smart assistant that picks out terms that are not just common, but also particularly significant in your text.\n",
        "\n",
        "TF-IDF：这个算法基于词频（TF）和逆文档频率（IDF）来计算词语在文档中的重要性。它能够帮助我们发现那些在某一特定文档中频繁出现，但在所有文档中都不常见的词语，这些通常是该文档的关键词。\n",
        "\n",
        "Rake：Rake算法通过分析词语的共现关系（即哪些词语经常一起出现）和出现的频率来快速识别关键词。它通常不需要像TF-IDF那样依赖大规模的语料库，可以快速识别文本中的关键短语。\n",
        "\n",
        "Yake：Yake也是一种基于统计的关键词提取算法，它类似于Rake，但引入了更多的统计特征来提高关键词提取的准确性。它不仅关注词语的频率，还会考虑其他因素，如词汇的独特性和与其他词的相关性，从而帮助识别文本中的关键内容。\n",
        "\n",
        "#### Graph-based Approaches **bold text**\n",
        "- **TextRank**. Imagine a system where words are nodes in a graph, connected based on their co-occurrence within a text. TextRank ranks these nodes (words) to figure out which are the most influential in the text. It’s like seeing which friend (word) in your social circle (document) is the most popular.\n",
        "- **SingleRank**. SingleRank takes TextRank a step further by adding weight to the connections between words based on their co-occurrence frequency. It’s akin to saying, not only is this friend popular, but their connections are super strong because they hang out together a lot.\n",
        "- **TopicRank**. TopicRank: This one groups words into clusters or topics first, and then performs a ranking similar to TextRank on these topics. It’s like organizing your friends into cliques and figuring out which clique is the life of the party.\n",
        "\n",
        "#### Deep Learning Techniques\n",
        "- **Word Embeddings** for Keyword Extraction. Deep learning brings us word embeddings, where words are converted into vector space models. This technique helps in extracting keywords by understanding the semantic richness of words, almost like capturing the essence of words in a mathematical form, which can really pinpoint the key concepts in a text.\n",
        "\n",
        "---\n",
        "\n",
        "### Applications of Keyword Extraction\n",
        "\n",
        "It's important to understand why keyword extraction is necessary. It seems that this is not as interesting a task as extracting named entities or training a neural network to generate text. But it is still a very important task.\n",
        "\n",
        "So, you can use Keywords for:\n",
        "\n",
        "- **Information Retrieval**: Enhancing search engine performance by indexing documents based on their keywords.\n",
        "- **Content Summarization**: Generating concise summaries of large texts.\n",
        "- **Document Clustering and Classification**: Organizing documents into categories based on their keywords.\n",
        "- **Trend Analysis**: Identifying emerging trends in large datasets by analyzing frequently occurring keywords.\n",
        "基于图的算法（如TextRank、SingleRank和TopicRank）使用图结构来理解词语之间的关系，这些算法可以通过计算词语在文本中的连接强度来识别关键词。TextRank看词语的整体影响力，SingleRank则通过共现频率进一步加权，TopicRank则在分组主题之后进行排名。\n",
        "\n",
        "深度学习技术（如词嵌入）为关键词提取提供了更为智能和精确的方法。通过将词语转化为向量，算法不仅关注词语的频率，还可以捕捉其在文本中的深层语义，使得关键词提取更加精准。\n",
        "\n",
        "以上都是一些关键词提取的算法我个人认为首次学习不用花太多时间搞懂它是干啥的\n",
        "---"
      ],
      "metadata": {
        "id": "wyA_1V8spA4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset Overview and EDA（了解项目用到的数据集）\n",
        "\n",
        "Before we break down each algorithm and put it into practice, let's look at the data we have. We want to understand the structure, size, and nature of our dataset.\n",
        "\n",
        "The NIPS (NeurIPS) paper dataset comprises thousands of research papers from the annual Neural Information Processing Systems conference. These papers cover a wide array of topics in machine learning, artificial intelligence, and statistics. The dataset typically includes titles, abstracts, full text of the papers, authors, and sometimes the keywords provided by the authors.\n",
        "\n",
        "### Setting Up the Environment\n",
        "\n",
        "First, ensure you have imported all necessary libraries and have the dataset ready. Kaggle datasets can typically be accessed directly within Kaggle notebooks through relative paths. If you are running this notebook locally, make sure that the imported libraries are installed in your environment."
      ],
      "metadata": {
        "id": "vga5uEAcpA4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#上传数据集\n",
        "\n",
        "#1.上传kaggle的API key\n",
        "from google.colab import files\n",
        "files.upload()  # 运行后，Colab会提示你选择文件\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "TwpBdB1dfDlX",
        "outputId": "36171d96-525d-4232-f7d8-a9a41c3c33c7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-326261a1-0191-4ee3-b9d4-9688cbc40505\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-326261a1-0191-4ee3-b9d4-9688cbc40505\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"dmhl1991121\",\"key\":\"85cca2fecbf2e0d6b5dd045dea8dedc7\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建Kaggle目录\n",
        "!mkdir -p ~/.kaggle"
      ],
      "metadata": {
        "id": "e3QHbT2tfpNT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 移动上传的 kaggle.json 文件到正确的目录\n",
        "!mv kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "mvAxao8fgAnF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置文件的权限\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "F8zRlg55gwAz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la ~/.kaggle\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3vAiwXhtgjk",
        "outputId": "28c7fcd7-1a18-419e-8514-a1369b812237"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwxr-xr-x 2 root root 4096 Jun 17 16:35 .\n",
            "drwx------ 1 root root 4096 Jun 17 16:23 ..\n",
            "-rw------- 1 root root   67 Jun 17 16:35 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#验证API是否导入成功\n",
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "906LT-zrhB9_",
        "outputId": "aff84993-1fe4-4d93-896d-54d55aac7fb8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                             title                                                     size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  -------------------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "rakeshkapilavai/extrovert-vs-introvert-behavior-data            Extrovert vs. Introvert Behavior Data                    31277  2025-06-13 14:26:48.303000          19056        416  1.0              \n",
            "bismasajjad/global-ai-job-market-and-salary-trends-2025         Global AI Job Market & Salary Trends 2025               529004  2025-06-01 07:20:49.537000           7392        119  0.9411765        \n",
            "adilshamim8/social-media-addiction-vs-relationships             Students' Social Media Addiction                          7851  2025-05-10 14:38:02.713000          18529        279  1.0              \n",
            "skullagos5246/upi-transactions-2024-dataset                     UPI Transactions 2024 Dataset                          5657850  2025-06-14 21:39:13.250000            651         22  1.0              \n",
            "therohithanand/used-car-price-prediction                        Used Car Price Prediction                               144387  2025-06-09 08:04:12.570000           2272         29  1.0              \n",
            "brendanartley/openfwi-preprocessed-72x72                        OpenFWI Preprocessed 72x72                         22114565718  2025-06-02 17:25:58.957000           3758         61  1.0              \n",
            "prajwaldongre/loan-application-and-transaction-fraud-detection  Loan Application & Transaction: Fraud Detection        8339124  2025-06-10 08:44:19.813000           1245         24  1.0              \n",
            "shalmamuji/personality-prediction-data-introvert-extrovert      Personality prediction data | introvert extrovert       164292  2025-06-12 10:38:45.273000            772         26  1.0              \n",
            "sahilislam007/sales-dataset                                     Sales Dataset                                             9016  2025-05-27 07:28:19.667000           1946         32  1.0              \n",
            "kanakbaghel/hospital-management-dataset                         Hospital Management Dataset                              11375  2025-05-30 14:40:55.287000           1574         24  0.9411765        \n",
            "samanfatima7/2020-2025-apple-stock-dataset                      2020-2025 Apple Stock Dataset                            52546  2025-06-03 11:57:03.600000           1114         26  0.9411765        \n",
            "abhishekdave9/digital-habits-vs-mental-health-dataset           Screen Time Impact on Mental Health                     559014  2025-06-13 11:42:28.647000            616         26  0.9411765        \n",
            "hbugrae/best-selling-steam-games-of-all-time                    Best-Selling Steam Games of All Time                    158051  2025-06-12 11:24:15.477000            966         23  1.0              \n",
            "flynn28/2025-premier-league-stats-matches-salaries              2025 Premier League: Stats, Matches, Salaries            59228  2025-05-26 01:25:53.030000           1407         30  1.0              \n",
            "shreyasdasari7/top-100-saas-companiesstartups                   Top 100 SaaS Companies/Startups 2025                      5050  2025-05-29 00:22:47.850000           1072         29  1.0              \n",
            "dansbecker/melbourne-housing-snapshot                           Melbourne Housing Snapshot                              461423  2018-06-05 12:52:24.087000         182563       1638  0.7058824        \n",
            "anirudhsub/twizzlerdata                                         Popularity of Twizzlers Dataset                            996  2025-06-13 22:01:17.120000            301         23  1.0              \n",
            "adilshamim8/rock-paper-scissors                                 Rock Paper Scissors SXSW: Hand Gesture Detection     210940029  2025-05-28 04:09:22.500000           1809         78  1.0              \n",
            "datasnaek/youtube-new                                           Trending YouTube Video Statistics                    210575746  2019-06-03 00:56:47.177000         274076       5688  0.7941176        \n",
            "zynicide/wine-reviews                                           Wine Reviews                                          53336293  2017-11-27 17:08:04.700000         327525       3743  0.7941176        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#从kaggle上下载数据集\n",
        "!kaggle datasets download -d nips-papers-1987-2019-updated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KovNsMvPssS1",
        "outputId": "0981811d-d59b-40b4-f052-3e3d70a95ed7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/metadata/dmhl1991121/nips-papers-1987-2019-updated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "根据以上代码结果可以看出无法顺利将数据集导入到colab，所以我选择手动从koggle上下载数据集然后再手动上传到../input/nips-papers-1987-2019-updated/  这个路径下"
      ],
      "metadata": {
        "id": "cTtYzU3fyUBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#创建路径\n",
        "import os\n",
        "\n",
        "# 创建指定路径\n",
        "path = '/content/input/nips-papers-1987-2019-updated/'\n",
        "os.makedirs(path, exist_ok=True)  # 如果路径已存在，则不会报错\n"
      ],
      "metadata": {
        "id": "QzcBpJaeyiMa"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 上传文件\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 将文件保存到指定路径\n",
        "for filename in uploaded.keys():\n",
        "    # 移动文件到指定路径\n",
        "    os.rename(filename, os.path.join(path, filename))\n",
        "\n",
        "print(\"文件已上传到：\", path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "jj2oNlLIyrgn",
        "outputId": "36878a9d-ac6b-41bf-a50d-edf99edebd7c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b69ecf47-1933-4f72-bb1f-2f6a3c473e6f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b69ecf47-1933-4f72-bb1f-2f6a3c473e6f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving authors.csv to authors.csv\n",
            "Saving papers.csv to papers.csv\n",
            "文件已上传到： /content/input/nips-papers-1987-2019-updated/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Display plots inline and set default figure size\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Assuming the dataset is loaded into a Kaggle notebook with the filename 'nips_papers.csv'\n",
        "nips_papers = pd.read_csv('../input/nips-papers-1987-2019-updated/papers.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-26T17:59:07.886037Z",
          "iopub.execute_input": "2024-04-26T17:59:07.886445Z",
          "iopub.status.idle": "2024-04-26T17:59:18.830569Z",
          "shell.execute_reply.started": "2024-04-26T17:59:07.886412Z",
          "shell.execute_reply": "2024-04-26T17:59:18.829444Z"
        },
        "trusted": true,
        "id": "XBFeO7c-pA4y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the Dataset Structure\n",
        "\n",
        "Let's begin with examining the first few rows, the data types of each column, and checking for missing values."
      ],
      "metadata": {
        "id": "2JgK1AADpA4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T20:13:14.18178Z",
          "iopub.execute_input": "2024-04-21T20:13:14.183011Z",
          "iopub.status.idle": "2024-04-21T20:13:14.201136Z",
          "shell.execute_reply.started": "2024-04-21T20:13:14.182973Z",
          "shell.execute_reply": "2024-04-21T20:13:14.200041Z"
        },
        "trusted": true,
        "id": "pTTFv-DRpA4z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we are most interested in the full_text column. It contains the text of the abstract and the paper."
      ],
      "metadata": {
        "id": "Dtjk9YCIpA4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nips_papers.isnull().sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:41:34.504507Z",
          "iopub.execute_input": "2024-04-20T19:41:34.505021Z",
          "iopub.status.idle": "2024-04-20T19:41:34.522897Z",
          "shell.execute_reply.started": "2024-04-20T19:41:34.504984Z",
          "shell.execute_reply": "2024-04-20T19:41:34.52061Z"
        },
        "trusted": true,
        "id": "dQ5HpIl3pA40"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "full_text is missing in only three rows. But there are more than three thousand rows where abstract text is missing."
      ],
      "metadata": {
        "id": "f7H2WuGkpA40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning the Data\n",
        "\n",
        "We'll just get rid of rows that don't have full_text."
      ],
      "metadata": {
        "id": "iFQGSSuApA40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping rows where 'abstract' is missing\n",
        "nips_papers.dropna(subset=['full_text'], inplace=True)\n",
        "\n",
        "# Reset index after dropping rows\n",
        "nips_papers.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:41:37.445413Z",
          "iopub.execute_input": "2024-04-20T19:41:37.445926Z",
          "iopub.status.idle": "2024-04-20T19:41:37.474138Z",
          "shell.execute_reply.started": "2024-04-20T19:41:37.445888Z",
          "shell.execute_reply": "2024-04-20T19:41:37.471594Z"
        },
        "trusted": true,
        "id": "gIahJDo1pA41"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing Trends\n",
        "\n",
        "Look for trends such as the number of papers published over the years. This can provide insights into the dataset's coverage and any growth trends in the field of machine learning and NLP."
      ],
      "metadata": {
        "id": "7tK0VOMWpA41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the number of papers per year\n",
        "papers_per_year = nips_papers.groupby('year')['title'].count()\n",
        "papers_per_year.plot(kind='line', marker='o', title='NIPS Papers per Year')\n",
        "plt.ylabel('Number of Papers')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:41:40.641949Z",
          "iopub.execute_input": "2024-04-20T19:41:40.643444Z",
          "iopub.status.idle": "2024-04-20T19:41:40.985424Z",
          "shell.execute_reply.started": "2024-04-20T19:41:40.643383Z",
          "shell.execute_reply": "2024-04-20T19:41:40.983974Z"
        },
        "trusted": true,
        "id": "5VoM8YzSpA41"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keyword Frequency Analysis\n",
        "\n",
        "Before diving into keyword extraction, analyzing the most frequent words in your texts (e.g., paper titles or abstracts) can give you a preliminary idea of common themes."
      ],
      "metadata": {
        "id": "wAOx3INCpA41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec = CountVectorizer(stop_words='english', max_features=100)\n",
        "word_counts = vec.fit_transform(nips_papers['full_text'])\n",
        "sum_words = word_counts.sum(axis=0)\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Plotting the top 20 words\n",
        "df_words_freq = pd.DataFrame(words_freq[:30], columns=['Word', 'Frequency'])\n",
        "sns.barplot(x='Frequency', y='Word', data=df_words_freq, palette='Blues_d')\n",
        "plt.title('Top 30 Most Frequent Words in NIPS Abstracts')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:42:05.706404Z",
          "iopub.execute_input": "2024-04-20T19:42:05.706968Z",
          "iopub.status.idle": "2024-04-20T19:42:56.844506Z",
          "shell.execute_reply.started": "2024-04-20T19:42:05.706926Z",
          "shell.execute_reply": "2024-04-20T19:42:56.843102Z"
        },
        "trusted": true,
        "id": "WcHEqGFHpA41"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "High-frequency words not filtered out by the standard stop words list but still irrelevant to content (like 'paper' or 'results') might need custom stop words processing or advanced techniques like TF-IDF to reduce their impact."
      ],
      "metadata": {
        "id": "P7phSGMqpA42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "### NLTK, spacy\n",
        "\n",
        "Two of the most popular Python libraries for these tasks are NLTK and spaCy.\n",
        "\n",
        "- [NLTK (Natural Language Toolkit)](https://www.nltk.org/) is widely used for teaching and research. It's a comprehensive library featuring many utilities for classical NLP tasks and text processing.\n",
        "- [spaCy](https://spacy.io/) is known for its fast processing speeds and ease of use. It also provides pre-trained models and word vectors and is designed specifically for production use."
      ],
      "metadata": {
        "id": "mXxG9LxVpA42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:44:26.560716Z",
          "iopub.execute_input": "2024-04-20T19:44:26.561248Z",
          "iopub.status.idle": "2024-04-20T19:44:26.700524Z",
          "shell.execute_reply.started": "2024-04-20T19:44:26.561215Z",
          "shell.execute_reply": "2024-04-20T19:44:26.699242Z"
        },
        "trusted": true,
        "id": "OR6K8sFnpA42"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization is the process of splitting text into meaningful units such as words, sentences, or tokens. This is typically the first step in text preprocessing.\n",
        "\n",
        "_*Running this cell takes time_"
      ],
      "metadata": {
        "id": "tTfNA9XgpA42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Using NLTK for tokenization\n",
        "nips_papers['tokens'] = nips_papers['full_text'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:44:33.509138Z",
          "iopub.execute_input": "2024-04-20T19:44:33.509622Z",
          "iopub.status.idle": "2024-04-20T19:54:50.762379Z",
          "shell.execute_reply.started": "2024-04-20T19:44:33.509589Z",
          "shell.execute_reply": "2024-04-20T19:54:50.759882Z"
        },
        "trusted": true,
        "id": "Bn4huWZ4pA42"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords removal\n",
        "\n",
        "Stopwords are commonly used words (such as \"the\", \"a\", \"an\", \"in\") that are often removed in the preprocessing phase because they carry less meaningful information for analysis."
      ],
      "metadata": {
        "id": "yH1PxDr5pA42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load general English stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T19:55:26.758982Z",
          "iopub.execute_input": "2024-04-20T19:55:26.760474Z",
          "iopub.status.idle": "2024-04-20T19:55:26.770547Z",
          "shell.execute_reply.started": "2024-04-20T19:55:26.760419Z",
          "shell.execute_reply": "2024-04-20T19:55:26.769209Z"
        },
        "trusted": true,
        "id": "31DAM8D6pA42"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with specialized datasets like NIPS papers, you might encounter domain-specific stopwords like \"neural\", \"network\", \"learning\", which are overly frequent and might be less informative for certain analyses."
      ],
      "metadata": {
        "id": "oLRo_IJKpA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom stopwords for NIPS context\n",
        "custom_stopwords = {\"cid\", \"neural\", \"network\", \"learning\", \"model\", \"algorithm\", \"data\",\n",
        "                    \"set\", \"function\", \"problem\", \"models\", \"number\", \"figure\", \"results\",\n",
        "                   \"information\", \"distribution\", \"using\", \"used\", \"use\", \"given\", \"method\",\n",
        "                   \"neural\", \"networks\"}\n",
        "\n",
        "# Function to remove both general and custom stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    return [token for token in tokens if token.lower() not in stop_words and\n",
        "            token.lower() not in custom_stopwords and not token.isdigit()]\n",
        "\n",
        "nips_papers['filtered_tokens'] = nips_papers['tokens'].apply(remove_stopwords)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:01:25.625652Z",
          "iopub.execute_input": "2024-04-20T20:01:25.626389Z",
          "iopub.status.idle": "2024-04-20T20:01:49.435082Z",
          "shell.execute_reply.started": "2024-04-20T20:01:25.626298Z",
          "shell.execute_reply": "2024-04-20T20:01:49.433166Z"
        },
        "trusted": true,
        "id": "U4h8KV3dpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "Stemming reduces words to their word stem, base, or root form. For example, \"fishing\", \"fished\", \"fisher\" all reduce to the stem \"fish\".\n",
        "\n",
        "_*Running this cell takes time_"
      ],
      "metadata": {
        "id": "efnrXbD7pA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to stem tokens\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "nips_papers['stemmed_tokens'] = nips_papers['filtered_tokens'].apply(stem_tokens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:02:18.102524Z",
          "iopub.execute_input": "2024-04-20T20:02:18.103082Z",
          "iopub.status.idle": "2024-04-20T20:13:41.111909Z",
          "shell.execute_reply.started": "2024-04-20T20:02:18.103041Z",
          "shell.execute_reply": "2024-04-20T20:13:41.10988Z"
        },
        "trusted": true,
        "id": "cGB-45DNpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers['stemmed_tokens'][0]"
      ],
      "metadata": {
        "trusted": true,
        "id": "-34p9ZnrpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *To fix: Lemmatization\n",
        "\n",
        "Lemmatization, similar to stemming, reduces words to their base or root form, but it ensures that the root word belongs to the language."
      ],
      "metadata": {
        "id": "9Zvf0jXTpA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize a list of tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Apply lemmatization to the filtered tokens\n",
        "nips_papers['lemmatized_tokens'] = nips_papers['filtered_tokens'].apply(lemmatize_tokens)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "17c3dSfMpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Algorithms\n",
        "\n",
        "### TF-IDF for Keyword Extraction\n",
        "\n",
        "![image.png](attachment:31a52603-1d81-4846-844a-79c77f5fac43.png)\n",
        "\n",
        "- **Calculate Term Frequency (TF)**: For each word in your document, calculate the term frequency. This is simply the number of times a word appears in a document divided by the total number of words in that document.\n",
        "\n",
        "$$\n",
        "\\text{TF}(word) = \\frac{\\text{Number of times the word appears in a document}}{\\text{Total number of words in the document}}\n",
        "$$\n",
        "\n",
        "    \n",
        "- **Calculate Inverse Document Frequency (IDF)**: This measures how common or rare a word is across all documents. It is calculated as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
        "\n",
        "$$\n",
        "\\text{IDF}(word) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing the word}}\\right)\n",
        "$$\n",
        "    \n",
        "- **Compute TF-IDF: Multiply TF by IDF.** The words with the highest TF-IDF scores are considered the most relevant keywords in the document.\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(word) = \\text{TF}(word) \\times \\text{IDF}(word)\n",
        "$$\n",
        "\n",
        "Very simple, huh?\n",
        "\n",
        "Let's implement this algorithm in pure Python."
      ],
      "metadata": {
        "id": "ir0etvzLpA43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_tf(tokenized_docs):\n",
        "    tf_dicts = []\n",
        "    for doc in tokenized_docs:\n",
        "        tf_dict = {}\n",
        "        total_terms = len(doc)\n",
        "        for term in doc:\n",
        "            tf_dict[term] = tf_dict.get(term, 0) + 1 / total_terms\n",
        "        tf_dicts.append(tf_dict)\n",
        "    return tf_dicts\n",
        "\n",
        "def compute_df(tokenized_docs):\n",
        "    df_dict = {}\n",
        "    for doc in tokenized_docs:\n",
        "        for term in set(doc):\n",
        "            df_dict[term] = df_dict.get(term, 0) + 1\n",
        "    return df_dict\n",
        "\n",
        "def compute_idf(df_dict, total_docs):\n",
        "    idf_dict = {}\n",
        "    for term, count in df_dict.items():\n",
        "        idf_dict[term] = math.log(total_docs / count)\n",
        "    return idf_dict\n",
        "\n",
        "def compute_tf_idf(tf_dicts, idf_dict):\n",
        "    tf_idf_dicts = []\n",
        "    for tf_dict in tf_dicts:\n",
        "        tf_idf_dict = {}\n",
        "        for term, tf in tf_dict.items():\n",
        "            tf_idf_dict[term] = tf * idf_dict.get(term, 0)\n",
        "        tf_idf_dicts.append(tf_idf_dict)\n",
        "    return tf_idf_dicts\n",
        "\n",
        "def select_top_keywords(tf_idf_dicts, top_n=5):\n",
        "    top_keywords = []\n",
        "    for tf_idf_dict in tf_idf_dicts:\n",
        "        sorted_terms = sorted(tf_idf_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_keywords.append([term for term, score in sorted_terms[:top_n]])\n",
        "    return top_keywords"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:34:02.487809Z",
          "iopub.execute_input": "2024-04-20T20:34:02.489016Z",
          "iopub.status.idle": "2024-04-20T20:34:02.500136Z",
          "shell.execute_reply.started": "2024-04-20T20:34:02.488959Z",
          "shell.execute_reply": "2024-04-20T20:34:02.498518Z"
        },
        "trusted": true,
        "id": "pg8_bSyTpA43"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_docs = nips_papers['stemmed_tokens'].tolist()\n",
        "tf_dicts = compute_tf(tokenized_docs)\n",
        "df_dict = compute_df(tokenized_docs)\n",
        "total_docs = len(tokenized_docs)\n",
        "idf_dict = compute_idf(df_dict, total_docs)\n",
        "tf_idf_dicts = compute_tf_idf(tf_dicts, idf_dict)\n",
        "top_tf_idf_keywords = select_top_keywords(tf_idf_dicts)\n",
        "\n",
        "# Update the DataFrame\n",
        "nips_papers['tf_idf_keywords'] = top_tf_idf_keywords"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:34:05.320552Z",
          "iopub.execute_input": "2024-04-20T20:34:05.321039Z",
          "iopub.status.idle": "2024-04-20T20:34:34.278624Z",
          "shell.execute_reply.started": "2024-04-20T20:34:05.321004Z",
          "shell.execute_reply": "2024-04-20T20:34:34.277292Z"
        },
        "trusted": true,
        "id": "Y4RHMuyXpA44"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers['tf_idf_keywords'][99]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:37:47.962391Z",
          "iopub.execute_input": "2024-04-20T20:37:47.962929Z",
          "iopub.status.idle": "2024-04-20T20:37:47.97194Z",
          "shell.execute_reply.started": "2024-04-20T20:37:47.962877Z",
          "shell.execute_reply": "2024-04-20T20:37:47.97052Z"
        },
        "trusted": true,
        "id": "nK03Pqm-pA44"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### TextRank ± filter punct.symbols, build graph\n",
        "\n",
        "_You might want to know how [PageRank](https://www.cis.upenn.edu/~mkearns/teaching/NetworkedLife/pagerank.pdf) algorithm works._\n",
        "\n",
        "![image.png](attachment:284b26a2-ced0-4091-9d48-8b7e9c148884.png)\n",
        "\n",
        "This algorithm is based on graphs. The idea is that after you get tokens from the text, you represent each token as a vertex of the graph. The links between these vertices are created when tokens occur close to each other in the text (i.e., we assume they are somehow related).\n",
        "\n",
        "The importance of each vertex is calculated using a method similar to PageRank. This involves iteratively assigning scores to each vertex based on the scores of its neighboring vertices."
      ],
      "metadata": {
        "id": "qbvXFcSUpA44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the networkx library to apply the PageRank algorithm on the graph."
      ],
      "metadata": {
        "id": "8hSAhhtSpA44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "def build_graph(words, window_size=4):\n",
        "    G = nx.Graph()\n",
        "    for index, word in enumerate(words):\n",
        "        for i in range(index + 1, index + window_size):\n",
        "            if i >= len(words):\n",
        "                break\n",
        "            node1, node2 = word, words[i]\n",
        "            if node1 != node2:\n",
        "                if G.has_edge(node1, node2):\n",
        "                    G[node1][node2]['weight'] += 1\n",
        "                else:\n",
        "                    G.add_edge(node1, node2, weight=1)\n",
        "    return G\n",
        "\n",
        "def extract_keywords(G):\n",
        "    ranks = nx.pagerank(G, weight='weight')\n",
        "    sorted_ranks = sorted(ranks.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [word for word, rank in sorted_ranks[:5]]  # Adjust the number of keywords as needed\n",
        "\n",
        "def apply_text_rank(tokens):\n",
        "    graph = build_graph(tokens)\n",
        "    return extract_keywords(graph)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T20:42:07.864165Z",
          "iopub.execute_input": "2024-04-20T20:42:07.864671Z",
          "iopub.status.idle": "2024-04-20T20:42:09.027968Z",
          "shell.execute_reply.started": "2024-04-20T20:42:07.864637Z",
          "shell.execute_reply": "2024-04-20T20:42:09.026853Z"
        },
        "trusted": true,
        "id": "9qqAPFdFpA44"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "_*Running this cell takes time_"
      ],
      "metadata": {
        "id": "413SANUHpA44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply TextRank to DataFrame\n",
        "nips_papers['text_rank_keywords'] = nips_papers['stemmed_tokens'].apply(apply_text_rank)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T21:16:34.778188Z",
          "iopub.execute_input": "2024-04-20T21:16:34.77875Z",
          "iopub.status.idle": "2024-04-20T21:40:55.767485Z",
          "shell.execute_reply.started": "2024-04-20T21:16:34.778709Z",
          "shell.execute_reply": "2024-04-20T21:40:55.766418Z"
        },
        "trusted": true,
        "id": "InwdSEc1pA45"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nips_papers['text_rank_keywords'][0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T21:46:28.697361Z",
          "iopub.execute_input": "2024-04-20T21:46:28.698168Z",
          "iopub.status.idle": "2024-04-20T21:46:28.707323Z",
          "shell.execute_reply.started": "2024-04-20T21:46:28.698117Z",
          "shell.execute_reply": "2024-04-20T21:46:28.705248Z"
        },
        "trusted": true,
        "id": "2_oB8TGYpA49"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, we will not visualize the full graph here. It is too large and there is little sense in it. But we can visualize a part of the graph for certain vertices."
      ],
      "metadata": {
        "id": "S4QxO2xnpA49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_keyword_subgraph(graph, keywords):\n",
        "    # Find nodes and neighbors\n",
        "    subgraph_nodes = set(keywords)\n",
        "    for keyword in keywords:\n",
        "        subgraph_nodes.update(graph.neighbors(keyword))\n",
        "\n",
        "    subgraph = graph.subgraph(subgraph_nodes)\n",
        "\n",
        "    # Draw the subgraph\n",
        "    pos = nx.spring_layout(subgraph)\n",
        "    nx.draw(subgraph, pos, with_labels=True, node_color='lightgreen', edge_color='#BBBBBB', node_size=300)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage: visualize subgraph for specific keywords\n",
        "keywords = ['speech', 'vision']\n",
        "visualize_keyword_subgraph(graph, keywords)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-20T21:46:44.448996Z",
          "iopub.execute_input": "2024-04-20T21:46:44.449497Z",
          "iopub.status.idle": "2024-04-20T21:46:44.518784Z",
          "shell.execute_reply.started": "2024-04-20T21:46:44.449457Z",
          "shell.execute_reply": "2024-04-20T21:46:44.516721Z"
        },
        "trusted": true,
        "id": "9RC-F54BpA49"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SingleRank** and **TopicRank** are some variations ofr TextRank."
      ],
      "metadata": {
        "id": "3gTohbabpA49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Rake\n",
        "\n",
        "The RAKE algorithm is designed to efficiently identify key phrases from a text by analyzing the frequency of word appearance and its co-occurrence with other words in the text.\n",
        "\n",
        "RAKE starts by splitting the text into an array of words and phrases. It specifically looks for phrases by splitting the text at delimiters, which are typically punctuation marks and stop words.\n",
        "\n",
        "Each word is scored based on its frequency and its degree (the total number of times it appears in different phrases). The score is calculated using a formula:\n",
        "\n",
        "$$\n",
        "\\text{Score(word)} = \\frac{\\text{degree(word)}}{\\text{frequency(word)}}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\text{degree(word)}$ is the number of times a word co-occurs with other words across all phrases.\n",
        "- $\\text{frequency(word)}$ is the number of times the word appears in the entire text.\n",
        "\n",
        "Phrases are scored by summing the scores of the words they contain. This step emphasizes multi-word expressions that are more likely to represent key concepts.\n",
        "\n",
        "Then, the highest-scoring phrases are selected as the key phrases for the text.\n",
        "\n",
        "We aren't gonna implement this algorithm form scratch. Instead, we are going to use python package pke: https://github.com/boudinfl/pke"
      ],
      "metadata": {
        "id": "QgAlaXeZpA4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "jHpiPKn0pA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pke\n",
        "\n",
        "# Initialize RAKE extractor\n",
        "extractor = pke.unsupervised.RAKE()\n",
        "\n",
        "# Sample text\n",
        "text = \"Compatibility of systems of linear constraints over the set of natural numbers. \" \\\n",
        "       \"Criteria of compatibility of a system of linear Diophantine equations, strict \" \\\n",
        "       \"inequations, and nonstrict inequations are considered.\"\n",
        "\n",
        "# Load the text into the extractor\n",
        "extractor.load_document(input=text, language='en')\n",
        "\n",
        "# Select the longest sequence of nouns and adjectives as candidates.\n",
        "extractor.candidate_selection()\n",
        "\n",
        "# Calculate scores using the default scoring function\n",
        "extractor.candidate_weighting()\n",
        "\n",
        "# Get the 5 highest scored candidates\n",
        "keywords = extractor.get_n_best(n=5)\n",
        "\n",
        "# Print extracted keywords\n",
        "print(keywords)"
      ],
      "metadata": {
        "id": "Fcd6QB5WpA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### YAKE!\n",
        "\n",
        "YAKE! (Yet Another Keyword Extraction) designed to automatically detect keywords in text using an unsupervised approach. This algorithm leverages statistical features from the text itself without needing any external data.\n",
        "\n",
        "YAKE! calculates a set of features for each word in the text, including:\n",
        "- Case insensitivity: Emphasizes lower-case versions of words.\n",
        "- Word position: Early occurrences may be more important.\n",
        "- Word frequency: Frequent words are not necessarily important.\n",
        "- Word relatedness to context: Looks at the co-occurrence of words within their surrounding text.\n",
        "- Word differentiation: Identifies how spread out the word’s contexts are.\n",
        "\n",
        "Using these features, YAKE! computes a score for each word. A lower score suggests a higher relevance as a keyword. The scoring formula accounts for the uniqueness of the word’s occurrence across the document and its significance in individual contexts.\n",
        "\n",
        "After scoring, YAKE! selects the best-scoring words as keywords, paying attention to avoiding redundancy and favoring words that provide unique information.\n",
        "\n",
        "this algorithm is useful for dealing with large documents or collections of documents where you need a quick and effective method to identify the most relevant keywords without prior training or configuration."
      ],
      "metadata": {
        "id": "X2ftJOp_pA4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pke\n",
        "\n",
        "# Initialize YAKE extractor\n",
        "extractor = pke.unsupervised.YAKE()\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural language processing enables computers to understand human language. This technology powers many applications like speech recognition, machine translation, and sentiment analysis.\"\n",
        "\n",
        "# Load the text into the extractor\n",
        "extractor.load_document(input=text, language='en')\n",
        "\n",
        "# Candidate selection with no specific n-gram size, using default parameters\n",
        "extractor.candidate_selection()\n",
        "\n",
        "# Candidate weighting using YAKE's default parameters\n",
        "extractor.candidate_weighting()\n",
        "\n",
        "# Get the 5 highest scored candidates\n",
        "keywords = extractor.get_n_best(n=5)\n",
        "\n",
        "# Print extracted keywords\n",
        "print(keywords)"
      ],
      "metadata": {
        "id": "UnB_6BXdpA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Word Embeddings (keyBERT)\n",
        "\n",
        "KeyBERT is a keyword extraction model that utilizes state-of-the-art natural language processing technology based on word embeddings, specifically leveraging models like BERT (Bidirectional Encoder Representations from Transformers). Unlike traditional keyword extraction methods that rely on statistical measures or graph-based representations, KeyBERT uses the semantic meanings of words as represented by their embeddings. Here's how KeyBERT works:\n",
        "\n",
        "    Semantic Representation: KeyBERT first converts the text into a dense vector representation using a pre-trained language model (like BERT, RoBERTa, or DistilBERT). These embeddings capture the contextual relationships between words in the text.\n",
        "\n",
        "    Similarity Calculation: It then calculates the cosine similarity between the embeddings of the document and the embeddings of candidate phrases or words extracted from the document. This step assesses how representative the phrases are of the document's main topics.\n",
        "\n",
        "    Keyword Selection: Keywords are selected based on their similarity scores. The higher the score, the more relevant the word or phrase is to the overall context of the document.\n",
        "\n",
        "KeyBERT's strength lies in its ability to understand the nuances of language due to the sophisticated nature of the embeddings used, allowing it to identify keywords that are contextually relevant rather than merely frequent or statistically significant."
      ],
      "metadata": {
        "id": "8OHbmztEpA4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from keybert import KeyBERT\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'full_text': [\n",
        "        \"Natural language processing enables computers to understand human language. This technology powers many applications like speech recognition, machine translation, and sentiment analysis.\",\n",
        "        \"Machine learning provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It's widely used in data analysis and complex algorithms.\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Load a model\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "\n",
        "# Initialize KeyBERT with the loaded model\n",
        "kw_model = KeyBERT(model=model)\n",
        "\n",
        "# Function to extract keywords\n",
        "def extract_keywords(text):\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)\n",
        "    return keywords\n",
        "\n",
        "# Apply the function to the 'full_text' column\n",
        "df['keywords'] = df['full_text'].apply(lambda x: extract_keywords(x))\n",
        "\n",
        "# Display the DataFrame with keywords\n",
        "print(df[['full_text', 'keywords']])"
      ],
      "metadata": {
        "id": "kPIVfnzfpA4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "first_abstract = nips_papers.iloc[0]['abstract']\n",
        "print(\"TF-IDF Keywords:\", extract_keywords(first_abstract, 'tfidf'))\n",
        "print(\"TextRank Keywords:\", extract_keywords(first_abstract, 'textrank'))\n",
        "print(\"YAKE Keywords:\", extract_keywords(first_abstract, 'yake'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T21:56:54.158309Z",
          "iopub.execute_input": "2024-04-15T21:56:54.158681Z",
          "iopub.status.idle": "2024-04-15T21:57:00.742926Z",
          "shell.execute_reply.started": "2024-04-15T21:56:54.158654Z",
          "shell.execute_reply": "2024-04-15T21:57:00.741743Z"
        },
        "trusted": true,
        "id": "XXMG9zGipA4_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an App\n",
        "\n",
        "We're going to build an application that allows users to upload file or paste a text into a website window and receive keywords.\n",
        "\n",
        "![image.png](attachment:33e9fd66-c190-44bb-a2ae-ed2ffec1f902.png)\n",
        "\n",
        "To do this, we'll need to create a project repository: we'll use **Flask** and a simple **Bootstrap template**, and also write some auxiliary code that will **process the received data** and **log everything that happens**. This is a great exercise for aspiring engineers who want not only to understand algorithms but also to apply them in real-world scenarios.\n",
        "\n",
        "### [To continue this project visit my github](https://github.com/mlsamurai/keyword-extraction-project-nlp/blob/main/tutorial/Part%202.%20Application.ipynb/)\n",
        "\n",
        "Please upvote this notebook if it was helpful for you and share if you know someone who might find it helpful! Thank you."
      ],
      "metadata": {
        "id": "GgiBZfrbpA4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References and Links"
      ],
      "metadata": {
        "id": "HNuaCx7ZpA4_"
      }
    }
  ]
}